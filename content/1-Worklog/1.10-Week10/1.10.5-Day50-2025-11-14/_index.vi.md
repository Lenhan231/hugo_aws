---
title: "NgÃ y 50 - Fine-tuning & á»¨ng Dá»¥ng Thá»±c Táº¿"
weight: 5
chapter: false
pre: "<b> 1.10.5. </b>"
---

**NgÃ y:** 2025-11-14 (Thá»© SÃ¡u)  
**Tráº¡ng ThÃ¡i:** "HoÃ n ThÃ nh"  

---

# **Tuáº§n 10 HoÃ n ThÃ nh: Tá»« LÃ½ Thuyáº¿t Äáº¿n Sáº£n Xuáº¥t**

BÃ¢y giá» báº¡n hiá»ƒu transfer learning, BERT vÃ  T5. NgÃ y cuá»‘i cÃ¹ng nÃ y bao gá»“m **cÃ¡ch thá»±c sá»± triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh nÃ y.**

---

# **Fine-tuning: Nghá»‡ Thuáº­t vÃ  Khoa Há»c**

## Bá»©c Tranh Lá»›n

```
MÃ´ HÃ¬nh ÄÆ°á»£c Huáº¥n Luyá»‡n TrÆ°á»›c (vÃ­ dá»¥: BERT-base)
â”œâ”€ ÄÃ£ biáº¿t tiáº¿ng Anh
â”œâ”€ ÄÃ£ hiá»ƒu ngá»¯ phÃ¡p
â”œâ”€ ÄÃ£ cÃ³ má»™t sá»‘ kiáº¿n thá»©c tháº¿ giá»›i
â””â”€ CÃ³ thá»ƒ Ä‘Æ°á»£c thÃ­ch á»©ng vá»›i cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ!

ThÃªm Classification Head
â”œâ”€ ÄÆ¡n Giáº£n: Láº¥y Ä‘áº¡i diá»‡n token [CLS]
â”œâ”€ ThÃªm: Dense layer (768 â†’ hidden_size)
â”œâ”€ ThÃªm: Classification layer (hidden_size â†’ num_classes)
â””â”€ Káº¿t Quáº£: MÃ´ hÃ¬nh riÃªng theo tÃ¡c vá»¥

Fine-tune trÃªn dá»¯ liá»‡u cá»§a báº¡n
â”œâ”€ ÄÃ¡nh giÃ¡ bá»™ phim: 5,000 vÃ­ dá»¥
â”œâ”€ Thá»i gian huáº¥n luyá»‡n: 2-3 giá» trÃªn GPU Ä‘Æ¡n
â”œâ”€ Káº¿t Quáº£: Äá»™ chÃ­nh xÃ¡c 94-96%
â””â”€ Triá»ƒn khai!
```

---

# **Chiáº¿n LÆ°á»£c Fine-tuning**

## Chiáº¿n LÆ°á»£c 1: Fine-tuning Äáº§y Äá»§

**Cáº­p nháº­t táº¥t cáº£ cÃ¡c tham sá»‘**

```python
# Pseudocode
pretrained_model = load_bert_base()
# KhÃ´ng cÃ³ lá»›p Ä‘Ã³ng bÄƒng!
for epoch in range(3):
    for batch in training_data:
        logits = pretrained_model(batch)
        loss = classification_loss(logits, batch.labels)
        loss.backward()
        optimizer.step()
        
# Káº¿t Quáº£: Hiá»‡u suáº¥t tá»‘t nháº¥t
# Thá»i Gian: 3+ giá» trÃªn GPU
# Bá»™ Nhá»›: YÃªu cáº§u gradient cho táº¥t cáº£ 110M tham sá»‘
```

**Khi NÃ o Sá»­ Dá»¥ng:**
- Táº­p dá»¯ liá»‡u lá»›n (10,000+ vÃ­ dá»¥)
- TÃ­nh toÃ¡n Ä‘á»§ (GPU/TPU)
- TÃ¡c vá»¥ ráº¥t khÃ¡c so vá»›i pre-training

---

## Chiáº¿n LÆ°á»£c 2: ÄÃ³ng BÄƒng Lá»›p

**ÄÃ³ng bÄƒng cÃ¡c lá»›p sá»›m, fine-tune cÃ¡c lá»›p sau**

```python
# ÄÃ³ng bÄƒng 10 lá»›p Ä‘áº§u tiÃªn
for param in model.bert.encoder.layer[:10].parameters():
    param.requires_grad = False

# Fine-tune lá»›p 11-12 vÃ  pháº§n Ä‘áº§u phÃ¢n loáº¡i
for param in model.bert.encoder.layer[10:].parameters():
    param.requires_grad = True

# Káº¿t Quáº£: Nhanh, hiá»‡u suáº¥t tá»‘t
# Thá»i Gian: 1-2 giá» trÃªn GPU
# Bá»™ Nhá»›: Chá»‰ gradient cho 2 lá»›p + head
```

**Khi NÃ o Sá»­ Dá»¥ng:**
- Táº­p dá»¯ liá»‡u trung bÃ¬nh (1,000-10,000 vÃ­ dá»¥)
- TÃ­nh toÃ¡n háº¡n cháº¿
- TÃ¡c vá»¥ hÆ¡i khÃ¡c so vá»›i pre-training

---

## Chiáº¿n LÆ°á»£c 3: Progressive Unfreezing

**Dáº§n dáº§n má»Ÿ khÃ³a cÃ¡c lá»›p tá»« trÃªn xuá»‘ng dÆ°á»›i**

```
Epoch 1: Chá»‰ fine-tune pháº§n Ä‘áº§u phÃ¢n loáº¡i
â”œâ”€ ÄÃ³ng BÄƒng: Táº¥t cáº£ 12 lá»›p
â”œâ”€ Huáº¥n Luyá»‡n: Classification head
â””â”€ Tá»· Lá»‡ Há»c: 1e-3

Epoch 2: Má»Ÿ KhÃ³a Lá»›p Cuá»‘i CÃ¹ng 1
â”œâ”€ ÄÃ³ng BÄƒng: Lá»›p 0-10
â”œâ”€ Huáº¥n Luyá»‡n: Lá»›p 11 + head
â””â”€ Tá»· Lá»‡ Há»c: 1e-4

Epoch 3: Má»Ÿ KhÃ³a 2 Lá»›p Cuá»‘i CÃ¹ng
â”œâ”€ ÄÃ³ng BÄƒng: Lá»›p 0-9
â”œâ”€ Huáº¥n Luyá»‡n: Lá»›p 10-11 + head
â””â”€ Tá»· Lá»‡ Há»c: 1e-4

...Tiáº¿p tá»¥c cho Ä‘áº¿n khi táº¥t cáº£ má»Ÿ khÃ³a

Káº¿t Quáº£: ThÆ°á»ng hiá»‡u suáº¥t tá»‘t nháº¥t!
Thá»i Gian: 5+ giá», nhÆ°ng Ä‘Ã¡ng giÃ¡ cho cÃ¡c tÃ¡c vá»¥ quan trá»ng
```

---

# **Lá»±a Chá»n SiÃªu Tham Sá»‘**

## Tá»· Lá»‡ Há»c

```
HÆ°á»›ng Dáº«n Chung:

Äá»‘i Vá»›i Fine-tuning Äáº§y Äá»§:
â”œâ”€ Báº¯t Äáº§u Vá»›i: 5e-5 (ráº¥t nhá»!)
â”œâ”€ Thá»­: 2e-5, 3e-5, 5e-5, 1e-4
â””â”€ KhÃ´ng Sá»­ Dá»¥ng: Tá»· Lá»‡ Há»c > 1e-4 (quÃªn tháº£m há»a)

Äá»‘i Vá»›i ÄÃ³ng BÄƒng Lá»›p:
â”œâ”€ Lá»›p ÄÃ³ng BÄƒng: KhÃ´ng Tá»· Lá»‡ Há»c (khÃ´ng cáº­p nháº­t)
â”œâ”€ Lá»›p Fine-tuned: 1e-4 - 1e-3
â””â”€ Pháº§n Äáº§u PhÃ¢n Loáº¡i: CÃ³ thá»ƒ sá»­ dá»¥ng cao hÆ¡n má»™t chÃºt

Táº¡i Sao Láº¡i Nhá»?
â”œâ”€ Trá»ng sá»‘ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã£ tá»‘t
â”œâ”€ KhÃ´ng muá»‘n phÃ¡ há»§y kiáº¿n thá»©c
â”œâ”€ Nhá»¯ng thay Ä‘á»•i nhá» an toÃ n hÆ¡n
```

## KÃ­ch ThÆ°á»›c LÃ´

```
TÃ¡c Äá»™ng KÃ­ch ThÆ°á»›c LÃ´:

CÃ¡c LÃ´ Nhá» (8-16):
â”œâ”€ Æ¯u Äiá»ƒm: Hoáº¡t Ä‘á»™ng vá»›i bá»™ nhá»› háº¡n cháº¿
â”œâ”€ NhÆ°á»£c Äiá»ƒm: Gradient á»“n Ã o hÆ¡n, khÃ´ng á»•n Ä‘á»‹nh
â”œâ”€ Sá»­ Dá»¥ng Khi: GPU Nhá» (< 8GB VRAM)

CÃ¡c LÃ´ Trung BÃ¬nh (32):
â”œâ”€ Æ¯u Äiá»ƒm: CÃ¢n Báº±ng Tá»‘t
â”œâ”€ NhÆ°á»£c Äiá»ƒm: Sá»­ Dá»¥ng Bá»™ Nhá»› Trung BÃ¬nh
â”œâ”€ Sá»­ Dá»¥ng Khi: GPU TiÃªu Chuáº©n (8-16GB VRAM)

CÃ¡c LÃ´ Lá»›n (64-256):
â”œâ”€ Æ¯u Äiá»ƒm: ÄÃ o Táº¡o á»”n Äá»‹nh, KhÃ¡i QuÃ¡t HÃ³a Tá»‘t
â”œâ”€ NhÆ°á»£c Äiá»ƒm: YÃªu Cáº§u Nhiá»u Bá»™ Nhá»› Hoáº·c TÃ­ch LÅ©y Gradient
â”œâ”€ Sá»­ Dá»¥ng Khi: TPUs, 24GB+ VRAM, Hoáº·c TÃ­ch LÅ©y Gradient
```

## Sá»‘ LÆ°á»£ng Epoch

```
TÃ¡c Vá»¥ PhÃ¢n Loáº¡i:
â”œâ”€ Äiá»ƒn HÃ¬nh: 3-5 epoch
â”œâ”€ Táº¡i Sao: MÃ´ hÃ¬nh há»™i tá»¥ nhanh
â””â”€ GiÃ¡m SÃ¡t: Dá»«ng sá»›m náº¿u xÃ¡c thá»±c ngá»«ng cáº£i thiá»‡n

TÃ¡c Vá»¥ Táº¡o (T5):
â”œâ”€ Äiá»ƒn HÃ¬nh: 10-20 epoch
â”œâ”€ Táº¡i Sao: TÃ¡c vá»¥ phá»©c táº¡p hÆ¡n, há»™i tá»¥ cháº­m hÆ¡n
â””â”€ GiÃ¡m SÃ¡t: Äiá»ƒm BLEU xÃ¡c thá»±c

NguyÃªn Táº¯c NgÃ³n Tay:
â”œâ”€ Nhiá»u Dá»¯ Liá»‡u HÆ¡n â†’ Ãt Epoch HÆ¡n (vÃ­ dá»¥: 2 epoch cho 100K vÃ­ dá»¥)
â”œâ”€ Ãt Dá»¯ Liá»‡u HÆ¡n â†’ Nhiá»u Epoch HÆ¡n (vÃ­ dá»¥: 5 epoch cho 1K vÃ­ dá»¥)
```

---

# **á»¨ng Dá»¥ng Fine-tuning Phá»• Biáº¿n**

## 1. PhÃ¢n TÃ­ch Cáº£m XÃºc

```
TÃ¡c Vá»¥: PhÃ¢n loáº¡i bÃ i Ä‘Ã¡nh giÃ¡ lÃ  dÆ°Æ¡ng/Ã¢m

Dá»¯ Liá»‡u: 5,000 bÃ i Ä‘Ã¡nh giÃ¡ bá»™ phim cÃ³ nhÃ£n
â”œâ”€ 80%: Huáº¥n luyá»‡n (4,000)
â”œâ”€ 20%: XÃ¡c thá»±c (1,000)

Fine-tuning:
â”œâ”€ MÃ´ HÃ¬nh: BERT-base
â”œâ”€ Epoch: 3
â”œâ”€ KÃ­ch ThÆ°á»›c LÃ´: 32
â”œâ”€ Tá»· Lá»‡ Há»c: 2e-5
â”œâ”€ Thá»i Gian Huáº¥n Luyá»‡n: 30 phÃºt

Káº¿t Quáº£:
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c: 94.2%
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c: 94.5%
â”œâ”€ Gá»£i Láº¡i: 93.9%
â””â”€ Tá»‘t hÆ¡n nhiá»u so vá»›i huáº¥n luyá»‡n tá»« Ä‘áº§u (78%)!
```

## 2. Nháº­n Dáº¡ng TÃªn Thá»±c Thá»ƒ (NER)

```
TÃ¡c Vá»¥: XÃ¡c Ä‘á»‹nh ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c trong vÄƒn báº£n

VÃ­ Dá»¥:
"John Smith works at Google in New York."
NhÃ£n:  [B-PER, I-PER, O, O, B-ORG, O, B-LOC, I-LOC]

ThÃ¡ch Thá»©c: PhÃ¢n loáº¡i cáº¥p token, khÃ´ng pháº£i cáº¥p cÃ¢u

Giáº£i PhÃ¡p:
â”œâ”€ Nháº­n BERT token embeddings
â”œâ”€ ThÃªm lá»›p tuyáº¿n tÃ­nh cho má»—i token
â”œâ”€ Giáº£i mÃ£ báº±ng CRF (TrÆ°á»ng Ngáº«u NhiÃªn CÃ³ Äiá»u Kiá»‡n)

Thá»i Gian Fine-tuning: 1-2 giá»
Hiá»‡u Suáº¥t: Äiá»ƒm F1 92%
```

## 3. Tráº£ Lá»i CÃ¢u Há»i

```
TÃ¡c Vá»¥: TÃ¬m span tráº£ lá»i trong má»™t Ä‘oáº¡n vÄƒn

Äáº§u VÃ o:
CÃ¢u Há»i: "What is the capital of France?"
Äoáº¡n VÄƒn: "Paris is the capital and most populous city of France..."

Äáº§u Ra:
CÃ¢u Tráº£ Lá»i: "Paris"

CÃ¡ch NÃ³ Hoáº¡t Äá»™ng:
â”œâ”€ MÃ£ HÃ³a cÃ¢u há»i + Ä‘oáº¡n vÄƒn cÃ¹ng nhau
â”œâ”€ Äá»‘i Vá»›i Má»—i Token, Dá»± ÄoÃ¡n: "ÄÃ¢y cÃ³ pháº£i lÃ  báº¯t Ä‘áº§u cá»§a cÃ¢u tráº£ lá»i khÃ´ng?"
â”œâ”€ Äá»‘i Vá»›i Má»—i Token, Dá»± ÄoÃ¡n: "ÄÃ¢y cÃ³ pháº£i lÃ  káº¿t thÃºc cá»§a cÃ¢u tráº£ lá»i khÃ´ng?"
â”œâ”€ TrÃ­ch Xuáº¥t Span Giá»¯a Kháº£ NÄƒng Cao Nháº¥t Báº¯t Äáº§u VÃ  Káº¿t ThÃºc

Thá»i Gian Fine-tuning: 2-3 giá»
Hiá»‡u Suáº¥t: Äiá»ƒm F1 89% trÃªn SQuAD
```

## 4. TÃ³m Táº¯t VÄƒn Báº£n

```
TÃ¡c Vá»¥: NÃ©n cÃ¡c tÃ i liá»‡u dÃ i

Sá»­ Dá»¥ng T5:

Äáº§u VÃ o:
"The quick brown fox jumps over the lazy dog. 
 This sentence contains all 26 letters of English alphabet.
 It's often used as a test string in computers."

Fine-tuning Vá»›i T5:
â”œâ”€ Tiá»n Tá»‘: "summarize:"
â”œâ”€ Huáº¥n Luyá»‡n Äáº§y Äá»§: 10-20 epoch
â”œâ”€ KÃ­ch ThÆ°á»›c LÃ´: 16
â”œâ”€ Tá»· Lá»‡ Há»c: 5e-5

Äáº§u Ra:
"A pangram sentence commonly used in computing."

Hiá»‡u Suáº¥t: Äiá»ƒm ROUGE 35-40 (so vá»›i 20-25 baseline)
```

## 5. TÆ°Æ¡ng Tá»± VÄƒn Báº£n Ngá»¯ NghÄ©a

```
TÃ¡c Vá»¥: ÄÃ¡nh GiÃ¡ Hai CÃ¢u Giá»‘ng Nhau Bao NhiÃªu (0-5)

CÃ¢u A: "The cat sat on the mat"
CÃ¢u B: "A feline rested on the rug"
NhÃ£n: 4.5 (ráº¥t tÆ°Æ¡ng tá»±)

Fine-tuning:
â”œâ”€ Láº¥y Token [CLS] Tá»« Cáº£ Hai CÃ¢u
â”œâ”€ MÃ£ HÃ³a CÃ¹ng Nhau
â”œâ”€ Regression Head: Dense Layer Äá»ƒ Xuáº¥t Äiá»ƒm (0-5)
â”œâ”€ Loss: Lá»—i BÃ¬nh PhÆ°Æ¡ng Trung BÃ¬nh (MSE)

Káº¿t Quáº£:
â”œâ”€ TÆ°Æ¡ng Quan Vá»›i PhÃ¡n ÄoÃ¡n Cá»§a Con NgÆ°á»i: 0.88 (ráº¥t tá»‘t!)
â”œâ”€ TÆ°Æ¡ng Quan Spearman: 87%
```

---

# **Nhá»¯ng Cáº¡m Báº«y Fine-tuning Cáº§n TrÃ¡nh**

## âŒ Cáº¡m Báº«y 1: Tá»· Lá»‡ Há»c QuÃ¡ Cao

```
Váº¥n Äá»: MÃ´ hÃ¬nh quÃªn kiáº¿n thá»©c Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c!

VÃ­ Dá»¥:
Tá»· Lá»‡ Há»c: 1e-3
Sau 1 Epoch: Loss = 0.5 (tá»‘t)
Sau 2 Epoch: Loss = 2.0 (khá»§ng khiáº¿p!)
Sau 3 Epoch: Loss = 5.0 (tá»‡ hÆ¡n!)

Táº¡i Sao: CÃ¡c cáº­p nháº­t lá»›n phÃ¡ há»§y trá»ng sá»‘ há»¯u Ã­ch

Giáº£i PhÃ¡p:
â”œâ”€ Sá»­ Dá»¥ng Tá»· Lá»‡ Há»c Nhá» HÆ¡n 1-2 báº­c
â”œâ”€ Báº¯t Äáº§u Vá»›i 2e-5, TÄƒng Chá»‰ Náº¿u Há»™i Tá»¥ QuÃ¡ Cháº­m
â””â”€ GiÃ¡m SÃ¡t: Loss XÃ¡c Thá»±c NÃªn Giáº£m
```

## âŒ Cáº¡m Báº«y 2: QuÃ¡ Ãt Epoch

```
Váº¥n Äá»: MÃ´ HÃ¬nh KhÃ´ng ThÃ­ch á»¨ng Vá»›i TÃ¡c Vá»¥ Má»›i

VÃ­ Dá»¥:
Dá»¯ Liá»‡u: 5,000 vÃ­ dá»¥
Epoch: 1
Hiá»‡u Suáº¥t: Äá»™ chÃ­nh xÃ¡c 88%

CÃ¹ng MÃ´ HÃ¬nh Vá»›i 3 Epoch:
Hiá»‡u Suáº¥t: Äá»™ chÃ­nh xÃ¡c 94%!

Táº¡i Sao: 1 Epoch = Má»—i VÃ­ Dá»¥ Tháº¥y Má»™t Láº§n
         KhÃ´ng Ä‘á»§ Ä‘á»ƒ há»c cÃ¡c máº«u riÃªng theo tÃ¡c vá»¥

Giáº£i PhÃ¡p:
â”œâ”€ Sá»­ Dá»¥ng Ãt Nháº¥t 3-5 Epoch
â”œâ”€ GiÃ¡m SÃ¡t Äá»™ ChÃ­nh XÃ¡c XÃ¡c Thá»±c
â”œâ”€ Dá»«ng Sá»›m Khi Äá»™ ChÃ­nh XÃ¡c XÃ¡c Thá»±c Ngá»«ng Cáº£i Thiá»‡n
```

## âŒ Cáº¡m Báº«y 3: Overfitting TrÃªn Dá»¯ Liá»‡u Nhá»

```
Váº¥n Äá»: MÃ´ HÃ¬nh Ghi Nhá»› Thay VÃ¬ KhÃ¡i QuÃ¡t HÃ³a

VÃ­ Dá»¥:
Dá»¯ Liá»‡u Huáº¥n Luyá»‡n: 100 vÃ­ dá»¥
Äá»™ ChÃ­nh XÃ¡c Huáº¥n Luyá»‡n: 99.8%
Äá»™ ChÃ­nh XÃ¡c Kiá»ƒm Tra: 72.0%

MÃ´ HÃ¬nh Ghi Nhá»›!

Giáº£i PhÃ¡p:
â”œâ”€ ThÃªm Dropout: Bá» 10-20% Neuron Ngáº«u NhiÃªn
â”œâ”€ Dá»«ng Sá»›m: Dá»«ng Khi Äá»™ ChÃ­nh XÃ¡c XÃ¡c Thá»±c Táº¯c
â”œâ”€ TÄƒng CÆ°á»ng Dá»¯ Liá»‡u: Táº¡o ThÃªm VÃ­ Dá»¥ Tá»« CÃ¡c VÃ­ Dá»¥ Hiá»‡n CÃ³
â”œâ”€ Giáº£m KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh: Sá»­ Dá»¥ng BERT-small Thay VÃ¬ BERT-large
```

## âŒ Cáº¡m Báº«y 4: KhÃ´ng Tinh Chá»‰nh SiÃªu Tham Sá»‘

```
Váº¥n Äá»: SiÃªu Tham Sá»‘ Máº·c Äá»‹nh KhÃ´ng Tá»‘i Æ¯u

VÃ­ Dá»¥:
Tá»· Lá»‡ Há»c Máº·c Äá»‹nh (1e-4): Äá»™ chÃ­nh xÃ¡c 92%
Tá»· Lá»‡ Há»c Tinh Chá»‰nh (3e-5): Äá»™ chÃ­nh xÃ¡c 95%!

Giáº£i PhÃ¡p:
â”œâ”€ Thá»­ 3-5 Tá»· Lá»‡ Há»c KhÃ¡c Nhau
â”œâ”€ Thá»­ 2-3 KÃ­ch ThÆ°á»›c LÃ´ KhÃ¡c Nhau
â”œâ”€ Thá»­ 3-5 Epoch
â”œâ”€ Cháº¡y Táº­p XÃ¡c Thá»±c Nhá» TrÃªn Má»—i Káº¿t Há»£p
â””â”€ Chá»n Káº¿t Há»£p Tá»‘t Nháº¥t Cho Huáº¥n Luyá»‡n Äáº§y Äá»§
```

---

# **CÃ¢n Nháº¯c Triá»ƒn Khai**

## KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh vs Tá»‘c Äá»™

```
Äá»‘i Vá»›i Triá»ƒn Khai Sáº£n Xuáº¥t:

BERT-base (110M):
â”œâ”€ KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh: 440 MB
â”œâ”€ Thá»i Gian Suy Luáº­n: 100-150 ms má»—i vÃ­ dá»¥
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c Tá»‘t
â””â”€ CÃ³ thá»ƒ PhÃ¹ Há»£p TrÃªn Háº§u Háº¿t CÃ¡c MÃ¡y Chá»§

BERT-large (340M):
â”œâ”€ KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh: 1.3 GB
â”œâ”€ Thá»i Gian Suy Luáº­n: 300-500 ms má»—i vÃ­ dá»¥
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c Tá»‘t HÆ¡n
â””â”€ Cáº§n Pháº§n Cá»©ng Tá»‘t HÆ¡n

DistilBERT (40M):
â”œâ”€ KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh: 160 MB (60% nhá» hÆ¡n!)
â”œâ”€ Thá»i Gian Suy Luáº­n: 30-50 ms (3x nhanh hÆ¡n!)
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c HÆ¡i Tháº¥p HÆ¡n (97% cá»§a BERT)
â””â”€ HoÃ n Háº£o Cho CÃ¡c Thiáº¿t Bá»‹ Di Äá»™ng/Edge!

CÃ¢y Quyáº¿t Äá»‹nh:
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c Tá»›i Háº¡n? â†’ Sá»­ Dá»¥ng BERT-base Hoáº·c BERT-large
â”œâ”€ Tá»‘c Äá»™ Tá»›i Háº¡n? â†’ Sá»­ Dá»¥ng DistilBERT Hoáº·c LÆ°á»£ng Tá»­ HÃ³a
â”œâ”€ CÃ¢n Báº±ng? â†’ Sá»­ Dá»¥ng BERT-base
```

## Ká»¹ Thuáº­t Tá»‘i Æ¯u HÃ³a

```
TrÆ°á»›c Triá»ƒn Khai:

1. LÆ°á»£ng Tá»­ HÃ³a (8-bit Thay VÃ¬ 32-bit):
   â”œâ”€ KÃ­ch ThÆ°á»›c MÃ´ HÃ¬nh: 1/4 Cá»§a Gá»‘c
   â”œâ”€ Tá»‘c Äá»™ Suy Luáº­n: Nhanh HÆ¡n 2-4 Láº§n
   â””â”€ Äá»™ ChÃ­nh XÃ¡c: 95-99% Cá»§a Äá»™ ChÃ­nh XÃ¡c Äáº§y Äá»§

2. ChÆ°ng Cáº¥t Kiáº¿n Thá»©c:
   â”œâ”€ Huáº¥n Luyá»‡n MÃ´ HÃ¬nh Nhá» TrÃªn Äáº§u Ra Cá»§a MÃ´ HÃ¬nh Lá»›n
   â”œâ”€ KÃ­ch ThÆ°á»›c: Nhá» HÆ¡n 10 Láº§n
   â”œâ”€ Tá»‘c Äá»™: Nhanh HÆ¡n 10 Láº§n
   â””â”€ Äá»™ ChÃ­nh XÃ¡c: 98% Cá»§a MÃ´ HÃ¬nh GiÃ¡o ViÃªn

3. Cáº¯t Tá»‰a:
   â”œâ”€ Loáº¡i Bá» Trá»ng Sá»‘ KhÃ´ng Quan Trá»ng
   â”œâ”€ KÃ­ch ThÆ°á»›c: Nhá» HÆ¡n 30-50%
   â”œâ”€ Tá»‘c Äá»™: Nhanh HÆ¡n 2-3 Láº§n
   â””â”€ Äá»™ ChÃ­nh XÃ¡c: 98% Cá»§a MÃ´ HÃ¬nh Äáº§y Äá»§

4. TorchScript/ONNX:
   â”œâ”€ BiÃªn Dá»‹ch MÃ´ HÃ¬nh Cho Sáº£n Xuáº¥t
   â”œâ”€ Tá»‘c Äá»™: Nhanh HÆ¡n 1.5-2 Láº§n
   â””â”€ KhÃ´ng Phá»¥ Thuá»™c Framework (TensorFlow, PyTorch, v.v.)
```

---

# **VÃ­ Dá»¥ Tháº¿ Giá»›i Thá»±c: XÃ¢y Dá»±ng Bá»™ PhÃ¢n Loáº¡i Cáº£m XÃºc**

## ÄÆ°á»ng á»ng HoÃ n Chá»‰nh

```
BÆ°á»›c 1: Chuáº©n Bá»‹ Dá»¯ Liá»‡u
â”œâ”€ Táº£i: 5,000 bÃ i Ä‘Ã¡nh giÃ¡ bá»™ phim cÃ³ nhÃ£n
â”œâ”€ TÃ¡ch: 80% huáº¥n luyá»‡n, 20% kiá»ƒm tra
â”œâ”€ Tokenize: Chuyá»ƒn Äá»•i ThÃ nh Token BERT
â””â”€ Dataloader: Táº¡o CÃ¡c LÃ´ CÃ³ KÃ­ch ThÆ°á»›c 32

BÆ°á»›c 2: Táº£i MÃ´ HÃ¬nh ÄÆ°á»£c Huáº¥n Luyá»‡n TrÆ°á»›c
â”œâ”€ Táº£i Xuá»‘ng: BERT-base Tá»« HuggingFace
â”œâ”€ ThÃªm Classification Head: 768 â†’ 2 (nhá»‹ phÃ¢n)
â””â”€ Di Chuyá»ƒn Äáº¿n: GPU

BÆ°á»›c 3: Fine-tune
â”œâ”€ Tá»‘i Æ¯u HÃ³a: AdamW (tá»‘t nháº¥t cho transformers)
â”œâ”€ Tá»· Lá»‡ Há»c: 2e-5
â”œâ”€ Epoch: 3
â”œâ”€ VÃ²ng Láº·p Huáº¥n Luyá»‡n: Forward Pass â†’ Loss â†’ Backward â†’ Cáº­p Nháº­t

BÆ°á»›c 4: ÄÃ¡nh GiÃ¡
â”œâ”€ Äá»™ chÃ­nh xÃ¡c xÃ¡c thá»±c: 94.2%
â”œâ”€ Äá»™ chÃ­nh xÃ¡c kiá»ƒm tra: 93.8%
â””â”€ CÃ¡c Sá»‘ Liá»‡u Tá»«ng Lá»›p: Äá»™ ChÃ­nh XÃ¡c 94%, Gá»£i Láº¡i 94%

BÆ°á»›c 5: LÆ°u & Triá»ƒn Khai
â”œâ”€ LÆ°u: model.pt, tokenizer, config.json
â”œâ”€ Kiá»ƒm Tra TrÃªn ÄÃ¡nh GiÃ¡ Má»›i: "Best movie ever!" â†’ Positive âœ“
â””â”€ Triá»ƒn Khai Äáº¿n Sáº£n Xuáº¥t!

Tá»•ng Thá»i Gian: 2-3 giá»
Tá»•ng Chi PhÃ­: ~$2-5 TrÃªn GPU ÄÃ¡m MÃ¢y
Hiá»‡u Suáº¥t: TiÃªn Tiáº¿n Nháº¥t!
```

---

# **Lá»£i Tháº¿ Transfer Learning**

## So SÃ¡nh

```
Baseline (Huáº¥n Luyá»‡n Tá»« Äáº§u):
â”œâ”€ Thá»i Gian Huáº¥n Luyá»‡n: 2-4 tuáº§n
â”œâ”€ Dá»¯ Liá»‡u ÄÆ°á»£c YÃªu Cáº§u: 100,000+ vÃ­ dá»¥
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c: 82-85%
â”œâ”€ Chi PhÃ­: $1000-10000 Trong TÃ­nh ToÃ¡n
â””â”€ KhÃ³: YÃªu Cáº§u ChuyÃªn MÃ´n ML

Transfer Learning (BERT Fine-tuning):
â”œâ”€ Thá»i Gian Huáº¥n Luyá»‡n: 2-3 giá»
â”œâ”€ Dá»¯ Liá»‡u ÄÆ°á»£c YÃªu Cáº§u: 100-1000 vÃ­ dá»¥
â”œâ”€ Äá»™ ChÃ­nh XÃ¡c: 92-95%
â”œâ”€ Chi PhÃ­: $1-10 Trong TÃ­nh ToÃ¡n
â””â”€ KhÃ³: CÃ³ thá»ƒ sá»­ dá»¥ng ThÆ° Viá»‡n HuggingFace!

TÄƒng Tá»‘c Äá»™: 200-300x nhanh hÆ¡n!
Giáº£m Dá»¯ Liá»‡u: Cáº§n 100x Ã­t dá»¯ liá»‡u!
Káº¿t Quáº£ Tá»‘t HÆ¡n: Äá»™ chÃ­nh xÃ¡c cao hÆ¡n 10-15%!
```

---

# **Nhá»¯ng Lá»£i Ãch ChÃ­nh**

âœ… **Transfer learning lÃ  thá»±c táº¿:** Hoáº¡t Ä‘á»™ng tá»‘t cho cÃ¡c váº¥n Ä‘á» thá»±c táº¿
âœ… **Fine-tuning ráº¥t Ä‘Æ¡n giáº£n:** ThÃªm head + huáº¥n luyá»‡n 3-5 epoch
âœ… **Tá»· Lá»‡ Há»c Quan Trá»ng:** Sá»­ Dá»¥ng 1-2 báº­c nhá» hÆ¡n
âœ… **TrÃ¡nh Overfitting:** GiÃ¡m SÃ¡t XÃ¡c Thá»±c, Sá»­ Dá»¥ng Dá»«ng Sá»›m
âœ… **CÃ¢n Nháº¯c Triá»ƒn Khai:** Tá»‘i Æ¯u HÃ³a Cho CÃ¡c RÃ ng Buá»™c Cá»§a Báº¡n
âœ… **HuggingFace LÃ  Báº¡n Cá»§a Báº¡n:** Sá»­ Dá»¥ng CÃ¡c MÃ´ HÃ¬nh ÄÆ°á»£c Huáº¥n Luyá»‡n TrÆ°á»›c + ThÆ° Viá»‡n

---

# **HÃ nh TrÃ¬nh NLP Cá»§a Báº¡n**

Báº¡n ÄÃ£ Há»c:

**Tuáº§n 1-7 (Ná»n Táº£ng):** CÆ¡ Báº£n, Xá»­ LÃ½ VÄƒn Báº£n, Embeddings
**Tuáº§n 8 (Ná»n Táº£ng NLP):** CÆ¡ Báº£n NgÃ´n Ngá»¯, TÃ¬m Kiáº¿m Giá»ng NÃ³i, Seq2seq, ChÃº Ã
**Tuáº§n 9 (Transformers):** Self-Attention, Scaled Dot-Product, CÆ¡ Cháº¿ ChÃº Ã, Triá»ƒn Khai
**Tuáº§n 10 (Transfer Learning):** Transfer Learning, BERT, MLM, T5, Fine-tuning

Tá»« Hiá»ƒu Biáº¿t NgÃ´n Ngá»¯ Äáº¿n XÃ¢y Dá»±ng Há»‡ Thá»‘ng Sáº£n Xuáº¥t!

---

# **CÃ¡c BÆ°á»›c Tiáº¿p Theo**

Äá»ƒ Trá»Ÿ ThÃ nh ChuyÃªn Gia NLP:

1. **XÃ¢y Dá»±ng Dá»± Ãn:** Fine-tune CÃ¡c MÃ´ HÃ¬nh TrÃªn Dá»¯ Liá»‡u Thá»±c
2. **Thá»­ CÃ¡c MÃ´ HÃ¬nh KhÃ¡c Nhau:** RoBERTa, ELECTRA, XLNet, GPT-2
3. **KhÃ¡m PhÃ¡ CÃ¡c Ká»¹ Thuáº­t NÃ¢ng Cao:** Prompt Tuning, In-context Learning, RAG
4. **Cáº­p Nháº­t:** Äá»c CÃ¡c BÃ i Viáº¿t, Theo DÃµi NghiÃªn Cá»©u (Papers with Code, Twitter)
5. **ÄÃ³ng GÃ³p:** CÃ¡c Dá»± Ãn NLP Nguá»“n Má»Ÿ

---

# **TÃ i NguyÃªn**

- **HuggingFace:** https://huggingface.co/ (Models & ThÆ° Viá»‡n)
- **BERT Paper:** "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2018)
- **T5 Paper:** "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (Raffel et al., 2019)
- **Attention Paper:** "Attention Is All You Need" (Vaswani et al., 2017)

---

# **ChÃºc Má»«ng!**

Báº¡n ÄÃ£ HoÃ n ThÃ nh 10 Tuáº§n ÄÃ o Táº¡o ToÃ n Diá»‡n NLP. BÃ¢y Giá» Báº¡n Hiá»ƒu:
- Transformers Hoáº¡t Äá»™ng NhÆ° Tháº¿ NÃ o (ToÃ¡n & Triá»ƒn Khai)
- CÃ¡ch Transfer Learning Cho PhÃ©p PhÃ¡t Triá»ƒn Nhanh ChÃ³ng
- CÃ¡ch Fine-tune CÃ¡c MÃ´ HÃ¬nh Cho CÃ¡c TÃ¡c Vá»¥ Cá»¥ Thá»ƒ
- CÃ¡ch Triá»ƒn Khai CÃ¡c MÃ´ HÃ¬nh Trong Sáº£n Xuáº¥t

**BÃ¢y Giá» HÃ£y XÃ¢y Dá»±ng Má»™t CÃ¡i GÃ¬ ÄÃ³ Tuyá»‡t Vá»i!** ğŸš€

TÆ°Æ¡ng Lai Cá»§a NLP KhÃ´ng Pháº£i LÃ  XÃ¢y Dá»±ng CÃ¡c MÃ´ HÃ¬nh Tá»« Äáº§uâ€”ÄÃ³ LÃ  Ãp Dá»¥ng SÃ¡ng Táº¡o CÃ¡c MÃ´ HÃ¬nh ÄÆ°á»£c Huáº¥n Luyá»‡n TrÆ°á»›c Äá»ƒ Giáº£i Quyáº¿t CÃ¡c Váº¥n Äá» Thá»±c Táº¿. Báº¡n CÃ³ Ná»n Táº£ng Äá»ƒ LÃ m ChÃ­nh XÃ¡c Äiá»u ÄÃ³.

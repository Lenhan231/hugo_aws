---
title: "Day 50 - Fine-tuning & Real-World Applications"
weight: 5
chapter: false
pre: "<b> 1.10.5. </b>"
---

**Date:** 2025-11-14 (Friday)  
**Status:** "Done"  

---

# **Week 10 Complete: From Theory to Production**

You now understand transfer learning, BERT, and T5. This final day covers **how to actually deploy these models.**

---

# **Fine-tuning: The Art and Science**

## The Big Picture

```
Pre-trained Model (e.g., BERT-base)
â”œâ”€ Already knows English
â”œâ”€ Already understands grammar
â”œâ”€ Already has some world knowledge
â””â”€ Can be adapted to specific tasks!

Add Classification Head
â”œâ”€ Simple: Take [CLS] token representation
â”œâ”€ Add: Dense layer (768 â†’ hidden_size)
â”œâ”€ Add: Classification layer (hidden_size â†’ num_classes)
â””â”€ Result: Task-specific model

Fine-tune on your data
â”œâ”€ Movie reviews: 5,000 examples
â”œâ”€ Training time: 2-3 hours on single GPU
â”œâ”€ Results: 94-96% accuracy
â””â”€ Deploy!
```

---

# **Fine-tuning Strategies**

## Strategy 1: Full Fine-tuning

**Update all parameters**

```python
# Pseudocode
pretrained_model = load_bert_base()
# No frozen layers!
for epoch in range(3):
    for batch in training_data:
        logits = pretrained_model(batch)
        loss = classification_loss(logits, batch.labels)
        loss.backward()
        optimizer.step()
        
# Results: Best performance
# Time: 3+ hours on GPU
# Memory: Requires gradients for all 110M params
```

**When to use:**
- Large dataset (10,000+ examples)
- Sufficient compute (GPU/TPU)
- Task very different from pre-training

---

## Strategy 2: Layer Freezing

**Freeze early layers, fine-tune later layers**

```python
# Freeze first 10 layers
for param in model.bert.encoder.layer[:10].parameters():
    param.requires_grad = False

# Fine-tune layers 11-12 and classification head
for param in model.bert.encoder.layer[10:].parameters():
    param.requires_grad = True

# Results: Fast, good performance
# Time: 1-2 hours on GPU
# Memory: Only gradients for 2 layers + head
```

**When to use:**
- Medium dataset (1,000-10,000 examples)
- Limited compute
- Task somewhat different from pre-training

---

## Strategy 3: Progressive Unfreezing

**Gradually unfreeze layers from top to bottom**

```
Epoch 1: Only fine-tune classification head
â”œâ”€ Freeze: All 12 layers
â”œâ”€ Train: Classification head
â””â”€ Learning rate: 1e-3

Epoch 2: Unfreeze last 1 layer
â”œâ”€ Freeze: Layers 0-10
â”œâ”€ Train: Layer 11 + head
â””â”€ Learning rate: 1e-4

Epoch 3: Unfreeze last 2 layers
â”œâ”€ Freeze: Layers 0-9
â”œâ”€ Train: Layers 10-11 + head
â””â”€ Learning rate: 1e-4

...Continue until all unfrozen

Results: Often best performance!
Time: 5+ hours, but worth it for important tasks
```

---

# **Hyperparameter Selection**

## Learning Rate

```
General Guidelines:

For full fine-tuning:
â”œâ”€ Start with: 5e-5 (very small!)
â”œâ”€ Try: 2e-5, 3e-5, 5e-5, 1e-4
â””â”€ Don't use: Learning rates > 1e-4 (catastrophic forgetting)

For layer freezing:
â”œâ”€ Frozen layers: No learning rate (not updated)
â”œâ”€ Fine-tuned layers: 1e-4 - 1e-3
â””â”€ Classification head: Can use slightly higher

Why so small?
â”œâ”€ Pre-trained weights are already good
â”œâ”€ Don't want to destroy the knowledge
â”œâ”€ Small changes are safer
```

## Batch Size

```
Batch Size Impact:

Small batches (8-16):
â”œâ”€ Pros: Works with limited memory
â”œâ”€ Cons: Noisier gradients, unstable
â”œâ”€ Use when: Small GPU (< 8GB VRAM)

Medium batches (32):
â”œâ”€ Pros: Good balance
â”œâ”€ Cons: Moderate memory usage
â”œâ”€ Use when: Standard GPU (8-16GB VRAM)

Large batches (64-256):
â”œâ”€ Pros: Stable training, better generalization
â”œâ”€ Cons: Requires lots of memory or gradient accumulation
â”œâ”€ Use when: TPUs, 24GB+ VRAM, or gradient accumulation
```

## Number of Epochs

```
Classification Tasks:
â”œâ”€ Typical: 3-5 epochs
â”œâ”€ Why: Model converges quickly
â””â”€ Monitor: Stop early if validation stops improving

Generation Tasks (T5):
â”œâ”€ Typical: 10-20 epochs
â”œâ”€ Why: More complex task, slower convergence
â””â”€ Monitor: Validation BLEU score

Rule of thumb:
â”œâ”€ More data â†’ Fewer epochs (e.g., 2 epochs for 100K examples)
â”œâ”€ Less data â†’ More epochs (e.g., 5 epochs for 1K examples)
```

---

# **Common Fine-tuning Applications**

## 1. Sentiment Analysis

```
Task: Classify reviews as positive/negative

Data: 5,000 movie reviews with labels
â”œâ”€ 80%: Training (4,000)
â”œâ”€ 20%: Validation (1,000)

Fine-tuning:
â”œâ”€ Model: BERT-base
â”œâ”€ Epochs: 3
â”œâ”€ Batch size: 32
â”œâ”€ Learning rate: 2e-5
â”œâ”€ Training time: 30 minutes

Results:
â”œâ”€ Accuracy: 94.2%
â”œâ”€ Precision: 94.5%
â”œâ”€ Recall: 93.9%
â””â”€ Much better than training from scratch (78%)!
```

## 2. Named Entity Recognition (NER)

```
Task: Identify people, places, organizations in text

Example:
"John Smith works at Google in New York."
Labels:  [B-PER, I-PER, O, O, B-ORG, O, B-LOC, I-LOC]

Challenge: Token-level classification, not sentence-level

Solution:
â”œâ”€ Get BERT token embeddings
â”œâ”€ Add linear layer for each token
â”œâ”€ Decode with CRF (Conditional Random Field)

Fine-tuning time: 1-2 hours
Performance: 92% F1 score
```

## 3. Question Answering

```
Task: Find the answer span in a passage

Input:
Question: "What is the capital of France?"
Passage: "Paris is the capital and most populous city of France..."

Output:
Answer: "Paris"

How it works:
â”œâ”€ Encode question + passage together
â”œâ”€ For each token, predict: "Is this the start of answer?"
â”œâ”€ For each token, predict: "Is this the end of answer?"
â”œâ”€ Extract span between highest probability start and end

Fine-tuning time: 2-3 hours
Performance: 89% F1 on SQuAD
```

## 4. Text Summarization

```
Task: Condensing long documents

Using T5:

Input:
"The quick brown fox jumps over the lazy dog. 
 This sentence contains all 26 letters of English alphabet.
 It's often used as a test string in computers."

Fine-tuning with T5:
â”œâ”€ Prefix: "summarize:"
â”œâ”€ Full training: 10-20 epochs
â”œâ”€ Batch size: 16
â”œâ”€ Learning rate: 5e-5

Output:
"A pangram sentence commonly used in computing."

Performance: ROUGE score of 35-40 (compared to 20-25 baseline)
```

## 5. Semantic Textual Similarity

```
Task: Rate how similar two sentences are (0-5)

Sentence A: "The cat sat on the mat"
Sentence B: "A feline rested on the rug"
Label: 4.5 (very similar)

Fine-tuning:
â”œâ”€ Take [CLS] token from both sentences
â”œâ”€ Encode together
â”œâ”€ Regression head: Dense layer to output score (0-5)
â”œâ”€ Loss: Mean Squared Error (MSE)

Results:
â”œâ”€ Correlation with human judgments: 0.88 (very good!)
â”œâ”€ Spearman correlation: 87%
```

---

# **Fine-tuning Pitfalls to Avoid**

## âŒ Pitfall 1: Learning Rate Too High

```
Problem: Model forgets pre-trained knowledge!

Example:
Learning rate: 1e-3
After 1 epoch: Loss = 0.5 (good)
After 2 epochs: Loss = 2.0 (terrible!)
After 3 epochs: Loss = 5.0 (worse!)

Why: Large updates destroy useful weights

Solution:
â”œâ”€ Use learning rate 1-2 orders of magnitude smaller
â”œâ”€ Start with 2e-5, increase only if converges too slowly
â””â”€ Monitor: Validation loss should decrease
```

## âŒ Pitfall 2: Too Few Epochs

```
Problem: Model doesn't adapt to new task

Example:
Data: 5,000 examples
Epochs: 1
Performance: 88% accuracy

Same model with 3 epochs:
Performance: 94% accuracy!

Why: 1 epoch = each example seen once
     Not enough to learn task-specific patterns

Solution:
â”œâ”€ Use at least 3-5 epochs
â”œâ”€ Monitor validation accuracy
â”œâ”€ Stop early when validation stops improving
```

## âŒ Pitfall 3: Overfitting on Small Data

```
Problem: Model memorizes instead of generalizing

Example:
Training data: 100 examples
Training accuracy: 99.8%
Test accuracy: 72.0%

Model memorized!

Solutions:
â”œâ”€ Add dropout: Drop 10-20% of neurons randomly
â”œâ”€ Early stopping: Stop when validation accuracy plateaus
â”œâ”€ Data augmentation: Create more examples from existing ones
â”œâ”€ Reduce model size: Use BERT-small instead of BERT-large
```

## âŒ Pitfall 4: Not Doing Hyperparameter Tuning

```
Problem: Default hyperparameters aren't optimal

Example:
Default learning rate (1e-4): 92% accuracy
Tuned learning rate (3e-5): 95% accuracy!

Solution:
â”œâ”€ Try 3-5 different learning rates
â”œâ”€ Try 2-3 different batch sizes
â”œâ”€ Try 3-5 epochs
â”œâ”€ Run small validation set on each combination
â””â”€ Pick best combination for full training
```

---

# **Deployment Considerations**

## Model Size vs Speed

```
For Production Deployment:

BERT-base (110M):
â”œâ”€ Model size: 440 MB
â”œâ”€ Inference time: 100-150 ms per example
â”œâ”€ Good accuracy
â””â”€ Can fit on most servers

BERT-large (340M):
â”œâ”€ Model size: 1.3 GB
â”œâ”€ Inference time: 300-500 ms per example
â”œâ”€ Better accuracy
â””â”€ Need better hardware

DistilBERT (40M):
â”œâ”€ Model size: 160 MB (60% smaller!)
â”œâ”€ Inference time: 30-50 ms (3x faster!)
â”œâ”€ Slightly lower accuracy (97% of BERT)
â””â”€ Perfect for mobile/edge devices!

Decision tree:
â”œâ”€ Accuracy critical? â†’ Use BERT-base or BERT-large
â”œâ”€ Speed critical? â†’ Use DistilBERT or quantization
â”œâ”€ Balanced? â†’ Use BERT-base
```

## Optimization Techniques

```
Before Deployment:

1. Quantization (8-bit instead of 32-bit):
   â”œâ”€ Model size: 1/4 of original
   â”œâ”€ Inference speed: 2-4x faster
   â””â”€ Accuracy: 95-99% of full precision

2. Knowledge Distillation:
   â”œâ”€ Train small model on outputs of large model
   â”œâ”€ Size: 10x smaller
   â”œâ”€ Speed: 10x faster
   â””â”€ Accuracy: 98% of teacher model

3. Pruning:
   â”œâ”€ Remove unimportant weights
   â”œâ”€ Size: 30-50% smaller
   â”œâ”€ Speed: 2-3x faster
   â””â”€ Accuracy: 98% of full model

4. TorchScript/ONNX:
   â”œâ”€ Compile model for production
   â”œâ”€ Speed: 1.5-2x faster
   â””â”€ Framework-agnostic (TensorFlow, PyTorch, etc.)
```

---

# **Real-World Example: Building a Sentiment Classifier**

## Complete Pipeline

```
Step 1: Prepare Data
â”œâ”€ Load: 5,000 movie reviews with labels
â”œâ”€ Split: 80% train, 20% test
â”œâ”€ Tokenize: Convert to BERT tokens
â””â”€ Dataloader: Create batches of 32

Step 2: Load Pre-trained Model
â”œâ”€ Download BERT-base from HuggingFace
â”œâ”€ Add classification head: 768 â†’ 2 (binary)
â””â”€ Move to GPU

Step 3: Fine-tune
â”œâ”€ Optimizer: AdamW (best for transformers)
â”œâ”€ Learning rate: 2e-5
â”œâ”€ Epochs: 3
â”œâ”€ Train loop: Forward pass â†’ Loss â†’ Backward â†’ Update

Step 4: Evaluate
â”œâ”€ Validation accuracy: 94.2%
â”œâ”€ Test accuracy: 93.8%
â””â”€ Per-class metrics: Precision 94%, Recall 94%

Step 5: Save & Deploy
â”œâ”€ Save: model.pt, tokenizer, config.json
â”œâ”€ Test on new review: "Best movie ever!" â†’ Positive âœ“
â””â”€ Deploy to production!

Total time: 2-3 hours
Total cost: ~$2-5 on cloud GPU
Performance: State-of-the-art!
```

---

# **The Transfer Learning Advantage**

## Comparison

```
Baseline (Training from Scratch):
â”œâ”€ Training time: 2-4 weeks
â”œâ”€ Required data: 100,000+ examples
â”œâ”€ Accuracy: 82-85%
â”œâ”€ Cost: $1000-10000 in compute
â””â”€ Difficulty: Requires ML expertise

Transfer Learning (BERT Fine-tuning):
â”œâ”€ Training time: 2-3 hours
â”œâ”€ Required data: 100-1000 examples
â”œâ”€ Accuracy: 92-95%
â”œâ”€ Cost: $1-10 in compute
â””â”€ Difficulty: Can use HuggingFace library!

Speedup: 200-300x faster!
Data reduction: 100x less data needed!
Better results: 10-15% higher accuracy!
```

---

# **Key Takeaways**

âœ… **Transfer learning is practical:** Works great for real problems
âœ… **Fine-tuning is simple:** Add head + train 3-5 epochs
âœ… **Learning rate matters:** Use 1-2 orders of magnitude smaller
âœ… **Avoid overfitting:** Monitor validation, use early stopping
âœ… **Consider deployment:** Optimize for your constraints
âœ… **HuggingFace is your friend:** Use pre-trained models + library

---

# **Your NLP Journey**

You've learned:

**Week 1-7 (Foundation):** Basics, text processing, embeddings
**Week 8 (NLP Foundations):** Linguistic fundamentals, voice search, seq2seq, attention
**Week 9 (Transformers):** Self-attention, scaled dot-product, attention mechanisms, implementation
**Week 10 (Transfer Learning):** Transfer learning, BERT, MLM, T5, fine-tuning

From understanding language to building production systems!

---

# **Next Steps**

To become an NLP expert:

1. **Build projects:** Fine-tune models on real data
2. **Try different models:** RoBERTa, ELECTRA, XLNet, GPT-2
3. **Explore advanced techniques:** Prompt tuning, in-context learning, RAG
4. **Stay updated:** Read papers, follow research (Papers with Code, Twitter)
5. **Contribute:** Open source NLP projects

---

# **Resources**

- **HuggingFace:** https://huggingface.co/ (Models & library)
- **BERT Paper:** "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2018)
- **T5 Paper:** "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (Raffel et al., 2019)
- **Attention Paper:** "Attention Is All You Need" (Vaswani et al., 2017)

---

# **Congratulations!**

You've completed 10 weeks of comprehensive NLP training. You now understand:
- How transformers work (math & implementation)
- How transfer learning enables rapid development
- How to fine-tune models for specific tasks
- How to deploy models in production

**Now go build something amazing!** ðŸš€

The future of NLP is not about building models from scratchâ€”it's about creatively applying pre-trained models to solve real problems. You have the foundation to do exactly that.

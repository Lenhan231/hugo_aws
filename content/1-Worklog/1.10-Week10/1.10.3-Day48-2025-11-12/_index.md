---title:"Day48-BERTPre-trainingObjectives"weight:3chapter:falsepre:"<b>1.10.3.</b>"---**Date:**2025-11-12(Wednesday)**Status:**"Done"---#**HowBERTActuallyLearned**BERTwasn'tjustaluckyguess.Googlescientistsdesignedtwospecificpre-trainingtasksthatmadeitlearnbidirectionalunderstanding.Thesetasksare**brilliantlysimple**.---#**Pre-trainingTask1:MaskedLanguageModeling(MLM)**##TheCoreIdea**Hiderandomwordsandpredictthem.**```Original:"Thequickbrownfoxjumpsoverthelazydog"Masked:"Thequick[MASK]foxjumpsoverthelazydog"Task:Predictwhat[MASK]is!Answer:"brown"Whythisworks:├─ModelmustunderstandcontextfromBOTHsides├─Topredict"brown",needtoknow:│├─Leftcontext:"Thequick"│└─Rightcontext:"foxjumpsoverthelazydog"└─Forcesbidirectionallearning!```##MaskingStrategyNotalltokensaremaskedthesameway:```Whenweselectatokentomask:80%ofthetime:Replacewith[MASK]token├─"Thequick[MASK]fox"├─Modelmustlearntheword└─Normalcase10%ofthetime:Replacewithrandomword├─"Thequickapplefox"(insteadof"brown")├─Modellearnstocorrectnoisyinput└─Robustnesstraining10%ofthetime:Keeptheoriginalword├─"Thequickbrownfox"├─Modellearnstherepresentationanyway└─Helpswithfine-tuningstability```##TheMathematics```Foreachtokenpositioniina15%maskedvocabulary:P(masktokenatpositioni)=0.15Loss_MLM=-log(P(correct_word|context))Formaskedpositionwithword"brown":├─Modeloutputs:[0.02,0.05,0.88,0.03,...](probabilitiesforallwords)│↑positionof"brown"├─Correctword:"brown"(position3)├─Loss=-log(0.88)=0.128└─Thispushesprobabilityhigher!Aggregate:Averagelossoverall15%maskedtokens```##ExampleWalkthrough```Sentence:"Thebankwasrobbedbymaskedmen"Tokens:[The,bank,was,robbed,by,masked,men]Mask15%:[The,bank,[MASK],robbed,by,masked,men]Step1:Tokenizeandembedalltokens├─InputIDs:[101,1996,2924,103,2001,10122,2039,2095,2273,102]├─Tokenembeddings:7x768dimensionalvectors└─Addpositionalembeddings:position0,1,2,3,...Step2:Processthrough12transformerlayers├─Layer1:Self-attentionacrossallpositions├─Layer2:Self-attentionwithupdatedrepresentations├─...└─Layer12:FinalcontextualrepresentationsStep3:Predictmaskedtoken├─Outputforposition2(was):[0.02,0.05,0.88,0.03,...]├─argmax=0.88→WordID3→"was"├─Correct!✓└─Loss:smallStep4:Backpropgradientthroughall12layers└─Updatesallparameterstomakethispredictionlikely```---#**Pre-trainingTask2:NextSentencePrediction(NSP)**##TheCoreIdea**Giventwosentences,predictifthey'reconsecutive.**```SentenceA:"Thebankwasrobbedyesterday."SentenceB:"Thepoliceareinvestigating."Question:Aretheseconsecutivesentences?Answer:Yes(IsNext)✓SentenceA:"Thecatsatonthemat."SentenceB:"Ienjoypizzaforlunch."Question:Aretheseconsecutivesentences?Answer:No(NotNext)✗Whythishelps:├─Modellearnstounderstandrelationshipsbetweensentences├─Importantfortaskslikequestionanswering├─Helpswithsemanticunderstanding└─But:RecentresearchshowsthisisactuallylessimportantthanMLM!```##InputFormat```BERT'sspecialtokensforsentencepairs:[CLS]SentenceA[SEP]SentenceB[SEP]↑↑↑TokenforSeparatorEndofclassificationbetweensequence(usedforNSP)sentencesSegmentIDs:[000000111111]CLSThebankwasrobbedSEPpoliceareinvestigatingTokenA=0(firstsegment)TokenB=1(secondsegment)CLStoken'srepresentationusedforNSPclassification!```##TheTask```NSPisabinaryclassificationproblem:Input:[CLS]...[SEP]...[SEP]↓Modeloutputs:[0.1,0.9]↑↑NotNextIsNextIflabelisIsNext:Loss=-log(0.9)=0.105IflabelisNotNext:Loss=-log(0.1)=2.303Modellearnstodistinguishconsecutivesentences!```---#**CombinedLossFunction**BERTtrainson**both**objectivessimultaneously:```Loss_total=Loss_MLM+Loss_NSPExampleiteration:├─Batchcontains32sentencepairs├─Foreachpair:│├─15%oftokensaremasked→MLMloss│├─Sentenceorderlabeled→NSPloss│└─Bothlossescomputed├─Averagelosses:2.15(MLM)+1.45(NSP)├─Totalloss:3.60└─BackpropupdatesallparametersWhyboth?├─MLMforcesbidirectionalunderstanding├─NSPforcessentence-levelsemantics└─Together:Richlinguisticknowledge!Note:Modernvariants(RoBERTa)dropNSP(FoundthatMLMaloneissufficient!)```---#**BERT'sInputEmbeddings(DeepDive)**##ThreeEmbeddingsCombined```BERT'sembedding=Token+Segment+PositionExample:"Thequickbrownfox"Position:0123TokenEmbedding:├─"The"(ID=1996):[0.2,-0.5,0.1,...]├─"quick"(ID=2522):[0.1,0.3,-0.2,...]├─"brown"(ID=2829):[-0.1,0.2,0.4,...]└─"fox"(ID=4397):[0.3,0.1,-0.3,...]SegmentEmbedding(samesentence):├─Position0:[0.1,0.2,0.3,...](SegmentA)├─Position1:[0.1,0.2,0.3,...](SegmentA)├─Position2:[0.1,0.2,0.3,...](SegmentA)└─Position3:[0.1,0.2,0.3,...](SegmentA)PositionalEmbedding:├─Position0:[0.0,0.5,-0.1,...]├─Position1:[1.0,0.3,0.2,...]├─Position2:[0.5,-0.1,0.5,...]└─Position3:[-0.3,0.6,0.1,...]FinalEmbedding(sumallthree):├─Position0:[0.3,0.7,0.3,...]├─Position1:[1.1,0.8,0.0,...]├─Position2:[0.5,0.3,0.9,...]└─Position3:[0.0,0.7,-0.2,...]```##PositionalEncodingDetails```BERTusessinusoidalpositionalencoding:PE(pos,2i)=sin(pos/10000^(2i/d_model))PE(pos,2i+1)=cos(pos/10000^(2i/d_model))Where:├─pos=positioninsequence(0,1,2,...)├─i=dimensionindex(0,1,2,...)├─d_model=768(hiddensize)Forposition0:├─PE(0,0)=sin(0)=0.0├─PE(0,1)=cos(0)=1.0Forposition1:├─PE(1,0)=sin(1/10000^0)=sin(1)=0.841├─PE(1,1)=cos(1/10000^0)=cos(1)=0.540Pattern:Differentposition→Differentencoding!Can'tchangetwopositionsandkeepencodingsame!```---#**Pre-trainingDataandScale**##Corpus```BERTPre-trainingCorpus:Wikipedia:13GB├─2,500Mwords├─Highquality,diversetopics└─Butrelativelysmall!BookCorpus:Notpubliclyreleasedbut~12GB├─800Mwords├─Bookswrittenbypeople→Goodgrammar└─Longdocuments(goodforlearninglong-rangedependencies)Total:~25GB(3.3billionwordpieces)├─Thisisthesecretsauce!├─Modernmodelsuse100-1000xmoredata└─Moredata=Betterperformance(scalinglaws)```##TrainingDetails```BERTPre-training:Duration:4days(BERT-base)Hardware:16TPUv2devicesBatchsize:256sequences(512tokenseach)=131,072tokensperbatchLearningrate:1e-4withwarmupOptimizer:AdamSteps:~1,000,000stepsComputationalcost:├─16TPUs×4days=64TPU-days├─~$100,000+incompute└─That'swhyweusepre-trainedmodelsnow!Aftertraining:├─Modelcanbeusedforfine-tuning├─Justaddclassificationhead!└─Noneedtotrainfromscratcheveragain```---#**WhyTheseTasksWorkSoWell**##MLMAdvantages✅**Bidirectionallearning:**Mustusebothsides✅**Naturaltask:**Similartohumanreading✅**Flexible:**Canmaskanyproportion✅**Hardenough:**Non-trivialprediction##NSPAdvantages✅**Discourseunderstanding:**Learnssentencerelationships✅**Pairtasks:**HelpswithQAandNLI✅**Classificationformat:**Easytoextractfeatureforclassification##Combined✅**Multi-tasklearning:**Richsupervisionsignal✅**Complementary:**MLM+NSPteachdifferentthings✅**Efficient:**Onetrainingrun,twolearningsignals---#**Comparison:DifferentPre-trainingObjectives**```Word2Vec:├─Task:Predictneighboringwords├─Result:Staticembeddings└─Problem:SameembeddingeverywhereELMo:├─Task:Bidirectionallanguagemodeling├─Result:Context-dependentembeddings└─Problem:StillsequentialprocessingGPT:├─Task:Next-wordprediction(left-to-right)├─Result:Goodforgeneration└─Problem:Can'tseefuturecontextBERT(MLM+NSP):├─Task:Maskedprediction+sentenceorder├─Result:Deepbidirectionalunderstanding└─Advantage:Bestofbothworlds!T5:├─Task:Text-to-textwithmultipleobjectives├─Result:Unifiedframeworkforalltasks└─Advantage:Simplerthantwoseparatelosses```---#**KeyTakeaways**✅**MLMisthestar:**Mostimportantforperformance✅**15%maskingrate:**Empiricallydeterminedtobeoptimal✅**Maskingstrategy:**80-10-10makesmodelrobust✅**NSPhelpsbutissecondary:**Mainlyforpairedtasks✅**Pre-trainingisexpensive:**Butreusable!✅**Simpleideas,powerfulresults:**BERT'sgenius---#**What'sNext**-**Day49:**HowT5scaledthisidea(anddroppedMLM+NSP)-**Day50:**Fine-tuningthesepre-trainedmodelsThepre-trainingobjectivesarethe**foundation**ofmodernNLP.Everythingdownstreamdependsonthesetwosimpletasks!
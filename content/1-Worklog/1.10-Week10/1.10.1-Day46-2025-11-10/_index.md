---title:"Day46-TransferLearningFundamentals"weight:1chapter:falsepre:"<b>1.10.1.</b>"---**Date:**2025-11-10(Monday)**Status:**"Done"---#**TransferLearning:WhyItMatters**Transferlearningis**THEmostimportanttechnique**inmodernNLP.It'swhyamodeltrainedonWikipediacanbefine-tunedindaysinsteadofmonths.---#**TheClassicalApproachvsTransferLearning**##ClassicalMachineLearning```Task:PredictmovieratingfromreviewStep1:Collectlabeleddata(movies+ratings)Step2:InitializemodelrandomlyStep3:Trainfromscratchforweeks/monthsStep4:DeployProblem:Slowandrequireslotsoflabeleddata```##TransferLearningApproach```Task:PredictmovieratingfromreviewStep1:Usepre-trainedmodel(trainedon800GBoftext)Step2:Fine-tuneonyoursmalldataset(100examples)Step3:Trainforhours/daysStep4:Deploywithbetterperformance!Benefit:Fastandworkswithlittledata```---#**TwoFormsofTransferLearning**##1.Feature-BasedTransferLearning**Concept:**Usepre-trainedembeddingsasfixedfeatures**Example:**```Pre-trainingTask:Learnword2vecembeddingsfromWikipediaStep1:TrainWord2VeconWikipedia"Thecatsatonthemat"Learns:embedding["cat"]=[0.2,-0.5,0.8,...]Step2:UsetheseembeddingsforadifferenttaskTask:SentimentanalysisonproductreviewsReview:"Greatproduct!"Embed:"Great"→[0.1,0.3,0.9,...]"product"→[0.5,-0.2,0.1,...]Feedembeddingstosimpleclassifier```**Pros:**-Simpletoimplement-Fastinference(pre-computedembeddings)-Workswithsmalldownstreammodels**Cons:**-Fixedembeddingsdon'tadapttonewdomain-Contextnotcapturedwell(sameembeddingfor"bank"everywhere)---##2.Fine-tuningTransferLearning**Concept:**Usepre-trainedmodelweightsandupdatethemfornewtask**Example:**```Pre-trainingTask:PredictnextwordonWikipedia(LanguageModeling)Step1:Traintransformeron800GBWikipediadataModellearns:linguisticpatterns,worldknowledge,grammarStep2:Fine-tuneondownstreamtaskMovieReview→RatingPredictionOptionA:Updatealllayers├─Lastlayer:Fine-tune(mosttask-specific)├─Middlelayers:Fine-tune(sometask-specific)└─Firstlayers:Fine-tune(generallanguage)OptionB:Freezesomelayers,trainnewones├─Layer1-10:FROZEN(keeppre-trainedweights)├─Layer11-12:Fine-tune└─Newclassificationhead:Trainfromscratch```**Pros:**-Weightsadapttonewtask-Betterperformancethanfeature-based-Fastconvergence**Cons:**-Computationalcost(needtoupdateallparameters)-Riskofcatastrophicforgetting---#**Pre-trainingDatavsDownstreamTask**##TheScaleDifference```Pre-training:800GBoftext├─Wikipedia:13GB├─Books:200GB├─Webcrawl:500+GB└─Totalknowledgelearned:MassiveDownstream:100-10,000examples├─Moviereviews:5,000examples├─Medicaltexts:1,000examples└─Customerfeedback:500examples```**KeyInsight:**Pre-traininghas80,000xmoredatathantypicaldownstreamtasks!Thisexplainswhytransferlearningworkssowell.---#**Pre-trainingObjectives**###Objective1:LanguageModeling**Task:**Predictnextwordgivenpreviouswords```Input:"Thequickbrown"Output:"fox"Loss:Cross-entropybetweenpredictedandactualnextwordExample:"Learningfromdeeplearning.aiislikewatchingthe_____"Modelpredicts:"sunset"Target:"sunset"Loss=0(perfectprediction!)```**Whyitworks:**-Modellearnsgrammar,syntax,semantics-Modellearnscommonpatterns-Modellearnsworldknowledge###Objective2:MaskedLanguageModeling**Task:**Predictmasked(hidden)words```Input:"Thequick[MASK]fox"Output:"brown"ThisiswhatBERTuses!Morecomplex:"The[MASK]brown[MASK]fox[MASK]"Predictallthree:"quick","jumps","here"```---#**AdvantagesofTransferLearning**##1.ReduceTrainingTime```ClassicalTraining:TransferLearning:┌─────────────────────┐┌──────────┐│Pre-training:3mo││Pre-train│(alreadydone!)│(Fromscratch!)││(reuse)│││└──────────┘│Fine-tuning:1mo│┌──────────┐│(Ontaskdata)││Fine-tune│1-7days!│││(ontask)││Total:4MONTHS│└──────────┘└─────────────────────┘Total:~1WEEK```Speedup:**15-30xfaster!**##2.ImprovePredictions```Smalldataset(100examples)withclassicalapproach:├─Modeloverfits(memorizesthe100examples)├─60%accuracyontestset└─EssentiallyuselessSmalldataset(100examples)withtransferlearning:├─Startfrompre-trainedmodel(knowslanguage!)├─Fine-tunecarefully├─85%accuracyontestset└─Muchbetter!Improvement:25%better!```##3.UseSmallerDatasets```Classical:"Youneed10,000labeledexamples"Transfer:"100-1000labeledexamplesisenough!"Why?Pre-trainedmodelalreadyknowslanguageYoujustneedtoteachityourspecifictaskLessdataneeded```---#**Pre-trainingvsFine-tuningData**##LabeledvsUnlabeledData```UnlabeledData(usedinpre-training):"Thequickbrownfoxjumpsover..."(Nolabelsneeded!Justrawtext)LabeledData(usedinfine-tuning):"Thequickbrownfoxjumpsover..."→Positive(label)(Requiresmanualannotation)Insight:Unlabeleddata>>LabeleddatainquantityPre-trainingcanusetrillionsoftokens!```---#**TransferLearningStrategy**###Strategy1:LightFine-tuning```Freezemostlayers,trainonlylastlayer├─Usecase:Largepre-trainedmodel+smalldataset├─Layersfrozen:1-11├─Layerstrained:12+classificationhead├─Trainingtime:1-2days└─Riskofunderfitting:Low```###Strategy2:FullFine-tuning```Updatealllayers├─Usecase:Mediumpre-trainedmodel+mediumdataset├─Layersfrozen:None├─Layerstrained:All12+classificationhead├─Trainingtime:3-7days└─Riskofoverfitting:Medium```###Strategy3:ProgressiveUnfreezing```Unfreezelayersgradually├─Day1:Unfreezelastlayer,train├─Day2:Unfreezelast2layers,train├─Day3:Unfreezelast3layers,train├─...├─Trainingtime:7+days└─Bestperformance:Often!```---#**WhenTransferLearningFails****Domainmismatch:**Pre-trainingonEnglish,fine-tuneonChinese**Catastrophicforgetting:**Updateallweightstooaggressively**Toomuchfine-tuning:**Usetoohighlearningrate**Poorfeatureextraction:**Pre-traintasktoodifferentfromdownstream---#**HistoricalTimeline**```2013:Word2Vec└─Firstsuccessfultransferlearning└─Simplewordembeddings2015:ELMo└─BidirectionalLSTM└─Context-awareembeddings2018:GPT└─Transformer-basedlanguagemodel└─Unidirectional(left-to-right)2018:BERT└─Bidirectionaltransformers!└─Maskedlanguagemodeling└─Hugeperformanceimprovements2019:T5└─Text-to-texttransfertransformer└─Multi-tasklearning└─State-of-the-artoneverything!2020+:GPT-2,GPT-3,RoBERTa,ELECTRA,XLNet...└─Scalinglawsdiscovered└─Biggermodels→Betterperformance```---#**KeyTakeaways****Transferlearningisthestandard:**MostNLPdonetodayusesit**Pre-trainingiscrucial:**800GBofpre-training>>10KBoffine-tuning**Speedadvantageismassive:**Daysinsteadofmonths**Dataefficiency:**Workswithfarlesslabeleddata**Domainadaptation:**Cantransferacrosslanguages,domains,tasks---#**WhatWe'llLearnNext**-**BERT:**Howbidirectionalcontextimprovesunderstanding-**MLM:**Maskedlanguagemodelingpre-training-**T5:**Text-to-textframeworkforallNLPtasks-**Fine-tuning:**PracticaltricksfordownstreamtasksThisfoundation(transferlearning)iswhyonepersoncannowbuildstate-of-the-artNLPsystemsinweeks!
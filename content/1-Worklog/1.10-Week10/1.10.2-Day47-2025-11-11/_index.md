---title:"Day47-BERTArchitecture"weight:2chapter:falsepre:"<b>1.10.2.</b>"---**Date:**2025-11-11(Tuesday)**Status:**"Done"---#**BERT:BidirectionalEncoderRepresentationsfromTransformers**BERTwasthebreakthroughthatchangedNLPforever.PublishedbyGooglein2018,itshowedthat**bidirectionalcontext**mattersmorethananyonethought.---#**What'sDifferentAboutBERT?**##TheEvolution```Word2Vec(2013)â”œâ”€Embedding:"cat"â†’[0.2,-0.5,0.8,...]â”œâ”€Problem:Sameembeddingeverywhere,nocontextâ””â”€Approach:Simple,staticâ†“ELMo(2015)â”œâ”€BiLSTM:ProcessleftANDrightâ”œâ”€Problem:Stillsequential,slowerâ””â”€Approach:Bidirectionalbutnotparallelâ†“GPT(2018)â”œâ”€Transformer:Parallelattention!â”œâ”€Problem:OnlylooksLEFT(left-to-right)â”‚"Thebankwasrobbed"â†’Can'tuse"robbed"tounderstand"bank"â””â”€Approach:Unidirectional,limitedâ†“BERT(2018)â­â”œâ”€Transformer:ParallelANDbidirectional!â”œâ”€Solution:SeesBOTHsidesofeachwordâ”‚"Thebankwasrobbed"â†’Usesboth"The"and"wasrobbed"tounderstand"bank"â””â”€Approach:Fullbidirectionalcontext```---#**BERTArchitectureOverview**##ModelSpecs```BERT-base:â”œâ”€Transformerlayers:12â”œâ”€Hiddensize:768â”œâ”€Attentionheads:12(768/12=64perhead)â”œâ”€Feed-forwardsize:3072â”œâ”€Totalparameters:110Millionâ”œâ”€Trainingtime:4dayson16TPUsâ””â”€Trainingdata:3.3billionwordpiecesfromWikipedia+BookCorpusBERT-large:â”œâ”€Transformerlayers:24â”œâ”€Hiddensize:1024â”œâ”€Totalparameters:340Millionâ””â”€Muchslowerbuthigherperformance```##Encoder-OnlyArchitecture```BERTvsTransformer:Transformer(Full):Inputâ†’Encoder(6layers)â†’Decoder(6layers)â†’Output(Canseeall)(Masked,can'tseefuture)BERT(EncoderOnly):Inputâ†’Encoder(12layers)â†’Output(Fullbidirectionalattention!)KeyDifference:â”œâ”€GPT=Encoder+Decoderbutdecoderisunidirectionalâ”œâ”€BERT=Encoderonly,fullybidirectionalâ””â”€T5=Encoder+Decoder,bothbidirectional```---#**TheSelf-AttentionMechanism(Review)**RememberfromDay43:```Foreachword,createthreevectors:â”œâ”€Query(Q):"WhatinformationdoIneed?"â”œâ”€Key(K):"WhatinformationdoIprovide?"â””â”€Value(V):"What'smyactualrepresentation?"AttentionScore=softmax(QÂ·K^T/âˆšd)Â·VExample:Processing"bank"in"Thebankwasrobbed"Queryfor"bank":[0.1,0.2,0.3,...]Keysforallwords:â”œâ”€"The"key:[0.2,0.1,0.1,...]â†’Score:0.7â”œâ”€"bank"key:[0.1,0.2,0.3,...]â†’Score:0.95(high!self-attention)â”œâ”€"was"key:[0.5,0.2,0.1,...]â†’Score:0.8â””â”€"robbed"key:[0.3,0.3,0.1,...]â†’Score:0.85Notice:"robbed"helpsunderstand"bank"!ThisiswhyBERTisbidirectional.```---#**BERTvsGPT:TheKeyDifference**##AttentionMasking###GPT(Left-to-Right,Decoder-style)```Processing:"Thebankwasrobbed"Position1("The"):â”œâ”€Canattendto:"The"âœ“â”œâ”€Canattendto:"bank"âœ—(masked,future)â”œâ”€Canattendto:"was"âœ—(masked,future)â””â”€Canattendto:"robbed"âœ—(masked,future)Position4("robbed"):â”œâ”€Canattendto:"The"âœ“â”œâ”€Canattendto:"bank"âœ“â”œâ”€Canattendto:"was"âœ“â””â”€Canattendto:"robbed"âœ“Problem:"The"can'tuse"robbed"tounderstanditsmeaning!```###BERT(Bidirectional,Encoder-style)```Processing:"Thebankwasrobbed"Position1("The"):â”œâ”€Canattendto:"The"âœ“â”œâ”€Canattendto:"bank"âœ“â”œâ”€Canattendto:"was"âœ“â””â”€Canattendto:"robbed"âœ“Position4("robbed"):â”œâ”€Canattendto:"The"âœ“â”œâ”€Canattendto:"bank"âœ“â”œâ”€Canattendto:"was"âœ“â””â”€Canattendto:"robbed"âœ“Benefit:"The"CANuse"robbed"tounderstandcontext!BetterrepresentationsforALLwords!```---#**WhyBidirectionalIsBetter**##LanguageUnderstandingExample```Sentence:"Iwenttothebanktodepositmoney"Word:"bank"(financialinstitution)GPTProcessing(Left-to-Right):â”œâ”€Cansee:"Iwenttothe"â”œâ”€Cannotsee:"todepositmoney"(future!)â”œâ”€Contextunderstanding:30%(incomplete)BERTProcessing(Bidirectional):â”œâ”€Cansee:"Iwenttothe"+"todepositmoney"â”œâ”€Fullcontext:100%â”œâ”€Contextunderstanding:90%(muchbetter!)```##WordSenseDisambiguation```"Iwenttothebanktodepositmoney"â†’Financialinstitution"Theriverbankwasveryscenic"â†’Landalongriver"Thebankwasrobbedyesterday"â†’Company/institutionGPT:Seesleftcontextonlyâ”œâ”€"Iwenttothe"(incomplete)â”œâ”€Risk:Wrongsense!BERT:Seesfullcontextâ”œâ”€"Iwenttothe_____todepositmoney"â”œâ”€Automaticallydisambiguates!âœ“```---#**BERT'sInputRepresentation**##TokenEmbeddings```BERTusesWordPiecetokenization:Input:"Thebankwasrobbed"Tokenization:â”œâ”€"The"â†’[101](CLStokenaddstobeginning)â”œâ”€"bank"â†’[1998]â”œâ”€"was"â†’[2003]â”œâ”€"robbed"â†’[6861]â”œâ”€"[SEP]"â†’[102](separatortokenaddstoend)Rawtokens:[101,1998,2003,6861,102]```##ThreeTypesofEmbeddings```BERTcombinesTHREEembeddingtypes:TokenEmbedding:"bank"â†’[0.2,-0.5,0.8,...]â”œâ”€Specificwordrepresentationâ”œâ”€Learnedduringpre-trainingSegmentEmbedding:SentenceAvsSentenceBâ†’[0.1,0.2,...]â”œâ”€Markswhichsentenceeachtokenbelongstoâ”œâ”€Forsentencepairtasks(NextSentencePrediction)PositionalEmbedding:Position2â†’[0.5,0.1,...]â”œâ”€Encodespositioninsequenceâ”œâ”€0,1,2,3,4...FinalEmbedding=Token+Segment+Positional=[0.2,-0.5,0.8,...]+[0.1,0.2,...]+[0.5,0.1,...]=[0.8,-0.2,1.3,...]```---#**Comparison:BERTvsGPTvsOriginalTransformer**```BERTGPTTransformerâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Encoder-DecoderEncoderonlyEncoder+DecoderEncoder+DecoderAttentionTypeBidirectionalUnidirectionalMixedPre-trainTaskMLM+NSPLanguageModelNone(N/A)DirectionBothwaysLefttorightDecodermaskedBestForUnderstandingGenerationTranslationPerformanceBest(90%)Good(85%)Varies(varies%)SpeedMediumFastSlowestParameters110M/340M117M/345M~100M```---#**BERT'sPerformanceonNLPTasks**##GLUEBenchmarkResults```BeforeBERT(State-of-the-art2017):â”œâ”€Sentiment:92.1%accuracyâ”œâ”€TextualSimilarity:80.5%Spearmanâ”œâ”€NamedEntity:91.6%F1â””â”€QuestionAnswering:78.2%F1BERT-base:â”œâ”€Sentiment:94.9%accuracy(+2.8%)â”œâ”€TextualSimilarity:85.8%Spearman(+5.3%)â”œâ”€NamedEntity:96.4%F1(+4.8%)â””â”€QuestionAnswering:84.2%F1(+6.0%)Improvement:4-6%acrosstheboard!Foradeployedsystem,that'shuge!```---#**WhyBERTBecameSoPopular**1.**SimpleFine-tuning:**Addoneclassificationhead,fine-tunealllayers2.**StrongPerformance:**Beatsallpreviousmethodsbylargemargins3.**PretrainedWeights:**Candownloadweights,useimmediately4.**Multilingual:**BERTtrainedon104languages(mBERT)5.**Transferable:**WorksforalmostanyNLPtask---#**BERT'sLimitations**âšï¸**Bidirectional=Can'tgeneratetextdirectly**```Problem:Youcan'tdo"given'Thebank',generatetherest"Reason:Eachtokencanseethefuture!Wouldcollapsethegenerationtask.Solution:Fine-tuneforclassification,notgeneration```âšï¸**Fixedcontextwindow:512tokensmaximum**```Longdocumentsdon'tfit!Solution:HierarchicalBERTorRoBERTa(4096tokens)```âšï¸**Computationallyexpensive**```110Mparameters=440MBjustforweightsInferenceslowerthansmallermodelsSolution:Distillation(DistilBERT,40%parameters,60%speed)```---#**VariantsofBERT**```BERTâ”œâ”€BERT-base:Original,110Mparamsâ”œâ”€BERT-large:Bigger,340Mparamsâ”œâ”€mBERT:104languages,multilingualâ”œâ”€DistilBERT:Distilled,40Mparams,fasterâ””â”€RoBERTa:Improvedtraining,125MparamsAllbasedonsameencoder-onlyarchitecture```---#**KeyTakeaways**âœ…**Bidirectionalcontextmatters:**BERT'sbreakthroughinsightâœ…**Encoder-onlyarchitecture:**Perfectforunderstandingâœ…**Pre-trainedweightsaregold:**Don'ttrainfromscratch!âœ…**Self-attentionwithfullcontext:**Betterrepresentationsâœ…**Simplefine-tuning:**Addhead+train1-3epochs---#**What'sComingNext**-**Day48:**HowBERTlearnedthisunderstanding(MLM+NSP)-**Day49:**T5-Scalinguptoencoder-decoder-**Day50:**Fine-tuningBERTforrealtasksThebeautyofBERT:Bidirectionaltransformersshowedthat**seeingeverythingatonce**isthekeytounderstandinglanguage.
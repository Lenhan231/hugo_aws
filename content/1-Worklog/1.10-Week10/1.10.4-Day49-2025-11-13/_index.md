---title:"Day49-T5:Text-to-TextTransferTransformer"weight:4chapter:falsepre:"<b>1.10.4.</b>"---**Date:**2025-11-13(Thursday)**Status:**"Done"---#**T5:TheUltimateTransferLearningModel**PublishedbyGooglein2019,T5(Text-to-TextTransferTransformer)unifiedALLNLPtasksintooneframework.Thekeyinsight:**EveryNLPtaskcanbephrasedastext-to-text.**---#**TheT5Revolution**##BeforeT5:Task-SpecificModels```SentimentAnalysis:Input:"Ilovethismovie!"Output:Positive(classification)Model:bert-sentiment-specificMachineTranslation:Input:"Ilovethismovie!"Output:"J'adorecefilm!"Model:marianmt-en-frSummarization:Input:"Themoviewasgreat...[1000words]"Output:"Greatmovie."Model:bart-summarizationQuestionAnswering:Input:"Whatisthecapital?"+WikipediapassageOutput:"Paris"(spanextraction)Model:bert-qa-specificProblem:Differentinput/outputtypes,differentmodels!```##AfterT5:UnifiedFramework```SentimentAnalysis:Input:"sentiment:Ilovethismovie!"Output:"positive"MachineTranslation:Input:"translateEnglishtoFrench:Ilovethismovie!"Output:"J'adorecefilm!"Summarization:Input:"summarize:Themoviewasgreat...[1000words]"Output:"Greatmovie."QuestionAnswering:Input:"question:Whatisthecapital?context:Parisis..."Output:"Paris"Solution:ONEmodel,ONEarchitecture,ONEtrainingprocedure!Justchangethetaskprefix!```---#**T5Architecture**##Encoder-DecoderTransformer```T5(BothFullyBidirectional):Input:"translateEnglishtoFrench:Helloworld"↓Encoder(12layersofbidirectionaltransformers)├─Layer1:Self-attentiononalltokens├─Layer2:Self-attentiononalltokens├─...├─Layer12:Self-attentiononalltokens└─Output:Contextualrepresentations↓Decoder(12layers,canattendtoencoder)├─Layer1:Self-attention(masked),Encoder-attention├─Layer2:Self-attention(masked),Encoder-attention├─...├─Layer12:Self-attention(masked),Encoder-attention└─Output:"Bonjourlemonde"KeyDifferencefromBERT:├─BERT:Encoderonly(understands)├─T5:Encoder+Decoder(understandsandgenerates)└─GPT:Decoderonly(generates)```##ModelSizes```T5-small:60millionparameters├─Encoder:6layers,hiddensize512├─Decoder:6layers,hiddensize512└─Fast,smallmemoryfootprintT5-base:220millionparameters├─Encoder:12layers,hiddensize768├─Decoder:12layers,hiddensize768└─GoodbalanceT5-large:770millionparameters├─Encoder:24layers,hiddensize1024├─Decoder:24layers,hiddensize1024└─HighperformanceT5-3BandT5-11B:Evenlarger├─Usedforverychallengingtasks└─Impressiveperformanceoneverything```---#**TheTaskPrefixSystem**ThegeniusofT5:**Simpletaskprefixesguidethemodel**```TaskPrefixExamples:1.Classification(Sentiment):Input:"sentiment:Ilovedthismovie!"Output:"positive"2.Translation:Input:"translateEnglishtoFrench:Hello"Output:"Bonjour"3.Summarization:Input:"summarize:Longdocument...[500words]...end"Output:"Summaryofkeypoints"4.QuestionAnswering:Input:"question:WhowroteHamlet?context:Shakespeare..."Output:"Shakespeare"5.Paraphrasing:Input:"paraphrase:Thecatisonthemat."Output:"Afelinerestsonthefloormat."6.Entailment:Input:"nli:Apersonisridingabike.Amancycles."Output:"entailment"or"neutral"or"contradiction"7.Zero-shotClassification:Input:"znli:Ilikedit.premise:Thespeakerlikedsomething."Output:"entailment"8.MultipleChoice:Input:"multiple_choice:Parisisthecapitalof?(A)France(B)Germany(C)Italy"Output:"A"Whythisworks:├─Samemodellearnsallthesepatterns├─Taskprefixactsasinstruction├─Scalabletoanytext-in/text-outtask└─Elegant!```---#**T5Pre-training:SPANCORRUPTION**T5introducedanewpre-trainingobjective:**SpanCorruption**##HowItWorks```Original:"Thequickbrownfoxjumpsoverthelazydog."SpanCorruptionProcess:Step1:Randomlyselectspanstocorrupt├─Select15%oftokens├─Groupintospans└─Example:[quickbrown]and[lazy]Step2:Replacespanswithsentineltokens├─"The[X]foxjumpsoverthe[Y]dog."├─[X]isauniqueplaceholderforfirstspan├─[Y]isauniqueplaceholderforsecondspanStep3:PredictthemissingspansInput:"The[X]foxjumpsoverthe[Y]dog."Output:"[X]quickbrown[Y]lazy[Z]"(includingstarttoken[Z])Task:Reconstructcorruptedspans!```##WhySpanCorruption>MLM```BERT'sMLM:├─Maskindividualtokens:"The[MASK]brown[MASK]jumps"├─Predicteachindependently├─Problem:Easierthanrealtextcorruption└─TokensarenotcorrelatedT5'sSpanCorruption:├─Corruptword-levelspans:"The[X][Y]foxjumps"├─Generateentirespans├─Advantage:Harder,morerealistic└─Spansarecorrelated(importantforgeneration!)Result:T5isbetteratgenerationtasks!```##MathematicalFormulation```Spancorruptionloss:ForeachcorruptedspanswithlengthL:Loss=-∑(logP(y_i|y_<i,x))Where:├─y_i=tokeniofthecorruptedspan├─y_<i=previouslygeneratedtokens├─x=inputwithcorruptedspans└─SumoveralltokensinspanThisisessentiallylanguagemodelinglossButonspansinsteadofindividualtokens!```---#**T5Pre-trainingData**##TheC4CorpusT5wastrainedon**C4:ColossalCleanCrawledCorpus**```C4Statistics:├─Source:750billionwebdocuments├─Size:800GBoftext├─200billiontokens├─Processed:Deduplicated,filtered,cleaned├─Languages:PrimarilyEnglish(butmultilingualversionsexist)Howitwascreated:├─TakeCommonCrawl(websnapshots)├─Filter:Removebadcontent├─Clean:FixHTML,removeboilerplate├─Deduplicate:Removenear-duplicates├─Result:High-quality800GBcorpus!Comparison:├─BERTpre-training:3.3billionwordpieces(~13GB)├─T5pre-training:200billiontokens(~800GB)└─T5trainedon60xmoredata!```---#**Multi-taskPre-training**T5introducedpre-trainingon**multipletaskssimultaneously**```Duringpre-training,T5seesdiverseobjectives:50%SpanCorruption(maintask):├─"The[X]foxjumpsover[Y]dog"├─Predict:"[X]quickbrown[Y]lazy"50%Task-specificobjectives:├─Translate(15%):"translateentofr:Hello"├─Summarize(15%):"summarize:Article..."├─QA(15%):"question:What...context:..."├─Othertasks(5%):VariousNLPtasksWhy?├─Exposesmodeltotaskdiversityearly├─Bettertransfertodownstreamtasks├─Modellearnsthatsameweightsworkformultiplethings└─Resultsinmoregeneralizablerepresentations```---#**T5ComparisonwithBERT**```T5BERT──────ArchitectureEnc-DecEnconlyMaskTypeSpanTokenPre-trainData800GB(C4)13GB(Wiki+Books)Scale220M-11B110M-340MGenerationExcellentCan'tgenerateUnderstandingGoodExcellentTrainingTime~31days(TPU)~4days(TPU)BestForGenerationUnderstandingFlexibilityText-to-textTask-specificfine-tuningWhichtouse?├─Translation:T5(handlessequencesbetter)├─Classification:BERT(faster,simpler)├─Summarization:T5(generationtask)├─Sentiment:BERT(classificationtask)├─QA:Bothwork,butT5ismoreflexible```---#**T5Performance**##GLUEBenchmark(Understanding)```TaskT5-baseBestPreviousSentiment(SST-2)94.9%92.1%Similarity(STS-B)89.2%87.1%Inference(RTE)93.5%91.0%QuestionAnswering(QNLI)95.1%93.2%Paraphrase(MRPC)89.2%87.1%Averageimprovement:+2-3%```##BLEUScore(Generation)```TaskT5-baseBestPreviousEnglishtoGerman28.423.1EnglishtoFrench41.035.2EnglishtoRomanian34.229.1T5ismuchbetteratgeneration!```---#**T5Variants**```T5├─T5-small(60M)├─T5-base(220M)├─T5-large(770M)├─T5-3B├─T5-11B└─mT5(multilingual,13variants)Eachscalestodifferenttrade-offs:├─Smaller=Faster,lessmemory└─Larger=Betterperformance,moreparameters```---#**WhyT5Matters**✅**Unification:**Onemodel,onearchitecture,manytasks✅**Text-to-textsimplicity:**Don'tdesigntask-specificarchitectures✅**Spancorruption:**Betterpre-trainingobjective✅**Largedata:**800GBcorpusshowsscalingbenefits✅**Flexible:**Cantackleanygenerationtask✅**Strongbaselines:**Bestperformanceonmanybenchmarks---#**KeyTakeaways**1.**TaskPrefixes:**Simplebutpowerfulwaytoguidemodels2.**SpanCorruption:**Betterthantokenmaskingforgeneration3.**LargePre-trainingData:**Moredata=bettertransfer4.**Encoder-Decoder:**Perfectforseq2seqtasks5.**UnifiedFramework:**SimplifiesNLPsystems---#**EvolutionofTransferLearning**```Word2Vec(2013)→ELMo(2015)→BERT(2018)→T5(2019)→GPT-3(2020)→PresentScale:Simple→Medium→Large→VeryLarge→Huge→MassiveArchitecture:Embeddings→BiLSTM→BiTransformer→Enc-Dec→Dec-onlyData:Millions→Billions→13GB→800GB→570GB→Trillions?Performance:Basic→Good→Excellent→Better→Amazing→Superhuman?```---#**Next:Fine-tuning**-**Day50:**HowtotakeT5(orBERT)andfine-tuneforspecifictasksT5isthe**ultimategeneralist**model.Withjustataskprefixand100-1000examples,youcanbuildsystemsthatworkbetterthanspecialistmodelsfrom2yearsago!
[
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Giới thiệu workshop serverless: tạo Lambda (Node.js/Python), nhận tham số đầu vào, tích hợp API Gateway, và triển khai API gợi ý in-app purchase. Nhấn mạnh cách giảm thời gian triển khai bằng Lambda, không cần quản lý server.\nKiến trúc tổng quát Lambda nhận event, xử lý logic, trả JSON. API Gateway làm lớp REST front door, map method → Lambda. Tùy chọn: custom domain/stage cho nhiều môi trường. Kết quả mong đợi 2 hàm Hello World (Node.js/Python) có tham số. 1 endpoint REST (GET/POST) qua API Gateway. 1 hàm logic kinh doanh (gợi ý mua hàng) trả JSON. Thời lượng và cấu trúc Phần 1: Lambda cơ bản, test trong console. Phần 2: API Gateway, mapping input, kiểm thử. Phần 3: Bài tập in-app purchase, dọn dẹp tài nguyên. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-lambda-basics/5.3.1-hello-node/",
	"title": "Lambda Hello World (Node.js)",
	"tags": [],
	"description": "",
	"content": "Tạo hàm Vào Lambda console → Create function → Author from scratch. Name: hello-node, Runtime: Node.js 18.x, Role: AWSLambdaBasicExecutionRole. Handler mẫu: exports.handler = async (event) =\u0026gt; { const name = (event?.queryStringParameters || {}).name || event?.name || \u0026#34;world\u0026#34;; return { statusCode: 200, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ message: `Hello, ${name}` }) }; }; Save và tạo test event (ví dụ {\u0026quot;name\u0026quot;:\u0026quot;Alice\u0026quot;}), xem log CloudWatch. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/",
	"title": "Ngày 01 - Giới thiệu về Điện toán Đám mây",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-08 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Điện toán Đám mây là gì? Hình thức cung cấp tài nguyên CNTT theo nhu cầu thông qua Internet với mô hình trả phí theo mức sử dụng. Lợi ích của Điện toán Đám mây Chỉ trả tiền cho phần tài nguyên thực tế sử dụng, tối ưu chi phí. Tăng tốc phát triển nhờ dịch vụ quản lý và tự động hóa. Mở rộng/thu hẹp tài nguyên linh hoạt theo nhu cầu. Triển khai ứng dụng toàn cầu chỉ trong vài phút. Vì sao chọn AWS? AWS giữ vị trí dẫn đầu thị trường cloud toàn cầu 13 năm liên tiếp (tính đến 2023). Văn hóa, tầm nhìn và sự ám ảnh khách hàng rất khác biệt. Triết lý giá: khách hàng nên trả ít hơn theo thời gian cho cùng một lượng tài nguyên. Mọi Nguyên tắc Lãnh đạo của AWS đều hướng đến giá trị thực cho khách hàng. Bắt đầu với AWS như thế nào? Có nhiều lộ trình học – tự học hoàn toàn khả thi. Đăng ký tài khoản AWS Free Tier để khám phá dịch vụ. Gợi ý nền tảng khóa học: Udemy A Cloud Guru Tham khảo thêm các lộ trình chính thức của AWS: AWS Learning Paths "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/",
	"title": "Ngày 06 - Kiến thức Cơ bản về Amazon VPC",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-15 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Dịch vụ mạng trên AWS Amazon Virtual Private Cloud (VPC) Amazon VPC cho phép triển khai tài nguyên AWS vào một mạng ảo do chính bạn định nghĩa. Mỗi VPC tồn tại trong phạm vi một Region. Khi tạo VPC, cần định nghĩa dải IPv4 CIDR (bắt buộc) và có thể thêm IPv6. Giới hạn mặc định: 5 VPC mỗi Region cho mỗi tài khoản. Thường dùng để tách biệt môi trường Production, Development, Staging. Muốn cô lập hoàn toàn tài nguyên, nên dùng các AWS Account khác nhau thay vì nhiều VPC cùng tài khoản. Subnet Mỗi subnet nằm trong một Availability Zone. CIDR của subnet phải là tập con của CIDR VPC cha. AWS dành sẵn 5 địa chỉ IP trong mỗi subnet: network, broadcast, router, DNS và một địa chỉ dự phòng. Ví dụ các IP được dành trước (10.0.0.0/24):\n10.0.0.0 – Địa chỉ mạng 10.0.0.1 – Router của VPC 10.0.0.2 – Máy chủ DNS 10.0.0.3 – Dành cho sử dụng trong tương lai 10.0.0.255 – Địa chỉ broadcast Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking Basics Tạo VPC → 03-03.1 Tạo Subnet → 03-03.2 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/",
	"title": "Ngày 11 - Kiến thức cơ bản về Amazon EC2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-22 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Compute trên AWS Amazon Elastic Compute Cloud (EC2) Amazon EC2 cung cấp năng lực tính toán có thể co giãn trên cloud, tương tự máy chủ ảo hoặc vật lý. Phù hợp cho các workload như web hosting, ứng dụng, cơ sở dữ liệu, dịch vụ xác thực và nhiều tác vụ máy chủ tổng quát khác. Instance Type\nCấu hình EC2 được xác định bởi instance type, không phải phần cứng tự chọn. Mỗi loại quy định: CPU (Intel, AMD, ARM – Graviton 1/2/3) / GPU Bộ nhớ Kết nối mạng Lưu trữ Nhóm instance tiêu biểu:\nGeneral Purpose: T3, T4g, M5, M6i (cân bằng giữa compute, memory, network). Compute Optimized: C5, C6i, C7g (hiệu năng xử lý cao). Memory Optimized: R5, R6i, X2 (tối ưu cho workload dùng nhiều RAM). Storage Optimized: I3, D2, H1 (tốc độ đọc/ghi tuần tự cao trên storage cục bộ). Accelerated Computing: P4, G5, Inf1 (GPU/FPGA cho ML, đồ họa). Amazon Machine Image (AMI) AMI (Amazon Machine Image) là mẫu chứa cấu hình phần mềm của instance, gồm hệ điều hành, ứng dụng và thiết lập. Các loại AMI: AMI do AWS cung cấp (Amazon Linux, Windows, Ubuntu, v.v.). AMI trên AWS Marketplace. AMI tùy chỉnh do người dùng tạo. Lợi ích của AMI tùy chỉnh\nLaunch và cấu hình instance nhanh hơn. Đơn giản hóa sao lưu và khôi phục. Đảm bảo môi trường nhất quán trên nhiều instance. Thành phần AMI:\nMẫu ổ đĩa gốc (OS và ứng dụng). Quyền launch. Ánh xạ thiết bị khối (block device mapping). Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Tạo tài khoản AWS → 01-01 Cấu hình thiết bị MFA ảo → 01-02 Tạo nhóm Admin và người dùng Admin → 01-03 Cập nhật hỗ trợ xác thực tài khoản → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/",
	"title": "Ngày 16 - Kiến thức cơ bản về Amazon S3",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-29 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Dịch vụ lưu trữ trên AWS Amazon Simple Storage Service (S3) Amazon S3 là dịch vụ lưu trữ đối tượng, cho phép lưu và truy xuất dữ liệu với quy mô gần như không giới hạn, độ sẵn sàng cao, bảo mật mạnh và hiệu năng tốt.\nTính năng cốt lõi của S3 Bucket và Object: Dữ liệu được lưu dưới dạng object trong bucket. Mỗi object tối đa 5 TB. Availability \u0026amp; Durability: Thiết kế đạt 99,99% availability và 99,999999999% (11 số 9) durability. Bảo mật: Nhiều lớp bảo vệ như IAM, bucket policy, ACL, mã hóa. Khả năng mở rộng: Tự động scale dung lượng và throughput mà không giảm hiệu năng. Cấu trúc object S3:\nKey: Tên/đường dẫn object. Value: Dữ liệu object. Version ID: Dùng khi bật versioning. Metadata: Metadata hệ thống và người dùng. Access Control: Quyền truy cập. S3 Access Points Access Point giúp đơn giản hóa việc quản lý truy cập cho dataset dùng chung.\nKiểm soát theo ứng dụng: Mỗi access point có policy riêng. Đơn giản vận hành: Dễ quản lý quyền cho dataset dùng chung nhiều ứng dụng. Kiểm soát mạng: Có thể cấu hình chỉ cho phép truy cập từ các VPC cụ thể. Các lớp lưu trữ S3 Chọn storage class phù hợp với mô hình truy cập và chi phí:\nS3 Standard: Dữ liệu truy cập thường xuyên; availability và hiệu năng cao nhất. S3 Intelligent-Tiering: Tự động di chuyển object giữa các tier để tối ưu chi phí. S3 Standard-IA: Dữ liệu ít truy cập, vẫn truy xuất mili-giây. S3 One Zone-IA: Tương tự Standard-IA nhưng lưu ở một AZ. S3 Glacier Flexible Retrieval: Lưu trữ chi phí thấp, truy xuất phút–giờ. S3 Glacier Deep Archive: Chi phí thấp nhất, truy xuất ~12 giờ. So sánh storage class:\nClass Độ bền Availability Lưu tối thiểu Thời gian truy xuất Standard 11 số 9 99,99% Không Tức thời Intelligent-Tiering 11 số 9 99,9% Không Tức thời Standard-IA 11 số 9 99,9% 30 ngày Tức thời One Zone-IA 11 số 9 99,5% 30 ngày Tức thời Glacier Flexible 11 số 9 99,99% 90 ngày Phút-giờ Glacier Deep Archive 11 số 9 99,99% 180 ngày 12 giờ Hands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Phần 1) Tạo S3 Bucket → 57-2.1 Tải dữ liệu lên → 57-2.2 Bật Static Website → 57-3 Cấu hình Public Access Block → 57-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/",
	"title": "Ngày 21 - Shared Responsibility &amp; Kiến thức IAM cơ bản",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-06 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bảo mật Mô hình Trách nhiệm chia sẻ (Shared Responsibility Model) Trên môi trường cloud, bảo mật là trách nhiệm được chia sẻ giữa nhà cung cấp và khách hàng. Khách hàng cần cấu hình dịch vụ an toàn, áp dụng best practice và triển khai các kiểm soát bảo mật từ lớp hypervisor trở lên (ứng dụng/dữ liệu). Phạm vi trách nhiệm thay đổi tùy mô hình dịch vụ: Dịch vụ cấp hạ tầng Dịch vụ được quản lý một phần Dịch vụ fully-managed Trách nhiệm của AWS (Security OF the Cloud):\nBảo mật vật lý trung tâm dữ liệu. Hạ tầng phần cứng và mạng. Lớp ảo hóa. Vận hành các dịch vụ managed. Trách nhiệm của khách hàng (Security IN the Cloud):\nMã hóa dữ liệu. Cấu hình mạng. Quản lý truy cập. Bảo mật ứng dụng. Vá OS (đối với EC2). AWS Identity and Access Management (IAM) Root Account Có quyền không giới hạn với mọi dịch vụ/tài nguyên AWS và có thể gỡ mọi quyền đã cấp. Best practice: Tạo và dùng IAM Administrator cho tác vụ hằng ngày. Lưu trữ thông tin root an toàn (dual control). Duy trì email và domain của root hợp lệ, được gia hạn. Bật MFA cho tài khoản root. Tổng quan IAM IAM kiểm soát quyền truy cập dịch vụ/tài nguyên trong tài khoản. Principal bao gồm: root user, IAM user, federated user, IAM role, phiên assumed-role, dịch vụ AWS và người dùng ẩn danh. Ghi nhớ: IAM user không phải là tài khoản AWS riêng. Người dùng mới tạo không có quyền mặc định. Cấp quyền bằng cách gắn policy vào user, group hoặc role. Dùng IAM group để quản lý nhiều user (group không lồng nhau được). Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Phần 1) Tạo EC2 Instance → 48-1.1 Tạo S3 Bucket → 48-1.2 Tạo IAM User và Access Key → 48-2.1 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/",
	"title": "Ngày 26 - Kiến thức Cơ bản về Cơ sở dữ liệu",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-13 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Ôn tập khái niệm cơ sở dữ liệu Một cơ sở dữ liệu là tập hợp dữ liệu có tổ chức (hoặc bán cấu trúc) được lưu trên thiết bị lưu trữ, cho phép nhiều người dùng/chương trình truy cập đồng thời cho các mục đích khác nhau. Phiên làm việc (Session) Phiên bắt đầu khi client kết nối tới DBMS và kết thúc khi đóng kết nối. Khóa chính (Primary Key) Primary key dùng để định danh duy nhất từng dòng trong bảng quan hệ. Khóa ngoại (Foreign Key) Foreign key ở một bảng sẽ trỏ đến primary key của bảng khác để tạo quan hệ dữ liệu. Chỉ mục (Index) Index giúp truy vấn nhanh hơn nhưng phải trả giá bằng ghi chép bổ sung và dung lượng lưu trữ để duy trì cấu trúc chỉ mục. Nhờ index, hệ quản trị có thể tìm bản ghi mà không phải quét toàn bộ bảng; có thể định nghĩa trên một hoặc nhiều cột. Các loại chỉ mục phổ biến:\nB-Tree: Cây cân bằng dùng cho hầu hết tình huống. Hash: Truy vấn theo giá trị chính xác cực nhanh. Bitmap: Hợp lý với cột có số lượng giá trị ít. Full-Text: Tối ưu cho tìm kiếm văn bản. Phân vùng (Partitioning) Partitioning chia một bảng lớn thành nhiều phần nhỏ độc lập (partition) và có thể đặt trên nhiều thiết bị lưu trữ khác nhau. Lợi ích: tăng tốc truy vấn, dễ bảo trì và mở rộng. Các kiểu thông dụng: Range (ví dụ theo ngày/tháng) List Hash Composite (kết hợp nhiều tiêu chí) Ví dụ partition theo range:\n-- Partition theo năm CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan Query plan mô tả cách DBMS thực thi câu lệnh SQL (đường truy cập, kiểu join, sort\u0026hellip;). Có hai dạng: Estimated plan: ước lượng trước khi chạy. Actual plan: sinh ra sau khi thực thi. Thành phần chính: table scan, index seek/scan, nested loop, hash/merge join, sort, aggregate, filter. Database Logs Database log ghi lại mọi thay đổi (INSERT/UPDATE/DELETE) và thao tác hệ thống. Thông dụng: log giao dịch, redo, undo, binary log\u0026hellip; Công dụng: khôi phục dữ liệu, đảm bảo toàn vẹn (ACID), hỗ trợ replicate, phân tích hiệu năng. Bộ đệm (Buffer) Buffer pool lưu các trang dữ liệu được đọc từ đĩa nhằm giảm I/O. Chiến lược quản lý: Thu hồi trang: LRU, FIFO, Clock\u0026hellip; Chính sách ghi: ghi ngay hay trì hoãn. Prefetching để sưởi ấm bộ đệm trước. Labs thực hành Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Phần 1) Tạo VPC → 05-2.1 Tạo Security Group cho EC2 → 05-2.2 Tạo Security Group cho RDS → 05-2.3 Tạo DB Subnet Group → 05-2.4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/",
	"title": "Ngày 31 - Khởi động Vertical Slice",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-20 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bối cảnh dự án Ebook Demo – Vertical Slice 0 Mục tiêu: demo trọn vẹn luồng xem chi tiết sách end-to-end trước khi xây toàn bộ hệ thống. Cách tiếp cận: Vertical Slice Architecture để phát triển từng lát cắt hoàn chỉnh thay vì chia theo tầng kỹ thuật. Lợi ích: trình diễn sớm, phát hiện lỗi sớm, tạo nhịp phối hợp giữa frontend/backend. Kiến trúc slice User → Frontend → API → Database → Response → UI Mỗi slice bao gồm UI, contract API, logic backend và dữ liệu giả phục vụ demo. Có thể thay thế từng thành phần độc lập mà không ảnh hưởng toàn bộ hệ thống. Vertical Slice Architecture Nguyên tắc chính Phát triển theo flow người dùng thay vì bóc tách theo tầng. Giữ scope nhỏ để demo nhanh và nhận feedback liên tục. Xác định rõ trách nhiệm của từng slice để dễ mở rộng. Lợi ích Tăng tốc đóng gói giá trị: có thể show cho stakeholder ngay. Giảm rủi ro tích hợp vì mỗi slice tự kiểm chứng được. Cho phép nhiều slice phát triển song song. Insight chính Vertical slice là nền tảng trước khi mở rộng feature khác. Mỗi slice cần checklist rõ (UI hoàn thiện, contract chuẩn, backend trả dữ liệu đúng). Xem slice như “mini product” với vòng đời riêng giúp giữ được chất lượng. Labs thực hành Xác định phạm vi slice 0 (luồng xem chi tiết sách, dữ liệu tối thiểu). Vẽ sơ đồ dòng dữ liệu và ranh giới giữa frontend/backend. Chuẩn hóa checklist demo (contract, mock, UI, backend). "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.1-day36-2025-10-27/",
	"title": "Ngày 36 - Nền Tảng NLP &amp; Ứng Dụng",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-27 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nXử Lý Ngôn Ngữ Tự Nhiên là gì? Xử Lý Ngôn Ngữ Tự Nhiên (NLP) là một lĩnh vực của Trí Tuệ Nhân Tạo tập trung vào việc giúp máy tính có thể hiểu, giải thích, tạo ra và tương tác với ngôn ngữ con người.\nNLP kết hợp ngôn ngữ học tính toán, học máy và học sâu để xử lý dữ liệu văn bản và lời nói quy mô lớn.\nCác Tác Vụ NLP Điển Hình: Phân loại văn bản Phân tích cảm xúc Nhận dạng thực thể (NER) Dịch máy Gắn nhãn từ loại (POS tagging) Nhận dạng giọng nói Các Thành Phần Ngôn Ngữ Cốt Lõi trong NLP Âm Vị Học – Những Âm Thanh của Lời Nói Con Người Âm Vị Học nghiên cứu các đặc tính vật lý của những âm thanh lời nói.\nBa Nhánh Chính: Âm vị học phát âm: cách những âm thanh được tạo ra (lưỡi, môi, dây thanh…) Âm vị học âm học: các đặc tính vật lý của âm thanh (tần số, biên độ, thời lượng) Âm vị học thính giác: con người cảm nhận âm thanh như thế nào Liên Quan NLP: Được sử dụng trong nhận dạng giọng nói, tổng hợp giọng nói (TTS), mô hình âm thanh.\nÂm Vị Học – Hệ Thống Âm Thanh của Ngôn Ngữ Âm Vị Học nghiên cứu cách những âm thanh hoạt động trong một ngôn ngữ cụ thể. Nó đề cập đến phonemes, mô hình nhấn mạnh, các tổ hợp âm thanh được phép.\nLiên Quan NLP: Chuyển đổi grapheme sang phoneme, mô hình phát âm.\nHình Thái Học – Cấu Trúc của Từ Hình Thái Học nghiên cứu cách những từ được hình thành từ những đơn vị nhỏ hơn gọi là morphemes.\nVí Dụ: Tiền tố: un-, re-, pre- Hậu tố: -ing, -ed, -ness Gốc/thân từ: run, happy, form Liên Quan NLP:\nStemming Lemmatization Tokenization Xây dựng từ vựng cho mô hình BoW Các Ứng Dụng NLP Công Cụ Tìm Kiếm Những tìm kiếm hàng ngày của bạn trên các công cụ tìm kiếm được tạo thuận lợi bởi NLP để hiểu truy vấn và xếp hạng kết quả.\nVí Dụ Nhận Dạng Ý Định Tìm Kiếm Khi ai đó tìm kiếm \u0026ldquo;glass coffee tables\u0026rdquo; (bàn cà phê mặt kính), công cụ nhận dạng ý định xác định rằng từ \u0026ldquo;glass\u0026rdquo; có khả năng đề cập đến giá trị của thuộc tính \u0026lsquo;Top Material\u0026rsquo; (Chất Liệu Bề Mặt) trong bàn cà phê. Sau đó, nó chỉ dẫn công cụ tìm kiếm hiển thị danh mục bàn cà phê với thuộc tính \u0026lsquo;Top Material\u0026rsquo; được đặt thành \u0026lsquo;glass\u0026rsquo;.\nQuảng Cáo Trực Tuyến NLP cho phép quảng cáo được nhắm mục tiêu bằng cách phân tích hành vi trực tuyến thông qua nhiều thành phần:\n1. Nhận Dạng Thực Thể (NER) Xác định các yếu tố thông tin được chọn gọi là Thực Thể. Do không có dữ liệu được gắn nhãn, các phương pháp bán giám sát được áp dụng để phát hiện các thực thể cụ thể cho từng trường hợp sử dụng.\n2. Trích Xuất Mối Quan Hệ Một trong những tác vụ NLP cổ điển nhằm trích xuất các mối quan hệ ngữ nghĩa từ các tài liệu văn bản không có cấu trúc hoặc bán cấu trúc.\n3. Nhận Dạng Khoảnh Khắc (MoRec) Cho phép các nhà phân tích hiểu các cuộc thảo luận trên diễn đàn trong giai đoạn khám phá kiến thức bằng cách xử lý văn bản thảo luận không có cấu trúc và trích xuất kiến thức dưới dạng sự kiện. Các sự kiện có thể được xác định và cấu hình tùy thuộc vào trường hợp sử dụng đang được điều tra.\nTrợ Lý Giọng Nói Siri, Alexa và Google Assistant sử dụng NLP để hiểu và trả lời các lệnh giọng nói của bạn.\nDịch Máy Các dịch vụ như Google Translate dựa vào NLP để chuyển đổi văn bản từ một ngôn ngữ sang ngôn ngữ khác.\nChatbot Chatbot dịch vụ khách hàng sử dụng NLP để tương tác với người dùng và cung cấp hỗ trợ.\nTóm Tắt Văn Bản Các thuật toán NLP có thể nén các bài viết dài thành những bản tóm tắt ngắn gọn.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.1-day41-2025-11-03/",
	"title": "Ngày 41 - Vấn Đề RNN &amp; Tại Sao Cần Transformers",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-03 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nVấn Đề với RNNs: Thắt Cổ Chai Xử Lý Tuần Tự RNNs đã thống trị NLP trong những năm qua, nhưng chúng có những hạn chế cơ bản mà transformers giải quyết. Hãy khám phá những vấn đề này.\nVấn Đề 1: Tính Toán Tuần Tự Cách RNNs Xử Lý Thông Tin RNNs phải xử lý đầu vào từng bước một, theo tuần tự:\nVí Dụ Dịch (Tiếng Anh → Tiếng Pháp):\nĐầu vào: \u0026#34;I am happy\u0026#34;\rBước Thời Gian 1: Xử lý \u0026#34;I\u0026#34;\rBước Thời Gian 2: Xử lý \u0026#34;am\u0026#34; Bước Thời Gian 3: Xử lý \u0026#34;happy\u0026#34; Tác Động:\nNếu câu của bạn có 5 từ → cần 5 bước tuần tự Nếu câu của bạn có 1000 từ → cần 1000 bước tuần tự Không thể song song hóa! Phải đợi bước t-1 trước khi tính bước t Tại Sao Điều Này Quan Trọng GPU và TPU hiện đại được thiết kế cho tính toán song song RNNs tuần tự không thể tận dụng song song hóa này Huấn luyện trở nên chậm hơn nhiều so với cần thiết Chuỗi dài hơn = thời gian huấn luyện dài hơn theo hàm mũ Vấn Đề 2: Vanishing Gradient Problem Nguyên Nhân Gốc Rễ Khi RNNs backpropagate qua nhiều time step, gradients được nhân lặp đi lặp lại:\nLuồng Gradient Qua T Bước:\n∂Loss/∂h₀ = ∂Loss/∂hₜ × (∂hₜ/∂hₜ₋₁) × (∂hₜ₋₁/∂hₜ₋₂) × ... × (∂h₁/∂h₀) Nếu mỗi ∂hᵢ/∂hᵢ₋₁ \u0026lt; 1 (điều này thường xảy ra):\nSau T phép nhân: gradient ≈ 0.5^100 ≈ 0 (với T=100) Gradient biến mất thành không Mô hình không thể học các phụ thuộc dài hạn Ví Dụ Cụ Thể Câu: \u0026ldquo;The students who studied hard\u0026hellip; passed the exam\u0026rdquo;\nTừ đầu \u0026ldquo;students\u0026rdquo; cần ảnh hưởng đến dự đoán \u0026ldquo;passed\u0026rdquo; Nhưng gradient đã biến mất khi tiếp cận \u0026ldquo;students\u0026rdquo; Mô hình không học được mối quan hệ này! Giải Pháp Hiện Tại LSTMs và GRUs giúp một chút với gates, nhưng:\nVẫn có vấn đề với chuỗi rất dài (\u0026gt;100-200 từ) Không thể hoàn toàn giải quyết vấn đề Vẫn yêu cầu xử lý tuần tự Vấn Đề 3: Thắt Cổ Chai Thông Tin Vấn Đề Nén Dữ Liệu Kiến Trúc Sequence-to-Sequence:\nEncoder: Word₁ → h₁ → h₂ → h₃ → hₜ (hidden state cuối cùng)\rDecoder: hₜ → Word₁\u0026#39; → Word₂\u0026#39; → Word₃\u0026#39; → ... Thắt Cổ Chai: Tất cả thông tin từ toàn bộ chuỗi đầu vào được nén vào một vector duy nhất hₜ (hidden state cuối cùng).\nTại Sao Điều Này Không Hoạt Động Ví Dụ Câu: \u0026ldquo;The government of the United States of America announced\u0026hellip;\u0026rdquo;\nKhi mã hóa câu 8 từ này:\nTừ đầu tiên \u0026ldquo;The\u0026rdquo; được xử lý Thông tin chảy qua các state: h₁ → h₂ → h₃ → \u0026hellip; → h₈ Bởi h₈, thông tin về \u0026ldquo;The\u0026rdquo; đã bị pha loãng/mất Chỉ h₈ được truyền đến decoder Decoder có thông tin ngữ cảnh hạn chế về các từ đầu Hậu Quả Chuỗi dài mất thông tin Ngữ cảnh quan trọng ở đầu bị pha loãng Mô hình gặp khó khăn với tài liệu dài Chất lượng dịch giảm cho các câu dài Tóm Tắt: Tại Sao RNNs Có Những Vấn Đề Cơ Bản Vấn Đề Tác Động Giải Pháp Hiện Tại Xử Lý Tuần Tự Không thể song song hóa, đào tạo chậm N/A - Cơ bản của thiết kế RNN Vanishing Gradients Không thể học phụ thuộc dài hạn LSTM/GRU gates (sửa chữa một phần) Thắt Cổ Chai Thông Tin Thông tin sớm bị mất Cơ chế Attention (sửa chữa một phần) Giải Pháp Transformer: \u0026ldquo;Attention is All You Need\u0026rdquo; Giới thiệu năm 2017 bởi các nhà nghiên cứu Google (Vaswani et al.), transformers hoàn toàn thay thế RNNs bằng các cơ chế attention.\nNhững Khác Biệt Chính Khía Cạnh RNNs Transformers Xử Lý Tuần tự (từng từ một lần) Song Song (tất cả từ cùng lúc) Phụ Thuộc Mỗi từ phụ thuộc vào state trước Tất cả từ attend tới tất cả từ Tốc Độ Huấn Luyện Chậm (tuần tự) Nhanh (song song) Chuỗi Dài Vanishing gradients Không thắt cổ chai tuần tự Phụ Thuộc Dài Hạn Khó khăn Dễ dàng (attention trực tiếp) Cách Transformers Hoạt Động (Tóm Tắt Ngắn) Không RNN: Loại bỏ hoàn toàn các hidden state tuần tự Attention Thuần: Để mỗi từ \u0026ldquo;attend tới\u0026rdquo; tất cả các từ khác Positional Encoding: Thêm thông tin vị trí vì chúng ta không có thứ tự tuần tự Xử Lý Song Song: Xử lý toàn bộ chuỗi cùng lúc Ví Dụ:\nCâu: \u0026#34;I am happy\u0026#34;\rThay vì:\rBước 1: Xử lý \u0026#34;I\u0026#34; → h₁\rBước 2: Xử lý \u0026#34;am\u0026#34; với h₁ → h₂\rBước 3: Xử lý \u0026#34;happy\u0026#34; với h₂ → h₃\rTransformer Làm:\rSong Song: \u0026#34;I\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rSong Song: \u0026#34;am\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rSong Song: \u0026#34;happy\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rTất cả cùng lúc! Tại Sao Mọi Người Nói Về Transformers Tốc Độ: Có thể huấn luyện nhanh hơn trên GPU/TPU (song parallel) Khả Năng Mở Rộng: Có thể xử lý chuỗi rất dài (không thắt cổ chai) Dài Hạn: Attention trực tiếp giải quyết các vấn đề gradient Tính Linh Hoạt: Hoạt động cho dịch, phân loại, QA, tóm tắt, chatbots\u0026hellip; Hiệu Năng: Đạt kết quả tiên tiến trên gần như mọi tác vụ NLP\nỨng Dụng của Transformers Transformers được sử dụng cho:\nDịch (Neural Machine Translation) - Chất lượng cao, nhanh Tóm Tắt Văn Bản (Abstractive \u0026amp; Extractive) Named Entity Recognition (NER) - Nhận dạng thực thể tốt hơn Hỏi Đáp - Hiểu ngữ cảnh tốt hơn Chatbots \u0026amp; Trợ Lý Thoại Phân Tích Cảm Xúc - Hiểu cảm xúc/ý kiến Tự Động Hoàn Thành - Đề xuất thông minh Phân Loại - Phân loại văn bản vào các danh mục Trí Tuệ Thị Trường - Phân tích cảm xúc thị trường Các Mô Hình Transformer Tiên Tiến Nhất GPT-2 (Generative Pre-trained Transformer) Tạo bởi: OpenAI Loại: Decoder-only transformer Chuyên môn: Tạo văn bản Nổi tiếng vì: Tạo văn bản giống con người (thậm chí đánh lừa các nhà báo năm 2019!) BERT (Bidirectional Encoder Representations from Transformers) Tạo bởi: Google AI Loại: Encoder-only transformer Chuyên môn: Hiểu văn bản \u0026amp; đại diện Sử dụng: Phân loại, NER, QA T5 (Text-to-Text Transfer Transformer) Tạo bởi: Google Loại: Full encoder-decoder (giống transformer gốc) Chuyên môn: Học đa tác vụ Rất linh hoạt: Một mô hình xử lý dịch, phân loại, QA, tóm tắt, hồi quy "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.1-day46-2025-11-10/",
	"title": "Ngày 46 - Nền tảng Transfer Learning",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-10 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTransfer Learning: Vì sao quan trọng Huấn luyện truyền thống bắt đầu từ đầu cho mọi tác vụ. Transfer learning tái dùng mô hình đã tiền huấn luyện để hội tụ nhanh hơn, chính xác hơn và cần ít dữ liệu gán nhãn hơn.\nSo sánh quy trình Truyền thống: dữ liệu -\u0026gt; mô hình khởi tạo ngẫu nhiên -\u0026gt; train -\u0026gt; dự đoán Transfer: tiền huấn luyện trên corpora lớn -\u0026gt; tái dùng trọng số -\u0026gt; fine-tune trên tác vụ đích -\u0026gt; dự đoán [Dữ liệu lớn (không/ có nhãn)] --pre-train--\u0026gt; [Trọng số gốc] \\ fine-tune trên dữ liệu tác vụ --\u0026gt; deploy Hai cách tiếp cận Feature-based: dùng embedding đã huấn luyện như đặc trưng cố định; train head mới. Fine-tuning: cập nhật (một phần) trọng số gốc trên dữ liệu downstream. Lợi ích Hội tụ nhanh nhờ khởi động nóng. Dự đoán tốt hơn từ biểu diễn giàu ngữ cảnh. Cần ít dữ liệu gán nhãn; tận dụng tiền huấn luyện trên dữ liệu thô. Lưu ý chính Lệch miền: chọn dữ liệu tiền huấn luyện gần với miền đích nếu có thể. Quên thảm họa: dùng learning rate nhỏ hoặc đóng băng lớp đầu. Đánh giá: so sánh đóng băng vs. fine-tune toàn bộ để tránh overfit. Việc thực hành hôm nay Phác quy trình transfer cho bài QA của bạn (dữ liệu, model base, head, metric). Quyết định lớp nào đóng băng và lớp nào fine-tune. Lên plan thử nhanh: feature-based vs. fine-tune và so sánh kết quả. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/1.11.1-day51-2025-11-17/",
	"title": "Ngày 51 - Tổng quan Lambda Managed Instances",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-17 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nVì sao dùng LMI LMI giữ trải nghiệm Lambda nhưng cho chọn họ máy EC2, tận dụng SP/RI, bỏ cold start và cho phép multi-concurrency trên mỗi instance.\nKhi nên dùng Lưu lượng cao, ổn định cần chi phí dự báo Nhu cầu compute/memory/network đặc thù Muốn áp dụng pricing EC2 cho hàm Lambda Khi ở lại Lambda mặc định Traffic đột biến, khó dự đoán Hàm ngắn, thưa cần scale-to-zero Lợi ích nhanh Giữ cách đóng gói/runtimes Lambda Không cold start, công suất ổn định Kiểm soát chi phí qua SP/RI Ghi chú từ CNS382 Instance do AWS quản lý: thấy nhưng không SSH/chỉnh Lambda lo vòng đời, patch, routing Multi-concurrency thay đổi profile giá/hiệu năng "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/1.12.1-day56-2025-11-24/",
	"title": "Ngày 56 - Dòng Nova &amp; Agent",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-24 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nĐiểm nhấn Nova 2 Sonic: speech-to-speech đa ngôn ngữ, giữ ngữ cảnh, điều khiển giọng nói Lite: reasoning nhanh/giá rẻ với context dài Omni (preview): đa phương thức (text/image/video/speech input) xuất text/image Forge: chương trình huấn luyện frontier model tùy biến trên hạ tầng Nova Agent cho UI \u0026amp; workflow Nova Act GA: agent trình duyệt (form, search, booking) hướng tới \u0026gt;90% độ tin cậy Việc cần làm Chọn một modality Nova để pilot (speech, đa phương thức, hoặc Lite) Định bộ đánh giá (độ trễ, chất lượng, chi phí) và kiểm tra an toàn/policy Kiểm tra quota/khu vực khả dụng "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/4.1-event1/",
	"title": "Sự Kiện 1 - Vietnam Cloud Day 2025",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Ngày \u0026amp; Giờ: Thứ Năm, 18 tháng 9 năm 2025 | 9:00 – 17:00 VNT\nĐịa Điểm: Amazon Web Services Vietnam, Tầng 36, 2 Đường Hai Triều, Phường Sài Gòn, Thành phố Hồ Chí Minh\nTrạng Thái Đăng Ký: Đã Đóng\nTổng Quan Sự Kiện Vietnam Cloud Day 2025 là một sự kiện AWS toàn diện được thiết kế cho các nhà xây dựng và lãnh đạo doanh nghiệp, với các bài phát biểu chính từ các nhà lãnh đạo chính phủ, các giám đốc điều hành AWS và các lãnh đạo ngành. Sự kiện này giới thiệu các dịch vụ mới nhất của AWS và các sáng kiến chiến lược cho AI và hiện đại hóa đám mây thông qua hai track chính: track phát sóng trực tiếp và các phiên breakout trực tiếp.\nChương Trình Track Phát Sóng Trực Tiếp Giờ (VNT) Phiên Diễn Giả 7:35 - 9:00 Đăng Ký - 9:00 - 9:20 Khai Mạc Nhà Lãnh Đạo Chính Phủ 9:20 - 9:40 Bài Phát Biểu Chính Eric Yeo, Giám Đốc Quốc Gia, Việt Nam, Campuchia, Lào \u0026amp; Myanmar, AWS 9:40 - 10:00 Bài Phát Biểu Khách Hàng 1 Tiến Sĩ Jens Lottner, CEO, Techcombank 10:00 - 10:20 Bài Phát Biểu Khách Hàng 2 Bà Trang Phùng, CEO \u0026amp; Đồng Sáng Lập, U2U Network 10:20 - 10:50 Bài Phát Biểu AWS Jaime Valles, Phó Chủ Tịch, Giám Đốc Tổng Quát Khu Vực Châu Á Thái Bình Dương và Nhật Bản, AWS 11:00 – 11:40 Thảo Luận Bảng: Điều Hướng Cuộc Cách Mạng GenAI Điều Phối Viên: Jeff Johnson, Giám Đốc Quản Lý, ASEAN, AWS Chi Tiết Thảo Luận Bảng: Điều Hướng Cuộc Cách Mạng GenAI: Chiến Lược Lãnh Đạo Cấp Cao Thảo luận này đã đi sâu vào cách các nhà lãnh đạo cấp cao có thể điều hướng hiệu quả các tổ chức của họ thông qua những tiến bộ nhanh chóng trong AI tạo sinh. Các thành viên bảng điều hành đã chia sẻ những hiểu biết và hành trình cá nhân của họ về:\nXây dựng văn hóa đổi mới Căn chỉnh các sáng kiến AI với các mục tiêu kinh doanh Quản lý những thay đổi tổ chức kèm theo tích hợp AI Các Thành Viên Bảng:\nVũ Văn, Đồng Sáng Lập \u0026amp; CEO, ELSA Corp Nguyễn Hòa Bình, Chủ Tịch, Nexttech Group Dieter Botha, CEO, TymeX Các Track Breakout (Phiên Trực Tiếp) Track 1: Tập Trung vào AI \u0026amp; Phân Tích Giờ (VNT) Phiên Diễn Giả 13:15 - 13:30 Khai Mạc \u0026amp; Giới Thiệu Track Jun Kai Loke, Chuyên Gia AI/ML SA, AWS 13:30 - 14:00 Xây Dựng Nền Tảng Dữ Liệu Thống Nhất trên AWS cho Khối Lượng Công Việc AI và Phân Tích Kiên Nguyễn, Kiến Trúc Sư Giải Pháp, AWS 14:00 - 14:30 Xây Dựng Tương Lai: Áp Dụng Gen AI và Lộ Trình trên AWS Jun Kai Loke, Chuyên Gia AI/ML SA, AWS; Tamelly Lim, Chuyên Gia Lưu Trữ SA, AWS 14:30 - 15:00 Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) - Định Hình Tương Lai của Triển Khai Phần Mềm Bình Trần, Kiến Trúc Sư Giải Pháp Cao Cấp, AWS 15:00 - 15:30 Giải Lao Uống Trà - 15:30 - 16:00 Bảo Mật Các Ứng Dụng AI Tạo Sinh với AWS: Nguyên Tắc Cơ Bản và Thực Tiễn Tốt Nhất Taiki Dang, Kiến Trúc Sư Giải Pháp, AWS 16:00 - 16:30 Vượt Ra Ngoài Tự Động Hóa: Các Tác Nhân AI là Bộ Nhân Năng Suất Tối Ưu của Bạn Michael Armentano, Chuyên Gia GTM Toàn Cầu Chính, AWS Chi Tiết Phiên Xây Dựng Nền Tảng Dữ Liệu Thống Nhất trên AWS cho Khối Lượng Công Việc AI và Phân Tích\nPhiên này đã đi sâu vào các chiến lược và thực tiễn tốt nhất để xây dựng nền tảng dữ liệu thống nhất, có thể mở rộng trên AWS. Các tham gia viên đã học cách tận dụng các dịch vụ AWS để tạo cơ sở hạ tầng dữ liệu mạnh mẽ có thể xử lý các yêu cầu của các ứng dụng hướng dữ liệu hiện đại. Các chủ đề chính được đề cập:\nNhập dữ liệu, lưu trữ, xử lý và quản trị Quản lý và sử dụng dữ liệu hiệu quả cho phân tích nâng cao và các sáng kiến AI Xây Dựng Tương Lai: Áp Dụng Gen AI và Lộ Trình trên AWS\nAWS đã trình bày tầm nhìn toàn diện, các xu hướng mới nổi và lộ trình chiến lược cho việc áp dụng các công nghệ AI Tạo Sinh (GenAI). Cuộc thảo luận đã bao gồm các dịch vụ AWS chính và các sáng kiến được thiết kế để trao quyền cho các tổ chức trong việc tận dụng GenAI để thúc đẩy đổi mới và hiệu quả.\nVòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) - Định Hình Tương Lai của Triển Khai Phần Mềm\nVòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) là một cách tiếp cận biến đổi, tập trung vào AI, định hình lại tương lai của triển khai phần mềm bằng cách nhúng đầy đủ AI như một cộng tác viên trung tâm trong toàn bộ vòng đời phát triển phần mềm. Không giống như các phương pháp truyền thống nhúng AI như một trợ lý cho các quy trình do con người điều khiển hiện có, AI-DLC tích hợp thực thi do AI điều khiển với giám sát của con người và hợp tác nhóm động để:\nCải thiện đáng kể tốc độ phát triển phần mềm Nâng cao chất lượng mã Thúc đẩy đổi mới Bảo Mật Các Ứng Dụng AI Tạo Sinh với AWS: Nguyên Tắc Cơ Bản và Thực Tiễn Tốt Nhất\nPhiên này đã khám phá các thách thức bảo mật độc đáo ở mỗi lớp của ngăn xếp AI tạo sinh—cơ sở hạ tầng, mô hình và ứng dụng. Các tham gia viên đã học cách AWS tích hợp các biện pháp bảo mật tích hợp sẵn như:\nMã hóa Kiến trúc không tin tưởng Giám sát liên tục Kiểm soát truy cập chi tiết Các biện pháp này bảo vệ khối lượng công việc AI tạo sinh, đảm bảo tính bảo mật và toàn vẹn dữ liệu trong suốt vòng đời AI.\nVượt Ra Ngoài Tự Động Hóa: Các Tác Nhân AI là Bộ Nhân Năng Suất Tối Ưu của Bạn\nPhiên này đã trình bày một sự thay đổi mô hình trong đó các tác nhân AI không chỉ là công cụ, mà là những đối tác thông minh chủ động thúc đẩy kinh doanh phát triển. Các khái niệm chính bao gồm:\nCác tác nhân AI học hỏi, thích ứng và thực thi các tác vụ phức tạp một cách tự chủ Chuyển đổi hoạt động từ các quy trình thủ công sang hiệu quả chưa từng có Nhân năng suất theo cấp số nhân thông qua sức mạnh của AI Track 2: Tập Trung vào Di Chuyển Đám Mây \u0026amp; Hiện Đại Hóa Giờ (VNT) Phiên Diễn Giả 13:15 - 13:30 Khai Mạc \u0026amp; Giới Thiệu Track Hùng Nguyễn Gia, Trưởng Kiến Trúc Sư Giải Pháp, AWS 13:30 - 14:00 Hoàn Thành Di Chuyển và Hiện Đại Hóa Quy Mô Lớn với AWS Sơn Đỗ, Quản Lý Tài Khoản Kỹ Thuật, AWS; Nguyễn Văn Hải, Giám Đốc Kỹ Thuật Phần Mềm, Techcombank 14:00 - 14:30 Hiện Đại Hóa Ứng Dụng với Công Cụ Được Hỗ Trợ bởi AI Tạo Sinh Phúc Nguyễn, Kiến Trúc Sư Giải Pháp, AWS; Alex Trần, Giám Đốc AI, OCB 14:30 - 15:00 Thảo Luận Bảng: Hiện Đại Hóa Ứng Dụng - Tăng Tốc Độ Chuyển Đổi Kinh Doanh Điều Phối Viên: Hùng Nguyễn Gia, Trưởng Kiến Trúc Sư Giải Pháp, AWS 15:00 - 15:30 Giải Lao - 15:30 - 16:00 Chuyển Đổi VMware với Hiện Đại Hóa Đám Mây Do AI Điều Khiển Hùng Hoàng, Quản Lý Giải Pháp Khách Hàng, AWS 16:00 - 16:30 Bảo Mật AWS Ở Quy Mô: Từ Phát Triển đến Sản Xuất Taiki Dang, Kiến Trúc Sư Giải Pháp, AWS Chi Tiết Phiên Hoàn Thành Di Chuyển và Hiện Đại Hóa Quy Mô Lớn với AWS\nPhiên này tập trung vào những bài học quý báu từ hàng nghìn doanh nghiệp đã di chuyển và hiện đại hóa khối lượng công việc tại chỗ của họ với AWS. Các chủ đề bao gồm:\nCác mô hình tư duy được chứng minh và thực tiễn kỹ thuật tốt nhất Các con đường hiện đại hóa giúp các tổ chức hiện đại hóa trong khi di chuyển Các bộ tăng tốc di chuyển AWS và các công cụ di chuyển và hiện đại hóa mới nhất Nghiên cứu trường hợp cho thấy cách các tổ chức đã thiết lập nền tảng mạnh mẽ và lộ trình chiến lược tận dụng các khả năng đám mây AWS để đạt được các mục tiêu chuyển đổi kỹ thuật số Hiện Đại Hóa Ứng Dụng với Công Cụ Được Hỗ Trợ bởi AI Tạo Sinh\nPhiên này đã khám phá cách Amazon Q Developer biến đổi vòng đời phát triển phần mềm (SDLC) thông qua các khả năng tác nhân của nó trên:\nAWS Console IDE CLI Các nền tảng DevSecOps Các khả năng chính được trình diễn:\nCác tác nhân Q tăng tốc độ tạo mã và cải thiện chất lượng mã Tích hợp liền mạch với các quy trình công việc hiện có Tạo tự động tài liệu toàn diện và bài kiểm tra đơn vị Cải thiện khả năng bảo trì mã và độ tin cậy Hiểu các cơ sở mã phức tạp và đề xuất tối ưu hóa Tự động hóa các tác vụ thường xuyên trong vòng đời phát triển Thảo Luận Bảng: Hiện Đại Hóa Ứng Dụng - Tăng Tốc Độ Chuyển Đổi Kinh Doanh\nCác Thành Viên Bảng:\nNguyễn Minh Ngân, Chuyên Gia AI, OCB Nguyễn Mạnh Tuyền, Trưởng Ứng Dụng Dữ Liệu, LPBank Securities Vinh Nguyễn, Đồng Sáng Lập \u0026amp; CTO, Ninety Eight Chuyển Đổi VMware với Hiện Đại Hóa Đám Mây Do AI Điều Khiển\nPhiên này cho thấy cách các tổ chức Việt Nam đang tăng tốc độ áp dụng đám mây với các tài sản VMware. Các chủ đề chính:\nCách AWS Transform giúp di chuyển nhanh, an toàn và hiệu quả về chi phí Sách hướng dẫn từng bước và các mô hình nhận thức về thời gian chết Lộ trình để hiện đại hóa lên EKS, RDS và serverless sau khi hạ cánh Lý tưởng cho các nhà lãnh đạo CNTT, kiến trúc sư và các nhóm vận hành lên kế hoạch di chuyển VMware-to-AWS quy mô lớn Bảo Mật AWS Ở Quy Mô: Từ Phát Triển đến Sản Xuất\nPhiên này đã khám phá cách nâng cao tư thế bảo mật đám mây trên toàn bộ vòng đời phát triển và sản xuất. Các chủ đề được đề cập:\nCách tiếp cận bảo mật toàn diện của AWS: xác định, phòng ngừa, phát hiện, phản ứng và khắc phục Các nguyên tắc bảo mật theo thiết kế trong suốt quá trình phát triển Các khả năng phát hiện và phản ứng nâng cao Cách AI tạo sinh nâng cao phân tích bảo mật và tự động hóa hoạt động Xây dựng các kiến trúc có khả năng phục hồi phát triển theo các mối đe dọa mới nổi Tạo các môi trường đám mây an toàn hơn, có thể mở rộng Những Điểm Chính Rút Ra Hiểu biết toàn diện về chiến lược AI và hiện đại hóa đám mây của AWS Những hiểu biết thực tế về áp dụng AI ở quy mô doanh nghiệp và triển khai Thực tiễn tốt nhất cho nền tảng dữ liệu, bảo mật và hiện đại hóa ứng dụng Các nghiên cứu trường hợp thực tế và bài học từ các lãnh đạo ngành Kiến thức thực hành về các dịch vụ AWS cho GenAI, di chuyển và hiện đại hóa "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-api-gateway/5.4.1-create-api/",
	"title": "Tạo REST API",
	"tags": [],
	"description": "",
	"content": "Tạo API Gateway (REST) Vào API Gateway → Create API → REST API → Build. Name: serverless-workshop-api; Endpoint type: Regional → Create API. Trong Resources: chọn / → Create Resource → Path: hello → Create. (Tùy chọn) bật CORS cho resource nếu muốn gọi từ browser. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.1-blog1/",
	"title": "Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code",
	"tags": [],
	"description": "",
	"content": "Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code bởi Lauren Mullennex, Anagha Barve, Anchit Gupta, và Bhargava Varadharajan vào ngày 12 THÁNG 9 2025 trong Amazon SageMaker AI, Amazon SageMaker Unified Studio, Announcements, Intermediate (200), Technical How-to\nCác nhà phát triển và kỹ sư học máy (ML) giờ đây có thể kết nối trực tiếp tới Amazon SageMaker Unified Studio từ trình soạn thảo Visual Studio Code (VS Code) cục bộ của họ. Với khả năng này, bạn có thể giữ nguyên quy trình phát triển hiện có và cấu hình môi trường phát triển tích hợp (IDE) cá nhân hóa, đồng thời truy cập các dịch vụ phân tích AWS và trí tuệ nhân tạo \u0026amp; máy học (AI/ML) trong một môi trường phát triển dữ liệu và AI hợp nhất. Sự tích hợp này cung cấp truy cập liền mạch từ môi trường phát triển cục bộ của bạn đến cơ sở hạ tầng có thể mở rộng để chạy xử lý dữ liệu, phân tích SQL và các luồng công việc ML. Bằng cách kết nối IDE cục bộ của bạn với SageMaker Unified Studio, bạn có thể tối ưu hóa luồng phát triển dữ liệu và AI mà không làm gián đoạn các thực tiễn phát triển đã thiết lập.\nTrong bài viết này, chúng tôi minh họa cách kết nối VS Code cục bộ của bạn đến SageMaker Unified Studio để bạn có thể xây dựng luồng công việc dữ liệu và AI đầu-cuối trong khi làm việc trong môi trường phát triển ưa thích của bạn.\nTổng quan giải pháp Kiến trúc giải pháp bao gồm ba thành phần chính:\nMáy tính cục bộ – Máy phát triển của bạn chạy VS Code với AWS Toolkit cho Visual Studio Code và Microsoft Remote SSH được cài đặt. Bạn có thể kết nối thông qua extension Toolkit cho VS Code bằng cách duyệt các không gian (spaces) SageMaker Unified Studio có sẵn và chọn môi trường mục tiêu của chúng.\nSageMaker Unified Studio – Là phần của thế hệ kế tiếp của Amazon SageMaker, SageMaker Unified Studio là một môi trường phát triển dữ liệu và AI duy nhất, nơi bạn có thể tìm và truy cập dữ liệu của mình và thao tác nó bằng các công cụ AWS quen thuộc cho phân tích SQL, xử lý dữ liệu, phát triển mô hình và phát triển ứng dụng AI tạo sinh.\nAWS Systems Manager – Một dịch vụ truy cập từ xa và quản lý an toàn, có khả năng mở rộng, giúp kết nối liền mạch giữa VS Code cục bộ của bạn và các không gian SageMaker Unified Studio để đơn giản hóa luồng phát triển dữ liệu và AI.\nSơ đồ sau đây biểu diễn sự tương tác giữa IDE cục bộ của bạn và các không gian SageMaker Unified Studio.\nCác điều kiện tiên quyết Để thử kết nối IDE từ xa, bạn phải có các điều kiện sau:\nTruy cập vào domain SageMaker Unified Studio có kết nối Internet. Với các domain được cấu hình ở chế độ chỉ VPC, domain của bạn phải có tuyến ra Internet qua proxy hoặc NAT gateway. Nếu domain của bạn hoàn toàn cô lập khỏi Internet, tham khảo tài liệu để thiết lập kết nối từ xa. Nếu bạn chưa có domain SageMaker Unified Studio, bạn có thể tạo một domain bằng tùy chọn thiết lập nhanh (quick setup) hoặc thiết lập thủ công (manual setup).\nMột người dùng với thông tin đăng nhập SSO thông qua IAM Identity Center được yêu cầu. Để cấu hình truy cập người dùng SSO, hãy xem tài liệu.\nTruy cập hoặc có thể tạo một dự án SageMaker Unified Studio.\nCompute Space JupyterLab hoặc Code Editor với yêu cầu loại instance tối thiểu 8 GB bộ nhớ. Trong bài viết này, chúng tôi dùng instance ml.t3.large. Phiên bản ảnh phân phối SageMaker Distribution image phiên bản 2.8 trở lên được hỗ trợ.\nBạn có VS Code bản ổn định mới nhất với Microsoft Remote SSH (phiên bản 0.74.0 trở lên) và extension AWS Toolkit (phiên bản 3.74.0) được cài đặt trên máy cục bộ của bạn.\nTriển khai giải pháp Để cho phép kết nối từ xa và kết nối tới không gian từ VS Code, hoàn tất các bước sau. Để kết nối tới một space SageMaker Unified Studio từ xa, space đó phải được bật tính năng truy cập từ xa.\nĐiều hướng tới space JupyterLab hoặc Code Editor của bạn. Nếu nó đang chạy, dừng space và chọn Configure space để bật truy cập từ xa\nBật Remote access để kích hoạt tính năng và chọn Save and restart.\nĐiều hướng tới AWS Toolkit trong cài đặt VS Code cục bộ của bạn.\nTrên tab SageMaker Unified Studio, chọn Sign in để bắt đầu và cung cấp URL domain SageMaker Unified Studio, ví dụ https://\u0026lt;domain‑id\u0026gt;.sagemaker.\u0026lt;region\u0026gt;.on.aws.\nBạn sẽ được yêu cầu chuyển hướng sang trình duyệt web để cho phép truy cập các extension IDE AWS. Chọn Open để mở tab trình duyệt mới.\nChọn Allow access để kết nối tới dự án qua VS Code.\nBạn sẽ nhận được thông báo Request approved, cho thấy bạn đã có quyền truy cập domain từ xa.\nQuay lại VS Code cục bộ của bạn để truy cập dự án và tiếp tục xây dựng các công việc ETL, pipeline dữ liệu, đào tạo \u0026amp; triển khai mô hình ML hoặc xây ứng dụng AI tạo sinh. Để kết nối tới dự án cho xử lý dữ liệu và phát triển ML, thực hiện các bước:\nChọn Select a project để xem dữ liệu và tài nguyên tính toán. Tất cả các dự án trong domain được liệt kê, nhưng bạn chỉ được phép truy cập các dự án mà bạn là thành viên.\nBạn chỉ có thể xem một domain và một dự án tại một thời điểm. Để chuyển dự án hoặc đăng xuất khỏi domain, chọn biểu tượng dấu ba chấm.Bạn cũng có thể xem tài nguyên dữ liệu và tính toán mà bạn đã tạo trước đó.\nKết nối space JupyterLab hoặc Code Editor bằng cách chọn biểu tượng kết nối. Nếu tùy chọn này không hiển thị, có thể bạn đã tắt truy cập từ xa trong space. Nếu space đang ở trạng thái “Stopped”, di chuột lên space và chọn nút connect, điều này sẽ bật truy cập từ xa, khởi động space và kết nối nó. Nếu space đang ở trạng thái “Running”, space phải được khởi động lại với truy cập từ xa được bật bằng cách dừng space rồi kết nối lại từ toolkit.\nMột cửa sổ VS Code khác sẽ mở ra và được kết nối tới space SageMaker Unified Studio của bạn qua remote SSH.\nĐiều hướng tới Explorer để xem notebook, file và script của space. Từ AWS Toolkit, bạn cũng có thể xem nguồn dữ liệu của bạn.\nSử dụng thiết lập VS Code tùy chỉnh với tài nguyên SageMaker Unified Studio Khi bạn kết nối VS Code với SageMaker Unified Studio, bạn giữ nguyên tất cả phím tắt cá nhân và tùy chỉnh của bạn. Ví dụ, nếu bạn dùng đoạn code snippet để nhanh chóng chèn các mẫu mã phân tích và ML phổ biến, chúng vẫn hoạt động với cơ sở hạ tầng được quản lý của SageMaker Unified Studio.\nTrong hình minh họa, chúng tôi thể hiện cách sử dụng các snippet luồng phân tích: snippet “show-databases” truy vấn Athena để hiển thị các database có sẵn, “show-glue-tables” liệt kê bảng trong AWS Glue Data Catalog, và “query-ecommerce” lấy dữ liệu sử dụng Spark SQL để phân tích.\nBạn cũng có thể dùng các snippet để tự động hóa việc build và training mô hình ML trên SageMaker AI. Trong hình bên dưới, các snippet mã thể hiện xử lý dữ liệu, cấu hình và khởi chạy job đào tạo SageMaker AI. Cách tiếp cận này cho thấy người làm dữ liệu có thể giữ thiết lập phát triển quen thuộc của mình trong khi sử dụng tài nguyên dữ liệu và AI được quản lý trong SageMaker Unified Studio.\nVô hiệu hóa truy cập từ xa trong SageMaker Unified Studio Như một quản trị viên, nếu bạn muốn vô hiệu hóa tính năng này cho người dùng, bạn có thể thực thi nó bằng cách thêm chính sách sau vào vai trò IAM của dự án:\n{\n\u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;,\n\u0026ldquo;Statement\u0026rdquo;: [\n{\n\u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;DenyStartSessionForSpaces\u0026rdquo;,\n\u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Deny\u0026rdquo;,\n\u0026ldquo;Action\u0026rdquo;: [\n\u0026ldquo;sagemaker:StartSession\u0026rdquo;\n],\n\u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:sagemaker:*:*:space/*/*\u0026rdquo;\n}\n]\n}\nDọn dẹp Theo mặc định, SageMaker Unified Studio tắt các tài nguyên nhàn rỗi như các space JupyterLab và Code Editor sau 1 giờ. Nếu bạn đã tạo một domain SageMaker Unified Studio cho mục đích bài viết này, nhớ xóa domain đó.\nKết luận Kết nối trực tiếp từ IDE cục bộ của bạn đến Amazon SageMaker Unified Studio giảm ma sát khi chuyển giữa phát triển cục bộ và hạ tầng dữ liệu \u0026amp; AI có thể mở rộng. Bằng cách giữ cấu hình IDE cá nhân hóa, điều này giảm sự cần thiết phải thích nghi giữa các môi trường phát triển khác nhau. Dù bạn đang xử lý các tập dữ liệu lớn, đào tạo mô hình nền tảng (foundation models, FMs), hoặc xây dựng ứng dụng AI tạo sinh, bạn giờ có thể làm việc từ thiết lập cục bộ của mình trong khi truy cập các khả năng của SageMaker Unified Studio. Bắt đầu ngay hôm nay bằng cách kết nối IDE cục bộ của bạn đến SageMaker Unified Studio để hợp lý hóa luồng xử lý dữ liệu và tăng tốc phát triển mô hình ML.\nGiới thiệu về các tác giả Lauren Mullennex\rLauren là Kiến trúc sư Giải pháp Chuyên gia GenAI/ML Cấp cao tại AWS, có hơn một thập kỷ kinh nghiệm về ML, DevOps và hạ tầng. Cô là tác giả sách về thị giác máy tính và yêu thích du lịch, leo núi cùng hai chú chó.\rBhargava Varadharajan\rBhargava là Kỹ sư Phần mềm Cấp cao tại AWS, phát triển các sản phẩm như SageMaker Studio, Studio Lab và Unified Studio. Anh đã dành hơn 5 năm để biến các quy trình AI/ML phức tạp thành trải nghiệm liền mạch, đồng thời theo đuổi mục tiêu khám phá 63 công viên quốc gia Hoa Kỳ, leo núi, đá bóng, trượt tuyết, làm DIY và đọc sách.\rAnagha Barve\rAnagha là Quản lý Phát triển Phần mềm của nhóm Amazon SageMaker Unified Studio.\rAnchit Gupta\rAnchit là Quản lý Sản phẩm Cấp cao cho Amazon SageMaker Unified Studio. Cô giúp các nhóm xây dựng giải pháp ML dễ dàng hơn và yêu thích nấu ăn, chơi board/card games, đọc sách khi rảnh rỗi.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/",
	"title": "Tuần 1 - Kiến thức Nền tảng Cloud Computing",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-08 đến 2025-09-12\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 1 Tuần này tập trung củng cố những khái niệm cơ bản về Cloud Computing, hạ tầng AWS và các công cụ quản trị.\nNội dung chính Giới thiệu Cloud Computing và lợi ích. AWS Global Infrastructure (Region, AZ, Edge Location). Bộ công cụ quản lý AWS (Console, CLI, SDK). Chiến lược tối ưu chi phí. AWS Well-Architected Framework. Labs thực hành Lab 01: Thiết lập tài khoản AWS \u0026amp; IAM. Lab 07: AWS Budgets \u0026amp; Cost Management. Lab 09: AWS Support Plans. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/",
	"title": "Worklog - Hành Trình Học AWS",
	"tags": [],
	"description": "",
	"content": "Worklog Tổng quan Worklog 12 tuần (5 ngày/tuần) bắt đầu 08/09/2025, tóm lược nội dung học phục vụ đánh giá: nền tảng AWS, kiến trúc, và các cập nhật GenAI/compute mới nhất từ re:Invent.\nẢnh nhanh từng tuần Tuần 1: Nền tảng đám mây, hạ tầng toàn cầu, Well-Architected Tuần 2: Mạng (VPC, SG/NACL, LB, TGW/peering, VPN/DC) Tuần 3: Tính toán (EC2/AMI/EBS, scaling, giá, EFS/FSx) Tuần 4: Lưu trữ (S3 classes, Glacier, Snow, DR/Backup) Tuần 5: Bảo mật \u0026amp; danh tính (IAM, Org, KMS, Security Hub, SSO) Tuần 6: CSDL (RDS/Aurora/Redshift, ElastiCache, DMS) Tuần 7: Serverless \u0026amp; Containers (Lambda, ECS/EKS, ECR) Tuần 8: Giám sát \u0026amp; vận hành (CloudWatch, X-Ray, CloudTrail, cost/ops) Tuần 9: Kiến trúc Transformer, attention, encoder-decoder, GPT/BERT/T5 Tuần 10: Transfer learning, QA modes, BERT \u0026amp; T5 fine-tuning Tuần 11: Lambda Managed Instances (capacity provider, mạng, scale, chi phí) Tuần 12: Thông báo re:Invent 2025 (Nova models, Bedrock, S3 Vectors, SageMaker serverless/elastic training, Graviton5, Trainium3) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.2-prerequisite/",
	"title": "Chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Tài khoản \u0026amp; quyền hạn AWS account với quyền tạo Lambda, CloudWatch Logs, API Gateway (IAM tối thiểu: AWSLambdaBasicExecutionRole). Không cần quyền EC2/VPC nâng cao cho workshop này. Công cụ Trình duyệt để thao tác AWS Console. curl/Invoke-RestMethod hoặc Postman để gọi API Gateway. Tùy chọn: editor cục bộ để soạn code Node.js/Python trước khi dán vào console. Thiết lập nhanh Chọn region gần (ví dụ us-east-1 hoặc ap-southeast-1). Tạo IAM role cho Lambda với trust policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sts:AssumeRole\u0026#34;], \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: [\u0026#34;lambda.amazonaws.com\u0026#34;] } } ] } Gán policy AWSLambdaBasicExecutionRole cho role. Đặt CloudWatch Logs retention (ví dụ 7–14 ngày) để kiểm soát chi phí: CloudWatch → Log groups → /aws/lambda/\u0026lt;tên-hàm\u0026gt; → Actions → Edit retention. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-lambda-basics/5.3.2-hello-python/",
	"title": "Lambda Hello World (Python)",
	"tags": [],
	"description": "",
	"content": "Tạo hàm Vào Lambda console → Create function → Author from scratch. Name: hello-python, Runtime: Python 3.12. Handler mẫu: import json def lambda_handler(event, context): name = ( (event.get(\u0026#34;queryStringParameters\u0026#34;) or {}).get(\u0026#34;name\u0026#34;) or event.get(\u0026#34;name\u0026#34;) or \u0026#34;world\u0026#34; ) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;message\u0026#34;: f\u0026#34;Hello, {name}\u0026#34;}) } Save, tạo test event (ví dụ {\u0026quot;name\u0026quot;:\u0026quot;Bob\u0026quot;}), xem response và log CloudWatch. Có thể bắt lỗi body không hợp lệ và trả 400 nếu cần. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/",
	"title": "Ngày 02 - Hạ tầng Toàn cầu của AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-09 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Hạ tầng AWS Trung tâm dữ liệu (Data Center) Mỗi trung tâm dữ liệu có thể chứa hàng chục nghìn máy chủ. AWS tự thiết kế và vận hành phần cứng riêng để tối ưu hiệu năng và độ tin cậy. Vùng khả dụng (Availability Zone - AZ) Một hoặc nhiều trung tâm dữ liệu tách biệt vật lý trong cùng một Region. Mỗi AZ được thiết kế cách ly lỗi. Kết nối với nhau bằng mạng riêng độ trễ thấp, băng thông cao. AWS khuyến nghị triển khai workload tối thiểu trên hai AZ. Region Mỗi Region chứa ít nhất ba AZ. Hiện có hơn 25 Region trên toàn thế giới. Các Region kết nối với nhau qua mạng backbone của AWS. Phần lớn dịch vụ mặc định ở phạm vi Region. Edge Location Mạng lưới edge toàn cầu giúp phân phối nội dung với độ trễ tối thiểu. Được sử dụng bởi các dịch vụ như: Amazon CloudFront (CDN) AWS WAF (Tường lửa ứng dụng web) Amazon Route 53 (Dịch vụ DNS) Hands-On Labs Lab 01 – Thiết lập Tài khoản AWS \u0026amp; IAM Tạo tài khoản AWS → 01-01 Cấu hình thiết bị MFA ảo → 01-02 Tạo nhóm Admin và người dùng Admin → 01-03 Cập nhật thông tin hỗ trợ xác thực tài khoản → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/",
	"title": "Ngày 07 - Định tuyến VPC &amp; Network Interface",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-16 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Định tuyến VPC \u0026amp; ENI Route Table Route table xác định cách điều hướng lưu lượng mạng. Mỗi VPC có một route table mặc định chỉ chứa tuyến local để các subnet giao tiếp nội bộ. Có thể tạo thêm route table tùy chỉnh, nhưng tuyến local không thể xóa. Elastic Network Interface (ENI) ENI là card mạng ảo có thể gắn sang các EC2 instance khác nhau. Khi chuyển ENI, địa chỉ IP riêng, EIP và MAC được giữ nguyên. Elastic IP (EIP) là địa chỉ IPv4 công cộng tĩnh có thể gắn vào ENI. Bị tính phí nếu EIP không gắn cho tài nguyên nào. Tình huống sử dụng ENI:\nTách mạng quản trị khỏi mạng dữ liệu. Xây dựng thiết bị mạng/bảo mật (appliance). Instance hai cổng mạng chạy workload ở các subnet khác nhau. Giải pháp high availability chi phí thấp. VPC Endpoint VPC Endpoint cho phép kết nối riêng tư tới dịch vụ AWS qua AWS PrivateLink mà không đi Internet công cộng. Hai loại endpoint: Interface Endpoint: Tạo một ENI với IP riêng. Gateway Endpoint: Sử dụng route table (chỉ dành cho S3 và DynamoDB). Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (tiếp tục) Tạo Internet Gateway (IGW) → 03-03.3 Tạo Route Table (Outbound qua IGW) → 03-03.4 Tạo Security Group → 03-03.5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/",
	"title": "Ngày 12 - Lưu trữ &amp; Sao lưu cho EC2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-23 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Lưu trữ \u0026amp; Bảo mật cho EC2 Sao lưu trong EC2 AWS Backup cung cấp giải pháp sao lưu tập trung cho nhiều dịch vụ, bao gồm EC2. EBS Snapshot sao lưu các volume EBS: Sao lưu theo thời điểm (point-in-time). Dạng incremental (chỉ lưu block thay đổi). Lưu trữ trong S3 (không truy cập trực tiếp). AMI Backup chụp toàn bộ cấu hình EC2 dưới dạng image. Best practices cho Snapshot:\nLên lịch snapshot định kỳ. Sao chép snapshot sang Region khác cho DR. Gắn thẻ (tag) để quản lý vòng đời. Sử dụng Amazon Data Lifecycle Manager (DLM). Key Pair Key Pair dùng để xác thực an toàn khi kết nối EC2: Public Key – lưu trên instance. Private Key – người dùng giữ để SSH (Linux) hoặc RDP (Windows). Thay thế mật khẩu, tăng cường bảo mật. Lưu ý: Nếu mất private key, AWS không thể khôi phục. Quản lý Key Pair:\nTạo key pair trên AWS hoặc import key sẵn có. Lưu trữ private key an toàn. Dùng key pair khác nhau cho từng môi trường. Luân phiên (rotate) định kỳ. Elastic Block Store (EBS) Amazon EBS cung cấp lưu trữ dạng block bền vững cho EC2. Các loại volume: General Purpose SSD (gp2/gp3) – cân bằng hiệu năng và chi phí. Provisioned IOPS SSD (io1/io2) – cho workload cần IOPS cao. Throughput Optimized HDD (st1) – dữ liệu lớn, truy cập tuần tự. Cold HDD (sc1) – dữ liệu ít truy cập, chi phí thấp. Tính năng chính:\nGắn/Tháo volume với instance. Dữ liệu vẫn giữ khi instance tắt. Tạo snapshot để sao lưu hoặc copy sang Region khác. Tự động nhân bản trong phạm vi AZ. So sánh volume EBS:\nLoại Tình huống Max IOPS Max Throughput gp3 Mục đích tổng quát 16.000 1.000 MB/s io2 Hiệu năng cao 64.000 1.000 MB/s st1 Big data 500 500 MB/s sc1 Lưu trữ lạnh 250 250 MB/s "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/",
	"title": "Ngày 17 - Tính năng nâng cao của S3",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-30 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Hosting website tĩnh trên Amazon S3 Hosting trực tiếp website tĩnh (HTML, CSS, JS, hình ảnh) từ S3.\nKhả năng chính Thiết lập đơn giản: Chỉ vài bước để bật chế độ static website cho bucket. Chi phí thấp: Trả phí lưu trữ và băng thông tiêu chuẩn, không cần máy chủ web riêng. Scale linh hoạt: Tự động xử lý spike traffic. Tích hợp CDN: Dễ dàng kết hợp Amazon CloudFront để tăng hiệu năng toàn cầu. Cấu hình website tĩnh:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS cho phép tài nguyên web (font, JavaScript, \u0026hellip;) trên một domain truy cập tài nguyên ở domain khác.\nCấu hình CORS trên S3 Định nghĩa policy: Chỉ rõ những origin nào được phép truy cập nội dung bucket. Kiểm soát method: Cho phép các HTTP method cụ thể (GET, PUT, POST, \u0026hellip;). Tăng cường bảo mật: Ngăn truy cập cross-origin trái phép. Ví dụ cấu hình CORS:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Hiệu năng \u0026amp; thiết kế khóa object Cách đặt tên object ảnh hưởng đáng kể tới hiệu năng S3:\nPrefix ngẫu nhiên: Phân tán key qua nhiều partition để tăng song song. Tránh prefix tuần tự: Không dùng tiền tố tăng dần (ví dụ timestamp) cho workload throughput cao. Truy cập song song: Thiết kế key hỗ trợ đọc/ghi đồng thời. Best practice đặt key:\n❌ Tệ: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg\r✅ Tốt: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier – Lưu trữ dài hạn Các lớp Glacier được tối ưu cho lưu trữ dài hạn chi phí thấp.\nTùy chọn truy xuất Expedited / Fast: Vài phút; chi phí cao. Standard: 3–5 giờ; cân bằng chi phí. Bulk: 5–12 giờ; rẻ nhất cho khôi phục khối lượng lớn. Glacier Deep Archive Lớp chi phí thấp nhất cho lưu trữ nhiều năm, thời gian truy xuất khoảng 12 giờ.\nHands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Phần 2) Cấu hình object public → 57-5 Kiểm tra website → 57-6 Chặn toàn bộ public access → 57-7.1 Cấu hình CloudFront → 57-7.2 Kiểm tra CloudFront → 57-7.3 Bật Versioning cho bucket → 57-8 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/",
	"title": "Ngày 22 - IAM Policies &amp; Roles",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-07 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học IAM Policies Policy IAM là tài liệu JSON mô tả quyền. Các loại: Identity-based policy (gắn vào principal). Resource-based policy (gắn vào resource). Quy tắc đánh giá: mọi explicit Deny sẽ ghi đè Allow trên tất cả policy. Ví dụ ràng buộc admin S3:\nCho phép toàn bộ s3:* trên một bucket cụ thể. Explicit Deny mọi hành động không phải S3. Cấu trúc policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Logic đánh giá policy:\nMặc định mọi request bị từ chối. Allow rõ ràng sẽ ghi đè deny mặc định. Deny rõ ràng ghi đè mọi Allow. Permissions boundary giới hạn quyền tối đa. IAM Roles Role cung cấp quyền tạm thời cho user, dịch vụ hoặc danh tính bên ngoài. Các tình huống phổ biến: Cho phép dịch vụ AWS hành động thay bạn (ví dụ EC2 ghi vào S3). Truy cập chéo tài khoản. Liên kết danh tính từ IdP ngoài (federation). Cấp credential cho ứng dụng trên EC2 mà không cần lưu access key. Lợi ích\nKhông có credential dài hạn, phiên ngắn, hỗ trợ nguyên tắc least privilege và quản lý truy cập quy mô lớn. Các loại role:\nService Role: Cho dịch vụ AWS (EC2, Lambda, \u0026hellip;). Cross-Account Role: Truy cập tài nguyên ở tài khoản khác. Identity Provider Role: Cho người dùng liên kết (federated). Instance Profile: Vỏ chứa role dành cho EC2 instance. Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Phần 2) Sử dụng Access Key → 48-2.2 Tạo IAM Role → 48-3.1 Gán IAM Role → 48-3.2 Dọn dẹp tài nguyên → 48-4 Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Phần 1) Tạo IAM User → 28-2.1 Tạo IAM Policy → 28-3 Tạo IAM Role → 28-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/",
	"title": "Ngày 27 - Amazon RDS &amp; Aurora",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-14 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học RDBMS vs NoSQL RDBMS RDBMS lưu dữ liệu trong các bảng có quan hệ (hàng/cột), đảm bảo ràng buộc toàn vẹn, dùng SQL và cung cấp đầy đủ đặc tính ACID. Engine phổ biến: Oracle, MySQL, SQL Server, PostgreSQL, IBM Db2. Tổng quan NoSQL NoSQL hướng tới dữ liệu phi cấu trúc/bán cấu trúc, ưu tiên khả năng mở rộng và hiệu năng cao. Các loại chính: Document (MongoDB, CouchDB) Key–Value (Redis, DynamoDB) Column-Family (Cassandra, HBase) Graph (Neo4j, Amazon Neptune) Đặc điểm: schema linh hoạt, mở rộng ngang, xử lý big data tốt, thiết kế theo CAP. So sánh nhanh RDBMS vs NoSQL OLTP vs OLAP OLTP: nhiều giao dịch nhỏ, đồng thời; dữ liệu chuẩn hóa; truy vấn ngắn, phụ thuộc index. OLAP: phân tích phức tạp trên dữ liệu lịch sử; schema sao/tuyết; đọc nhiều, ghi ít. Amazon RDS \u0026amp; Aurora Amazon Relational Database Service (RDS) Dịch vụ CSDL quan hệ managed giúp đơn giản hóa triển khai, vá lỗi, backup và HA.\nEngine hỗ trợ: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Amazon Aurora. Tính năng chính: backup/patch tự động, mở rộng dễ dàng, Multi-AZ cho HA, bảo mật với VPC/IAM/SSL. Kiểu triển khai: Single-AZ Multi-AZ (standby đồng bộ ở AZ khác) Read Replica để scale đọc Tính năng RDS:\nAutomated Backups: khôi phục tới từng thời điểm trong 35 ngày. Manual Snapshots: backup thủ công do người dùng kích hoạt. Multi-AZ: tự động failover để duy trì HA. Read Replica: tối đa 15 replica (tùy engine) cho workload đọc. Parameter Groups: quản lý cấu hình database. Option Groups: bật tính năng bổ sung (ví dụ Oracle Advanced Security). Amazon Aurora Hệ CSDL tương thích MySQL/PostgreSQL được thiết kế lại cho cloud.\nĐiểm nổi bật: Hiệu năng ~5× MySQL / ~3× PostgreSQL (benchmark điển hình). Storage tự động mở rộng tới 128 TB. Sao chép 6 bản trên 3 AZ, tự chữa lành storage. Aurora Serverless mở rộng theo nhu cầu. Global Database cho độ trễ thấp đa vùng. Aurora Features:\nAurora Replicas: tối đa 15 read replica với độ trễ \u0026lt; 10 ms. Aurora Serverless: tự động scale compute. Aurora Global Database: replicate xuyên vùng \u0026lt; 1 giây. Aurora Backtrack: tua ngược DB về thời điểm cụ thể. Aurora Parallel Query: tăng tốc truy vấn phân tích trên dữ liệu hiện thời. Aurora Machine Learning: tích hợp ML native. Aurora vs RDS:\nTính năng Aurora RDS Hiệu năng ~5× MySQL, ~3× PostgreSQL Chuẩn Lưu trữ Tự mở rộng tới 128 TB Tăng thủ công Replica Tối đa 15 Tối đa 5 (MySQL) Failover \u0026lt; 30 giây 1–2 phút Backtrack Có Không Labs thực hành Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Phần 2) Tạo EC2 Instance → 05-3 Tạo RDS Database Instance → 05-4 Triển khai ứng dụng → 05-5 Backup \u0026amp; Restore → 05-6 Dọn tài nguyên → 05-7 Lab 43 – AWS Database Migration Service (DMS) (Phần 1) EC2 Connect RDP Client → 43-01 EC2 Connect Fleet Manager → 43-02 Cấu hình nguồn SQL Server → 43-03 Oracle Connect Source DB → 43-04 Oracle Config Source DB → 43-05 Drop Constraint → 43-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/",
	"title": "Ngày 32 - Contract-First &amp; Mocking",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-21 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Contract-First Development Quy trình 5 bước Viết OpenAPI spec để định nghĩa contract. Dùng spec làm Single Source of Truth cho cả frontend và backend. Frontend dựng UI với mock data dựa trên spec. Backend implement API bám sát schema (status code, payload). Chạy contract testing để chắc chắn backend tuân thủ spec. paths: /books/{id}: get: summary: Get book detail responses: \u0026#34;200\u0026#34;: $ref: \u0026#34;#/components/responses/BookDetail\u0026#34; Lợi ích Giảm mismatch API vì mọi người xem cùng một spec. Documentation, mock server, test script có thể sinh tự động. Dễ review và versioning trước khi triển khai thật. Insight Viết contract trước code giúp giảm ~80% lỗi integration khi frontend/backend phát triển song song.\nMock API với Prism Prism đọc OpenAPI để sinh response giả, cho phép frontend test UI sớm. Hỗ trợ nhiều scenario (200, 404, 500) bằng cách khai báo example trong spec. Giữ nhịp làm việc khi backend chưa xong hoặc đang refactor. Khi nên dùng Sprint đầu của vertical slice. Cần demo flow nhưng chưa có dữ liệu thật. Muốn viết test tự động cho UI dựa trên contract. Ghi chú vận hành Chạy Prism tại localhost:4010, cấu hình NEXT_PUBLIC_API_URL trỏ đến mock. Đảm bảo header CORS trong mock giống backend production. Luôn commit spec trước khi mock để mọi người dùng đúng version. Labs thực hành Tạo OpenAPI spec cho endpoint /books/{id}. Khởi chạy Prism mock server và test luồng UI. Viết checklist review contract (status code, schema, example data). "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.2-day37-2025-10-28/",
	"title": "Ngày 37 - Tìm Kiếm Giọng Nói &amp; Kiến Trúc Chatbot",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-28 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTìm Kiếm Giọng Nói (Cách Siri Hoạt Động) Hệ thống tìm kiếm giọng nói tuân theo một pipeline từ đầu vào giọng nói đến phản hồi có thể thực hiện được:\nCác Thành Phần Pipeline: 1. Chuyển Đổi Analog sang Digital Lời nói (phát âm) → Mô hình sóng âm thanh → Spectrogram (mô hình tần số) → Chuỗi các khung âm thanh sử dụng Fast Fourier Transform (FFT)\n2. Nhận Dạng Giọng Nói Tự Động (ASR) Phân tích đặc trưng: Trích xuất các đặc trưng âm thanh Hidden Markov Model (HMM): Nhận dạng mẫu cho chuyển giọng nói sang văn bản Thuật toán Viterbi: Tìm chuỗi trạng thái ẩn có khả năng nhất Từ điển ngữ âm: Ánh xạ âm thanh thành từ Mô hình ngôn ngữ: Đảm bảo tính chính xác ngữ pháp 3. Chú Thích NLP Tokenization Gắn nhãn POS Nhận dạng thực thể (NER) 4. Ánh Xạ Mẫu-Hành Động Ánh xạ các ý định được nhận dạng thành các hành động phù hợp\n5. Quản Lý Dịch Vụ API nội bộ \u0026amp; bên ngoài (email, SMS, bản đồ, thời tiết, cổ phiếu, v.v.) Thực hiện hành động được yêu cầu 6. Chuyển Văn Bản Thành Giọng Nói (TTS) Chuyển đổi phản hồi trở lại thành giọng nói\n7. Phản Hồi Người Dùng Hệ thống học từ các sửa chữa để cải thiện độ chính xác\nKiến Trúc Voicebot Pipeline xử lý voicebot bao gồm nhiều cấp độ ngôn ngữ:\nCác Lớp Xử Lý: Phân Tích Giọng Nói (Âm Vị Học) Nhận dạng và phiên âm giọng nói sử dụng Nhận Dạng Giọng Nói Tự Động (ASR)\nPhân Tích Hình Thái và Từ Vựng (Hình Thái Học) Phân tích cấu trúc và ý nghĩa của từ sử dụng các quy tắc hình thái và từ vựng\nPhân Tích Cú Pháp (Cú Pháp) Hiểu cấu trúc câu sử dụng từ vựng và quy tắc ngữ pháp\nSuy Luận Ngữ Cảnh (Ngữ Nghĩa) Hiểu ý nghĩa trong ngữ cảnh sử dụng ngữ cảnh diễn ngôn\nSuy Luận và Thực Thi Ứng Dụng (Lý Luận) Sử dụng kiến thức miền để quyết định hành động\nLập Kế Hoạch Phát Ngôn Lập kế hoạch những gì sẽ nói trong phản hồi\nHiện Thực Hóa Cú Pháp Tạo ra các câu chính xác ngữ pháp\nHiện Thực Hóa Hình Thái Áp dụng các dạng từ chính xác\nMô Hình Phát Âm Tạo ra phát âm phù hợp\nTổng Hợp Giọng Nói Chuyển đổi văn bản trở lại thành giọng nói\nQuy Trình Làm Việc Chatbot Quy Trình Từng Bước: 1. Người Dùng → Ứng Dụng Chat Người dùng gõ: \u0026ldquo;Tôi muốn kiểm tra số dư tài khoản.\u0026rdquo; Ứng Dụng Chat = giao diện nơi người dùng gõ (web, app, messenger)\n2. Ứng Dụng Chat → Chatbot Tin nhắn được gửi đến hệ thống chatbot\n3. Chatbot → NLP Engine Chatbot gửi tin nhắn đến NLP Engine để phân tích\nNLP Engine thực hiện hai tác vụ chính: (a) Phát Hiện Ý Định Xác định người dùng muốn làm gì\nVí dụ: kiểm_tra_số_dư (b) Trích Xuất Thực Thể Trích xuất dữ liệu quan trọng từ câu\nVí dụ: tài_khoản = thanh toán/tiết kiệm? 4. NLP Engine → Logic Nghiệp Vụ / Dịch Vụ Dữ Liệu Dựa trên ý định, chatbot gọi dịch vụ phù hợp:\nTruy vấn cơ sở dữ liệu Gọi API Thực thi quy tắc nghiệp vụ Xử lý logic backend Ví dụ: Gọi API để lấy số dư từ hệ thống ngân hàng\n5. Dịch Vụ Dữ Liệu → Chatbot Backend trả về kết quả:\n\u0026ldquo;Số dư tài khoản của bạn là 12.500.000₫\u0026rdquo;\n6. Chatbot → Ứng Dụng Chat Chatbot đóng gói thông tin thành phản hồi ngôn ngữ tự nhiên\n7. Hiển Thị Cho Người Dùng Người dùng nhìn thấy phản hồi\nChatbot = Lắng Nghe + Trò Chuyện Lắng Nghe (NLP - Hiểu) Nhận dạng ý định Trích xuất thực thể Hiểu ngữ cảnh Trò Chuyện (NLG - Tạo) Tạo ngôn ngữ tự nhiên Công thức hóa phản hồi Cá nhân hóa Đằng Sau Hậu Trường: Dữ liệu dựa trên kiến thức: Sự thật, quy tắc, FAQ Học máy: Học từ các tương tác Logic nghiệp vụ: Quy tắc cụ thể của ứng dụng Phân Biệt Quan Trọng: Từ Khóa vs Thực Thể Từ Khóa = các từ chỉ ra chủ đề hoặc đối tượng Thực Thể = các điểm dữ liệu cụ thể với loại và giá trị\nVí dụ: \u0026ldquo;Đặt chuyến bay đến Paris vào thứ Sáu\u0026rdquo;\nTừ khóa: đặt, chuyến bay Thực thể: điểm_đến = \u0026ldquo;Paris\u0026rdquo; (ĐỊA ĐIỂM) ngày = \u0026ldquo;thứ Sáu\u0026rdquo; (NGÀY THÁNG) Không phải tất cả từ khóa đều là thực thể, nhưng tất cả thực thể đều được trích xuất từ từ khóa!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.2-day42-2025-11-04/",
	"title": "Ngày 42 - Tổng Quan Kiến Trúc Transformer",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-04 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nKiến Trúc Transformer: Bức Tranh Toàn Cảnh Mô hình transformer được giới thiệu trong bài báo \u0026ldquo;Attention is All You Need\u0026rdquo; đã cách mạng hóa NLP. Hãy hiểu cấu trúc hoàn chỉnh của nó.\nKiến Trúc Cấp Cao CỬA VÀO CHUỖI\r↓\r[Tokenization \u0026amp; Embedding]\r↓\r[Thêm Positional Encoding]\r↓\r┌─────────────────────────────────┐\r│ ENCODER (N layers) │\r│ ├─ Multi-Head Attention │\r│ ├─ Layer Normalization │\r│ ├─ Feed-Forward Network │\r│ └─ Residual Connections │\r└─────────────────────────────────┘\r↓\r[Context Vectors từ Encoder]\r↓\r┌─────────────────────────────────┐\r│ DECODER (N layers) │\r│ ├─ Masked Multi-Head Attention │\r│ ├─ Encoder-Decoder Attention │\r│ ├─ Feed-Forward Network │\r│ └─ Layer Normalization │\r└─────────────────────────────────┘\r↓\r[Linear Layer + Softmax]\r↓\rĐẦU RA XÁC SUẤT Thành Phần 1: Word Embeddings Mỗi từ được chuyển đổi thành một vector dày đặc (thường là 512-1024 chiều).\nVí Dụ:\nTừ: \u0026#34;happy\u0026#34;\rEmbedding: [0.2, -0.5, 0.8, ..., 0.1] // 512 giá trị Thành Phần 2: Positional Encoding Vấn Đề: Transformers không có thứ tự tuần tự được tích hợp sẵn (không giống RNNs). Vì vậy chúng ta phải thêm thông tin vị trí một cách rõ ràng.\nGiải Pháp: Thêm các vector positional encoding vào embeddings.\nCông Thức:\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\rPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\rTrong đó:\r- pos = vị trí trong chuỗi (0, 1, 2, ...)\r- i = chỉ số chiều\r- d_model = chiều embedding (512, 1024, v.v.) Ý Tưởng:\nVị trí 0: \u0026ldquo;I\u0026rdquo; nhận PE₀ Vị trí 1: \u0026ldquo;am\u0026rdquo; nhận PE₁ Vị trí 2: \u0026ldquo;happy\u0026rdquo; nhận PE₂ Ví Dụ:\nEmbedding(\u0026#34;I\u0026#34;) = [0.2, -0.5, 0.8, ..., 0.1]\rPE(pos=0) = [0.0, 1.0, 0.0, 1.0, ..., 0.5]\rFinal = [0.2, 0.5, 0.8, 1.0, ..., 0.6] Thành Phần 3: Multi-Head Attention Thay vì một cơ chế attention, chúng ta có h \u0026ldquo;đầu\u0026rdquo; khác nhau chạy song song.\nKhái Niệm:\nĐầu vào: Query (Q), Key (K), Value (V) matrices\rĐầu 1: ScaledDotProductAttention(Q₁, K₁, V₁)\rĐầu 2: ScaledDotProductAttention(Q₂, K₂, V₂)\r...\rĐầu h: ScaledDotProductAttention(Qₕ, Kₕ, Vₕ)\rĐầu ra = Concatenate(Head₁, Head₂, ..., Headₕ) Tại Sao Nhiều Đầu?\nĐầu 1 có thể học mối quan hệ \u0026ldquo;chủ ngữ-động từ\u0026rdquo; Đầu 2 có thể học mối quan hệ \u0026ldquo;tính từ-danh từ\u0026rdquo; Đầu 3 có thể học mối quan hệ \u0026ldquo;đại từ-tham chiếu\u0026rdquo; Cùng nhau: Hiểu biết ngữ cảnh phong phú Cấu Hình Điển Hình:\nSố lượng đầu: 8-16 Chiều trên mỗi đầu: 64 (nếu tổng = 512, thì 512/8 = 64) Thành Phần 4: Residual Connections \u0026amp; Layer Normalization Residual Connections Đầu ra = Đầu vào + Attention(Đầu vào) Điều này giúp luồng gradient trong suốt huấn luyện và cho phép mạng lưới đi sâu hơn.\nLayer Normalization Normalized = (x - mean) / sqrt(variance + epsilon) Ổn định huấn luyện và tăng tốc độ hội tụ.\nThành Phần 5: Feed-Forward Network Sau attention, có một mạng feed-forward 2 lớp đơn giản:\nĐầu ra = ReLU(Linear₁(x)) → Linear₂ Các chiều điển hình:\nĐầu vào: [batch_size, seq_length, 512]\r↓ Linear₁ (512 → 2048)\r[batch_size, seq_length, 2048]\r↓ ReLU (non-linear)\r[batch_size, seq_length, 2048]\r↓ Linear₂ (2048 → 512)\r[batch_size, seq_length, 512] Điều này mở rộng rồi co lại, cho phép các phép biến đổi phi tuyến.\nEncoder: Chế Độ Xem Chi Tiết Lớp Encoder Duy Nhất:\nĐầu vào (x)\r↓\r[Multi-Head Self-Attention]\r↓\r[+ Residual Connection với đầu vào]\r↓\r[Layer Normalization]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual Connection]\r↓\r[Layer Normalization]\r↓\rĐầu ra Điểm Chính: Trong encoder, mỗi từ attend tới TẤT CẢ các từ (bao gồm cả chính nó) trong cùng một câu.\nEncoder cho: Đại diện ngữ cảnh của mỗi từ, xem xét tất cả các từ khác.\nDecoder: Chế Độ Xem Chi Tiết Decoder tương tự nhưng với masking:\nĐầu vào (shifted right by 1)\r↓\r[Masked Multi-Head Self-Attention] ← Chỉ có thể attend vào các vị trí trước\r↓\r[+ Residual + LayerNorm]\r↓\r[Encoder-Decoder Attention] ← Attend vào đầu ra encoder\r↓\r[+ Residual + LayerNorm]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual + LayerNorm]\r↓\rĐầu ra Ba Cơ Chế Attention trong Decoder:\nMasked Self-Attention:\nQueries, Keys, Values từ decoder Mỗi vị trí chỉ có thể attend tới các vị trí trước đó Ngăn chặn rò rỉ thông tin (decoder không thấy các từ tương lai) Encoder-Decoder Attention:\nQueries từ decoder Keys, Values từ encoder Decoder có thể attend tới bất kỳ vị trí encoder nào Feed-Forward:\nMạng 2 lớp giống như encoder Kết Hợp Tất Cả: Transformer Đầy Đủ Giai Đoạn Huấn Luyện Đầu vào: \u0026#34;Je suis heureux\u0026#34; (Tiếng Pháp)\rMục Tiêu: \u0026#34;I am happy\u0026#34; (Tiếng Anh)\rĐầu vào Encoder:\r- Tokenize: [Je, suis, heureux]\r- Embed mỗi token\r- Thêm positional encoding\r- Xử lý qua N lớp encoder\r→ Đầu ra: C (context vectors)\rĐầu vào Decoder:\r- Mục tiêu shifted right: [\u0026lt;START\u0026gt;, I, am]\r- Embed mỗi token\r- Thêm positional encoding\r- Xử lý qua N lớp decoder\r- Sử dụng masked self-attention\r- Sử dụng encoder-decoder attention trên C\r→ Đầu ra logits cho mỗi vị trí\rMất mát: So sánh dự đoán \u0026#34;am happy\u0026#34; với thực tế \u0026#34;am happy\u0026#34;\rBackprop: Cập nhật tất cả các trọng số Giai Đoạn Suy Luận Đầu vào Encoder: [Je, suis, heureux]\r→ Đầu ra: C (context vectors)\rDecoder:\rBước 1: Bắt đầu với [\u0026lt;START\u0026gt;]\rDự đoán từ tiếp theo: \u0026#34;I\u0026#34;\rBước 2: [\u0026lt;START\u0026gt;, I] Dự đoán từ tiếp theo: \u0026#34;am\u0026#34;\rBước 3: [\u0026lt;START\u0026gt;, I, am]\rDự đoán từ tiếp theo: \u0026#34;happy\u0026#34;\rBước 4: [\u0026lt;START\u0026gt;, I, am, happy]\rDự đoán từ tiếp theo: \u0026lt;END\u0026gt;\rĐầu ra: \u0026#34;I am happy\u0026#34; Tóm Tắt: Tại Sao Kiến Trúc Này Hoạt Động Tính Năng Lợi Ích Không RNN Hoàn toàn có thể song song hóa - huấn luyện trên GPU hiệu quả Self-Attention trong Encoder Mỗi từ nhận ngữ cảnh từ TẤT CẢ các từ khác Masked Attention trong Decoder Không thể nhìn thấy tương lai - huấn luyện với tạo hình autoregressive Positional Encoding Bảo tồn thứ tự từ mà không xử lý tuần tự RNN Multi-Head Attention Học nhiều loại mối quan hệ đồng thời Residual Connections Luồng gradient - cho phép huấn luyện mạng sâu Layer Normalization Ổn định - hội tụ nhanh hơn Các Đổi Mới Chính Song Song Hóa: O(1) độ sâu thay vì O(n) cho RNNs Phụ Thuộc Dài Hạn: Attention có thể kết nối trực tiếp bất kỳ hai vị trí nào Khả Năng Mở Rộng: Có thể tăng kích thước mô hình với cải thiện dự đoán Transfer Learning: Các transformer được huấn luyện trước (BERT, GPT) hoạt động trên các tác vụ "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.2-day47-2025-11-11/",
	"title": "Ngày 47 - Hai chế độ Question Answering",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-11 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nQA có ngữ cảnh vs. QA closed-book Cùng dùng transformer, nhưng khác ở dữ liệu đầu vào và cách đánh giá.\nCó ngữ cảnh (open book) Input: câu hỏi + đoạn văn hỗ trợ. Output: trích span hoặc sinh ngắn, bám sát ngữ cảnh. Train: span có nhãn (start/end) hoặc seq2seq kèm context. Lỗi thường gặp: sai span khi ngữ cảnh nhiễu. Closed-book Input: chỉ câu hỏi; model dựa vào kiến thức bên trong. Output: câu trả lời sinh ra không có context tường minh. Train: kiểu language modeling trên corpora lớn, fine-tune với QA pairs. Lỗi: bịa/hallucination; giảm bằng pre-train mạnh hoặc distillation. Chọn chế độ nào? Có thể cấp tài liệu lúc suy luận -\u0026gt; ưu tiên có ngữ cảnh (kiểm soát tốt, dễ trích dẫn). Bị giới hạn latency/lưu trữ -\u0026gt; closed-book nhẹ hơn nhưng rủi ro cao hơn. Đánh giá Có ngữ cảnh: Exact Match / F1 trên span; kiểm tra bám văn bản. Closed-book: BLEU/ROUGE + đánh giá factuality; cân nhắc thêm retrieval nếu lệch. Việc thực hành hôm nay Soạn ví dụ cho cả hai chế độ với dữ liệu miền của bạn. Định nghĩa metric theo chế độ (EM/F1 span vs. ROUGE/factuality). Liệt kê phương án retrieval để nâng closed-book lên open-book khi cần. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/1.11.2-day52-2025-11-18/",
	"title": "Ngày 52 - Thiết lập Capacity Provider",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-18 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nXây capacity provider Bước 1 cho LMI: định nghĩa lớp instance (capacity provider) với VPC, role và lựa chọn loại máy.\nThông tin bắt buộc IAM role cho phép Lambda launch/manage instance VPC config (subnet + SG) nơi instance LMI chạy Chọn loại máy Hỗ trợ: họ C/M/R đời mới (x86/AMD/Graviton), kích cỡ large+ Tùy biến: allow/deny list loại máy; đặt kiến trúc ARM nếu cần EBS: mã hóa mặc định; có thể dùng KMS của bạn Lưu ý mạng Dàn subnet qua 3 AZ cho sẵn sàng Tất cả egress/log đi qua ENI của instance; không cấu hình VPC ở mức function Đóng inbound SG; đảm bảo đường ra tới dependency/CloudWatch Guardrail Giới hạn max vCPU để chặn bùng chi phí Có thêm tham số scale ở mức provider khi cần "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/1.12.2-day57-2025-11-25/",
	"title": "Ngày 57 - Bedrock &amp; AgentCore",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-25 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nCập nhật Bedrock Thêm model open-weight managed (Mistral Large 3, Ministral 3 3B/8B/14B và đối tác khác) Reinforcement fine-tuning: tuning theo phản hồi, cải thiện độ chính xác lớn mà không cần bộ nhãn lớn AgentCore Thêm policy control và quality monitoring cho agent Cải thiện bộ nhớ và hội thoại tự nhiên để triển khai an toàn hơn Việc cần làm Chọn model để đánh giá (latency/chi phí/chất lượng so với hiện tại) Lên thử nghiệm RFT cho tác vụ mục tiêu; định nghĩa tín hiệu phản hồi Xác định guardrail/policy trước khi bật AgentCore production "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Thư Viện Online - Nền Tảng Nội Dung Serverless Cho Nhóm Nhỏ 1. Tổng quan điều hành Dự án Thư Viện Online nhằm xây dựng một nền tảng serverless, chi phí thấp để lưu trữ và phân phối nội dung (PDF/ePub) cho một nhóm người dùng nhỏ (ban đầu ~100 người, nhóm người dùng gồm sinh viên/lab cần chia sẻ tài liệu nghiên cứu nội bộ có kiểm duyệt). Giải pháp này ưu tiên tính bảo mật, quy trình duyệt nội dung (Admin Approval), và chi phí vận hành minh bạch, tuyến tính khi mở rộng. Kiến trúc sử dụng AWS Serverless hoàn toàn (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB). Chi phí dự kiến cho MVP (không tính Free Tier) ≈ $9.80/tháng, đảm bảo khả năng mở rộng lên 5.000 đến 50.000 người dùng với chi phí dễ dự đoán.\n2. Vấn đề Vấn đề là gì? Tài liệu và sách bị phân tán; thiếu một hệ thống truyền tải nội dung an toàn và có kiểm soát truy cập; quy trình thêm hoặc kiểm duyệt nội dung tốn thời gian và nhiều vấn đề liên quan đến pháp lý.\nGiải pháp Xây dựng một pipeline serverless trên AWS: Người dùng tải lên qua Presigned PUT URL (tới S3 tạm); Admin phê duyệt → Lambda di chuyển file đến thư mục công khai (nhưng được bảo vệ); Người đọc truy cập qua Signed GET URL (từ CloudFront/CDN) để đảm bảo tốc độ và kiểm soát truy cập.\nLợi ích và Tỷ suất hoàn vốn Giá trị kinh doanh: Tập trung hóa nội dung; kiểm soát chất lượng qua quy trình duyệt; triển khai nhanh chóng với CI/CD. Lợi ích kỹ thuật: Chi phí vận hành thấp (≈ $9.80/tháng ở MVP, không tính Free Tier); kiến trúc Serverless có thể mở rộng quy mô lớn (scale) dễ dàng; bảo mật truy cập nội dung. 3. Kiến trúc giải pháp A. High-level B. Luồng xử lý yêu cầu Dịch vụ AWS Sử Dụng Dịch vụ Vai trò chính Hoạt động cụ thể Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, quản lý domain Cognito Authentication Đăng ký/Đăng nhập, cấp JWT, refresh token API Gateway Entry point API Nhận request, xác thực JWT, route đến Lambda Lambda Business Logic Xử lý upload, duyệt, tạo signed URL, ghi metadata S3 Object Storage Lưu file gốc, file đã duyệt, được download qua Cloudfront Signed URL CloudFront CDN Phân phối nhanh nội dung, chặn direct access qua OAC DynamoDB Database Lưu metadata (tên sách, uploader, trạng thái duyệt) Route 53 DNS Trỏ domain đến Amplify Hosting, API Gateway, CloudFront CloudWatch Monitoring Lưu log Lambda, cảnh báo lỗi hoặc chi phí bất thường Tìm kiếm (Search):\nTìm kiếm đơn giản theo trường (VD: tên sách, tác giả), sử dụng DynamoDB GSIs cho các thuộc tính này và query theo GSI. Luồng xử lý yêu cầu User Upload: Presigned PUT tới S3 thư mục uploads/. Admin Approval: Lambda copy file từ uploads/ sang public/books/ khi được duyệt. Reader Security: CloudFront sử dụng Origin Access Control (OAC) để chặn truy cập trực tiếp S3 và chỉ cho phép đọc qua Signed URL (ngắn hạn) do Lambda tạo ra. Kiến trúc tìm kiếm Tìm kiếm đơn giản: Thiết kế GSI cho title và author (ví dụ: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Thêm endpoint GET /search?title=...\u0026amp;author=... để query theo GSI thay vì Scan. Phân quyền Admin Sử dụng Cognito User Groups với một nhóm Admins trong User Pool. Khi Admin đăng nhập, JWT sẽ chứa cognito:groups: [\u0026quot;Admins\u0026quot;]. Các Lambda thuộc nghiệp vụ Admin (ví dụ approveBook, takedownBook) phải kiểm tra claim này; nếu thiếu group, trả 403 Forbidden. Có thể dùng JWT Authorizer (API Gateway HTTP API) để xác thực, phần phân quyền chi tiết xử lý trong Lambda dựa trên claim. 4. Triển khai Kỹ Thuật Triển khai Thiết kế \u0026amp; IaC (Infra-as-Code): Xây dựng các stack CDK (Cognito, DDB, S3, Amplify, Lambda, API). Flow Upload \u0026amp; Duyệt: Triển khai Presigned PUT, lưu metadata (trạng thái pending), và logic Admin duyệt (copy file). Flow Đọc Sách: Triển khai endpoint Signed GET, và giao diện đọc (FE stream qua CloudFront). Vận hành (Ops): Thiết lập logs CloudWatch (retention ngắn), cảnh báo ngân sách (Budget Alerts), hardening IAM. Search: MVP: thêm GSI cho title, author và endpoint GET /search query theo GSI. Yêu cầu Kỹ Thuật Sử dụng CDK để định nghĩa toàn bộ hạ tầng. API Gateway phải là HTTP API để tối ưu chi phí. Lambda (Python) xử lý logic nghiệp vụ và tương tác DynamoDB/S3. S3 Bucket Policy phải chặn truy cập công khai và chỉ cho phép CloudFront OAC. 5. Lộ trình và các mốc tiến độ Lộ trình Dự án Nền tảng \u0026amp; Xác thực (Tuần 1-2) Mục tiêu là thiết lập hạ tầng và cho phép người dùng đăng nhập.\nTác vụ Backend (CDK/DevOps): Viết stack CDK/IaC cho Cognito (User Pool, App Client). Viết stack CDK cho DynamoDB (bảng chính, chưa cần GSI). Viết stack CDK cho S3 (Bucket uploads, public, logs) và cấu hình OAC (Origin Access Control). Triển khai API Gateway (HTTP API) và một Lambda \u0026ldquo;hello world\u0026rdquo; để kiểm thử. Tác vụ Frontend (Amplify): Cấu hình Amplify Hosting và kết nối với repo GitHub (CI/CD). Tích hợp Amplify UI / Cognito SDK cho các trang: Đăng ký, Xác thực email, Đăng nhập, Quên mật khẩu. Kết quả (Milestone): Developer có thể git push và FE tự động deploy. Người dùng có thể đăng ký/đăng nhập và nhận được JWT token. Luồng Upload \u0026amp; Duyệt (Tuần 2-3) Mục tiêu là cho phép người dùng (đã đăng nhập) tải file lên và Admin duyệt file đó.\nTác vụ Backend (CDK/Lambda): Viết Lambda createUploadUrl: Xác thực JWT (phải đăng nhập). Tạo Presigned PUT URL trỏ đến thư mục uploads/ trên S3. Ghi metadata vào DynamoDB (status: PENDING). Viết Lambda approveBook: Xác thực JWT (phải là Admin). Copy file từ uploads/ sang public/books/. Cập nhật status trong DynamoDB (status: APPROVED). Tác vụ Frontend: Xây dựng Form Upload (kéo thả, chọn file). Gọi API createUploadUrl để lấy URL. Thực hiện upload file (HTTP PUT) trực tiếp lên S3 Presigned URL. Xây dựng Giao diện Admin: Lấy danh sách sách có status PENDING. Có nút \u0026ldquo;Duyệt\u0026rdquo; (gọi API approveBook). Luồng Đọc \u0026amp; Tìm kiếm (Tuần 3-4) Mục tiêu là cho phép người dùng đọc và tìm kiếm sách đã được duyệt.\nTác vụ Backend (CDK/Lambda): Viết Lambda getReadUrl: Xác thực JWT (phải đăng nhập). Kiểm tra xem sách có status APPROVED không. Tạo Signed GET URL (ngắn hạn) qua CloudFront trỏ đến file trong public/books/. Cập nhật CDK: Thêm GSI (Global Secondary Index) cho title và author vào bảng DynamoDB. Viết Lambda searchBooks: Query DynamoDB dựa trên GSI (không dùng Scan). Tác vụ Frontend: Xây dựng Trang chủ: Hiển thị danh sách sách (từ API, không có URL). Xây dựng Thanh tìm kiếm (gọi API searchBooks). Xây dựng Giao diện Đọc sách (Reader): Khi bấm \u0026ldquo;Đọc\u0026rdquo;, gọi API getReadUrl. Dùng URL nhận được để render file (ví dụ: dùng react-pdf). Vận hành \u0026amp; Bảo mật (Tuần 5-6) Mục tiêu là \u0026ldquo;hóa cứng\u0026rdquo; hệ thống, làm cho nó an toàn và dễ giám sát.\nTác vụ Backend (CDK/Lambda): Thiết lập S3 Event Notification (cho uploads/). Viết Lambda validateMimeType: Trigger khi có file mới, đọc \u0026ldquo;magic bytes\u0026rdquo; để xác thực đúng là PDF/ePub. Nếu sai, cập nhật status: REJECTED_INVALID_TYPE. Viết Lambda takedownBook (API cho Admin) và deleteUpload (xóa file PENDING sau 72h). Tác vụ DevOps (AWS Console/CDK): Thiết lập AWS Budget Alerts (cảnh báo khi chi phí vượt $X). Thiết lập CloudWatch Alarms (ví dụ: Lambda error rate \u0026gt; 5%). Rà soát lại IAM (đảm bảo \u0026ldquo;least-privilege\u0026rdquo;), CORS (chỉ cho phép domain của Amplify). 6. Budget Estimation You can find the budget estimation on the: AWS Pricing Calculator\nDưới đây là ước tính chi phí hàng tháng nghiêm ngặt (giả định không áp dụng AWS Free Tier) tại quy mô MVP (100 người dùng).\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10 000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10 000 HTTP API calls/tháng 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10 000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/tháng 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Chi phí hạ tầng Mô hình chi phí này cho thấy sự hiệu quả của kiến trúc serverless: chi phí tập trung chủ yếu vào giá trị mang lại cho người dùng (Cognito MAU) thay vì trả tiền cho \u0026ldquo;máy chủ chờ\u0026rdquo; (idle servers).\n7. Đánh giá rủi ro Ma trận rủi ro Rủi ro Tác động Chiến lược giảm thiểu Chi phí tăng khi user đột biến Cao Giới hạn MAU, cache metadata qua CloudFront Upload lạm dụng Trung bình Giới hạn ≤ 50MB/file, xóa auto sau 72h File loại giả mạo/độc hại Trung bình S3 Event → Lambda xác thực MIME (magic bytes) Giám sát quá tải Thấp CloudWatch alert, log 14 ngày Chiến lược giảm thiểu Chi phí: Đặt AWS Budget Alerts cho CloudFront và Cognito. Nhận thức rằng Signed URL có TTL ngắn nên không cache công khai dài hạn; thay vào đó, cache metadata/API response (danh sách sách, chi tiết) trên CloudFront 3–5 phút để giảm tải API. Chỉ tạo Signed URL khi người dùng thực sự bấm đọc (on‑demand), không tạo sẵn cho cả danh sách. Tải lên: Giới hạn kích thước file ≤ 50MB cho MVP. (Có thể nâng lên 200MB khi cần, dùng multipart upload ở FE để tránh timeout.) Áp dụng Rate Limit/Throttling trên API Gateway cho các endpoint tạo Presigned URL. Thiết lập S3 Lifecycle Policy để tự động xóa file chưa duyệt ở uploads/ sau 72h. Thêm Server‑side Validation: S3 Event Notifications → Lambda đọc magic bytes (vd. thư viện file-type) để xác thực đúng PDF/ePub; nếu sai, tự động xóa và ghi trạng thái REJECTED_INVALID_TYPE vào DynamoDB. Bản quyền (DMCA): Lưu Audit Log trong DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp để phục vụ truy vết. Xây dựng Takedown API (chỉ Admin): cập nhật status TAKEDOWN; tùy chọn di chuyển object từ public/books/ sang quarantine/books/ (không xóa hẳn) để lưu vết. Kế hoạch ứng phó Nếu chi phí tăng vượt ngân sách, có thể tạm thời giới hạn người dùng mới thông qua hệ thống mời để kiểm soát MAU Cognito và tối ưu hóa file.\n8. Kết quả mong đợi Cải tiến kỹ thuật: Đảm bảo tốc độ truyền tải nhanh và bảo mật nội dung (CDN + Signed URL). Tạo ra một kiến trúc Serverless tiêu chuẩn trên AWS, dễ dàng mở rộng lên đến 50.000 người dùng mà không cần thay đổi kiến trúc cốt lõi. Hệ thống CI/CD hoàn toàn tự động cho cả Frontend và Backend (CDK/Amplify). Giá trị lâu dài Thiết lập một nền tảng dữ liệu tập trung và có cấu trúc cho nội dung sách. Cung cấp một tài liệu tham khảo sống về việc triển khai Serverless E2E. Khả năng tích hợp các dịch vụ phân tích (như Amazon QuickSight) hoặc AI/ML trong tương lai. Hệ thống này chứng minh khả năng xây dựng nền tảng nội dung bảo mật, tiết kiệm chi phí và mở rộng dễ dàng bằng AWS Serverless — phù hợp triển khai thực tế cho nhóm nhỏ.\n9. Word đính kèm "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/4.2-event2/",
	"title": "Sự Kiện 2 - AWS GenAI Builder Club",
	"tags": [],
	"description": "",
	"content": "AWS GenAI Builder Club: Vòng Đời Phát Triển Do AI Điều Khiển - Tái Tưởng Tượng Kỹ Thuật Phần Mềm Ngày \u0026amp; Giờ: Thứ Sáu, 3 tháng 10 năm 2025 | 14:00 (2:00 PM)\nĐịa Điểm: AWS Event Hall, L26 Tòa Nhà Bitexco, Thành phố Hồ Chí Minh\nGiảng Viên: Toàn Huỳnh \u0026amp; Mỹ Nguyễn\nĐiều Phối Viên: Diễm Mỹ, Đại Trương, Định Nguyễn\nTổng Quan Sự Kiện Phiên làm việc AWS GenAI Builder Club này khám phá Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC), một cách tiếp cận biến đổi đối với kỹ thuật phần mềm tích hợp AI như một cộng tác viên trung tâm trong toàn bộ quá trình phát triển. Phiên làm việc này có các bản trình diễn thực hành về Amazon Q Developer và Kiro, giới thiệu các ứng dụng thực tế của AI trong phát triển phần mềm hiện đại.\nChương Trình Giờ Phiên Giảng Viên 14:00 - 14:15 Chào Mừng - 14:15 - 15:30 Tổng Quan Vòng Đời Phát Triển Do AI Điều Khiển \u0026amp; Bản Trình Diễn Amazon Q Developer Toàn Huỳnh 15:30 - 15:45 Giải Lao - 15:45 - 16:30 Bản Trình Diễn Kiro Mỹ Nguyễn Các Khái Niệm \u0026amp; Bài Học Chính 1. Tổng Quan Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) Triết Lý Cốt Lõi Vòng Đời Phát Triển Do AI Điều Khiển đại diện cho một sự thay đổi cơ bản trong cách phần mềm được xây dựng. Thay vì coi AI là một suy nghĩ sau hoặc công cụ hoàn thành mã đơn giản, AI-DLC nhúng AI như một đối tác thông minh trong toàn bộ quá trình phát triển.\nCác Nguyên Tắc Chính:\nBạn Kiểm Soát - AI là trợ lý của bạn, không phải người quản lý của bạn. Bạn phải duy trì quyền quyết định về hướng dự án và chi tiết triển khai.\nAI Là Cộng Tác Viên, Không Phải Thay Thế - AI nên đặt những câu hỏi quan trọng về yêu cầu, kiến trúc và mục tiêu dự án của bạn. Sự hợp tác nên là hai chiều, với bạn hướng dẫn các đề xuất của AI.\nLập Kế Hoạch Trước Triển Khai - Luôn tạo một kế hoạch toàn diện trước khi đi vào mã. AI có thể giúp tạo kế hoạch này, nhưng bạn phải xem xét, xác thực và tinh chỉnh nó.\nQuy Trình Phát Triển Bước 1: Tạo Kế Hoạch Dự Án\nXác định rõ ràng yêu cầu và phạm vi dự án Yêu cầu AI tạo kế hoạch dựa trên thông số kỹ thuật của bạn Xem xét kế hoạch một cách phê phán và yêu cầu sửa đổi Đảm bảo kế hoạch chi tiết và rõ ràng Bước 2: Chia Nhỏ Thành User Stories\nChuyển đổi kế hoạch thành user stories với tiêu chí chấp nhận rõ ràng Chia phạm vi lớn thành các đơn vị nhỏ hơn, dễ quản lý Mỗi đơn vị trở thành một dự án nhỏ có thể được giao cho các thành viên nhóm Ước tính thời gian cho mỗi đơn vị (nhưng cẩn thận với việc ước tính quá cao) Bước 3: Xác Định Ngăn Xếp Công Nghệ\nChỉ định rõ ràng các công nghệ, framework và công cụ sẽ được sử dụng Thay vì bảo AI \u0026ldquo;đừng triển khai cái này\u0026rdquo;, hãy bảo nó \u0026ldquo;triển khai theo cách này\u0026rdquo; Hướng dẫn tích cực mang lại tỷ lệ thành công cao hơn các ràng buộc tiêu cực Bước 4: Yêu Cầu \u0026amp; Thiết Kế Chi Tiết\nViết yêu cầu với độ chính xác và rõ ràng Hợp tác với AI để tạo các thông số kỹ thuật chi tiết Xác định các mô hình dữ liệu, hợp đồng API và kiến trúc hệ thống Tạo tài liệu thiết kế trước khi triển khai bắt đầu Bước 5: Triển Khai \u0026amp; Xác Minh\nTriển khai các tính năng theo kế hoạch Sử dụng cách tiếp cận phát triển mob (nhóm làm việc cùng nhau trên mã) Xác minh tất cả mã đầu ra như một nhóm Tiến hành đánh giá mã và kiểm tra chất lượng Bước 6: Kiểm Thử \u0026amp; Triển Khai\nDi chuyển qua các môi trường: Development (Dev) → Testing (QA) → User Acceptance Testing (UAT) → Production (Prod) Đảm bảo các cổng chất lượng ở mỗi giai đoạn Xác thực chức năng trước khi phát hành sản xuất Các Yếu Tố Thành Công Quan Trọng Tạo Kế Hoạch Trước - Đừng mong đợi AI xử lý mọi thứ. Luôn bắt đầu với một kế hoạch rõ ràng. Xem Xét Thường Xuyên - Liên tục xem xét các đề xuất và đầu ra của AI. Tỷ lệ lỗi cao là có thể. Bạn Là Người Quản Lý - Giá trị của bạn nằm ở xác thực mã và quản lý dự án, không phải viết từng dòng mã. Đặt Câu Hỏi Làm Rõ - Đảm bảo AI hiểu ngữ cảnh dự án của bạn bằng cách đặt những câu hỏi quan trọng về yêu cầu, kiến trúc và mục tiêu. Sử Dụng Mẫu Prompt - Tạo các prompt có cấu trúc bao gồm ngữ cảnh người dùng, user stories và yêu cầu cụ thể để nhận được phản hồi AI rõ ràng hơn. Xuất Kế Hoạch Thành Tệp - Yêu cầu AI tạo kế hoạch dưới dạng tệp bạn có thể lưu, xem xét và sửa đổi. Điều này tạo ra một tài liệu sống cho tham chiếu trong tương lai. Lịch Sự Với AI - Duy trì giao tiếp tôn trọng với các công cụ AI. Mối quan hệ tốt có thể giúp ích trong các tương tác trong tương lai (và đó chỉ là thực hành tốt!). 2. Bản Trình Diễn Amazon Q Developer Amazon Q Developer Là Gì? Amazon Q Developer là một trợ lý được hỗ trợ bởi AI biến đổi vòng đời phát triển phần mềm (SDLC) thông qua các khả năng tác nhân trên nhiều nền tảng:\nAWS Console - Giúp với cấu hình cơ sở hạ tầng và dịch vụ IDE (Integrated Development Environment) - Cung cấp các đề xuất tạo mã và tối ưu hóa CLI (Command Line Interface) - Hỗ trợ tạo lệnh và tự động hóa Các Nền Tảng DevSecOps - Tích hợp các thực tiễn bảo mật vào quy trình phát triển Các Khả Năng Chính Tạo Mã \u0026amp; Chất Lượng\nTăng tốc độ tạo mã với các đề xuất được hỗ trợ bởi AI Cải thiện chất lượng mã thông qua các khuyến nghị thông minh Duy trì tích hợp liền mạch với các quy trình công việc hiện có Hiểu các cơ sở mã phức tạp và đề xuất tối ưu hóa Tài Liệu \u0026amp; Kiểm Thử\nTự động tạo tài liệu toàn diện Tạo bài kiểm tra đơn vị với nỗ lực thủ công tối thiểu Cải thiện đáng kể khả năng bảo trì mã và độ tin cậy Giảm boilerplate và các tác vụ mã lặp lại Hợp Tác Thông Minh\nHoạt động như một cộng tác viên thông minh tận dụng các mô hình ngôn ngữ lớn Kết hợp kiến thức dịch vụ AWS sâu sắc với chuyên môn mã hóa Giúp các nhà phát triển tăng tốc độ các chu kỳ phát triển Nâng cao chất lượng mã và tăng cường tư thế bảo mật Tự Động Hóa Trên Toàn Bộ Vòng Đời Phát Triển\nTự động hóa các tác vụ thường xuyên trên toàn bộ vòng đời phát triển Giảm công việc thủ công, lặp lại Cho phép các nhà phát triển tập trung vào các tác vụ sáng tạo có giá trị cao hơn Cải thiện năng suất và hiệu quả tổng thể Thực Tiễn Tốt Nhất Khi Sử Dụng Amazon Q Developer Cung Cấp Ngữ Cảnh Rõ Ràng - Cung cấp cho Q thông tin chi tiết về dự án, kiến trúc và yêu cầu của bạn Sử Dụng Prompt Cụ Thể - Thay vì các yêu cầu mơ hồ, cung cấp các prompt cụ thể, chi tiết với các ví dụ Xem Xét Các Đề Xuất - Luôn xem xét các đề xuất của Q trước khi triển khai chúng Lặp Lại \u0026amp; Tinh Chỉnh - Nếu đề xuất đầu tiên không hoàn hảo, tinh chỉnh prompt của bạn và thử lại Tận Dụng Kiến Thức AWS - Tận dụng sự hiểu biết sâu sắc của Q về các dịch vụ AWS và thực tiễn tốt nhất 3. Bản Trình Diễn Kiro Kiro Là Gì? Kiro là một IDE tác nhân (Integrated Development Environment) được phát triển bởi Amazon Web Services lấp khoảng cách giữa việc tạo prototype nhanh được hỗ trợ bởi AI và phát triển phần mềm sẵn sàng cho sản xuất. Nó hiện đang ở giai đoạn xem trước công khai.\nTriết Lý Cốt Lõi Kiro thể hiện nguyên tắc rằng AI nên nâng cao năng suất của nhà phát triển trong khi duy trì các tiêu chuẩn chuyên nghiệp, cấu trúc rõ ràng, kiểm thử toàn diện, tài liệu và khả năng bảo trì lâu dài.\nCác Tính Năng Chính Phát Triển Theo Đặc Tả\nKhi bạn gửi yêu cầu (ví dụ: \u0026ldquo;thêm hệ thống đánh giá sản phẩm\u0026rdquo;), Kiro chuyển đổi nó thành: User stories với tiêu chí chấp nhận rõ ràng Tài liệu thiết kế Danh sách tác vụ và kế hoạch triển khai Thông số kỹ thuật có cấu trúc trước khi tạo mã Agent Hooks \u0026amp; Tự Động Hóa\nTự động kích hoạt các tác vụ dựa trên các sự kiện: Lưu tệp kích hoạt cập nhật tài liệu Commit kích hoạt tạo bài kiểm tra Các hành động cụ thể kích hoạt tối ưu hóa hiệu năng Giảm công việc thủ công, lặp lại Steering \u0026amp; Ngữ Cảnh Dự Án\nTạo các tệp steering (markdown) để mô tả: Cấu trúc và tổ chức dự án Tiêu chuẩn mã hóa và quy ước Các mô hình kiến trúc mong muốn Hướng dẫn nhóm và thực tiễn tốt nhất Giúp Kiro hiểu sâu sắc ngữ cảnh dự án của bạn Phân Tích Đa Tệp \u0026amp; Hiểu Ý Định\nPhân tích nhiều tệp đồng thời Hiểu các mục tiêu chức năng trên toàn bộ cơ sở mã Thực hiện các thay đổi phù hợp với các mục tiêu dự án tổng thể Vượt ra ngoài hoàn thành mã đơn giản Tích Hợp VS Code\nĐược xây dựng dựa trên nền tảng mã nguồn mở của VS Code Nhập cài đặt, chủ đề và tiện ích mở rộng từ VS Code Giao diện quen thuộc cho người dùng VS Code hiện có Chuyển đổi liền mạch cho các nhà phát triển Lựa Chọn Mô Hình AI Linh Hoạt\nHiện sử dụng Claude Sonnet 4 làm mặc định Chế độ \u0026ldquo;Auto\u0026rdquo; kết hợp nhiều mô hình dựa trên ngữ cảnh Cân bằng giữa chất lượng và chi phí Linh hoạt để chọn các mô hình khác nhau cho các tác vụ khác nhau Ưu Điểm Của Việc Sử Dụng Kiro Tăng Tính Minh Bạch \u0026amp; Kiểm Soát\nBắt đầu với các thông số kỹ thuật trước khi tạo mã Xem xét và xác thực các thông số kỹ thuật trước khi triển khai Giảm mã \u0026ldquo;ảo tưởng\u0026rdquo; hoặc triển khai không phù hợp Duy trì khả năng truy xuất rõ ràng từ yêu cầu đến mã Giảm Boilerplate \u0026amp; Các Tác Vụ Lặp Lại\nAgent hooks tự động hóa tạo tài liệu Tạo bài kiểm tra đơn vị tự động Cập nhật thông tin tự động Giải phóng các nhà phát triển cho công việc có giá trị cao hơn Bảo Mật \u0026amp; Quyền Riêng Tư\nHầu hết các hoạt động mã xảy ra cục bộ Dữ liệu chỉ được gửi bên ngoài với sự cho phép rõ ràng Duy trì kiểm soát thông tin nhạy cảm Khả Năng Mở Rộng \u0026amp; Linh Hoạt\nTích hợp các công cụ bên ngoài thông qua MCP (Model Context Protocol) Hỗ trợ nhiều mô hình AI Không bị ràng buộc vào một môi trường AI duy nhất Thích ứng với các quy trình làm việc nhóm khác nhau Hạn Chế \u0026amp; Cân Nhắc Trạng Thái Xem Trước - Vẫn ở giai đoạn xem trước công khai; tính ổn định và tính năng có thể thay đổi Các Dự Án Phức Tạp - Có thể gặp khó khăn trong việc hiểu ngữ cảnh sâu sắc trong các dự án rất phức tạp Cần Giám Sát - Người dùng vẫn cần giám sát và xác thực các quyết định của AI Giá Cả Trong Tương Lai - Các tầng giá dự kiến: Miễn phí: ~50 tác vụ/tháng Pro: ~1.000 tác vụ/tháng Pro+: ~3.000 tác vụ/tháng Khi Nào Nên Sử Dụng Kiro Bạn muốn một quy trình làm việc AI + lập trình duy trì tính chuyên nghiệp và cấu trúc rõ ràng Xây dựng prototype nhanh nhưng lo ngại về tính bền vững sản xuất Khám phá cách AI có thể trở thành một đồng nghiệp lập trình thực sự, không chỉ là công cụ gợi ý mã Bạn cần phát triển theo đặc tả với tài liệu và kiểm thử tự động Các Lỗi Thường Gặp Khi Sử Dụng AI Trong Phát Triển 1. Mong Đợi AI Xử Lý Mọi Thứ Vấn Đề: Nhiều nhà phát triển mong đợi AI hoàn thành toàn bộ dự án một cách tự chủ.\nGiải Pháp: Luôn tạo kế hoạch trước và xem xét thường xuyên. AI là công cụ để nâng cao năng suất, không phải thay thế phán đoán của nhà phát triển.\n2. Tỷ Lệ Lỗi Cao Vấn Đề: AI có thể mắc lỗi, đặc biệt là trong các tình huống phức tạp.\nGiải Pháp: Triển khai các chu kỳ xem xét thường xuyên. Xác thực tất cả mã do AI tạo trước khi triển khai.\n3. Thiếu Yêu Cầu Rõ Ràng Vấn Đề: Yêu cầu mơ hồ hoặc không rõ ràng dẫn đến đầu ra AI mơ hồ.\nGiải Pháp: Viết yêu cầu với độ chính xác. Hợp tác với AI để tạo các thông số kỹ thuật chi tiết trước khi triển khai.\n4. Ràng Buộc Tiêu Cực Thay Vì Hướng Dẫn Tích Cực Vấn Đề: Bảo AI \u0026ldquo;đừng làm cái này\u0026rdquo; ít hiệu quả hơn \u0026ldquo;làm cái này\u0026rdquo;.\nGiải Pháp: Sử dụng các hướng dẫn tích cực, cụ thể. Tỷ lệ thành công cao hơn đến từ hướng dẫn tích cực rõ ràng.\n5. Ngữ Cảnh Dự Án Không Đủ Vấn Đề: AI không hiểu các yêu cầu và ràng buộc độc đáo của dự án của bạn.\nGiải Pháp: Tạo các tệp steering, cung cấp ngữ cảnh chi tiết và đặt những câu hỏi quan trọng cho AI về dự án của bạn.\n6. Coi AI Là Người Quản Lý Vấn Đề: Để AI quyết định tất cả về hướng dự án và kiến trúc.\nGiải Pháp: Nhớ: Bạn là người quản lý. Giá trị của bạn nằm ở xác thực mã và giám sát dự án, không phải viết từng dòng mã.\nNhững Điểm Chính Rút Ra AI Là Trợ Lý Của Bạn - Duy trì kiểm soát các quyết định dự án và hướng triển khai\nLập Kế Hoạch Trước, Mã Sau - Luôn tạo kế hoạch toàn diện trước khi triển khai\nHợp Tác Hơn Tự Động Hóa - AI nên đặt câu hỏi và hợp tác, không chỉ thực thi lệnh\nYêu Cầu Rõ Ràng Quan Trọng - Độ chính xác trong yêu cầu dẫn đến đầu ra AI tốt hơn\nXem Xét Thường Xuyên Là Cần Thiết - Đừng mong đợi AI hoàn hảo; xem xét và xác thực liên tục\nBạn Là Người Quản Lý Mã - Giá trị của bạn nằm ở xác thực và giám sát, không phải viết từng dòng\nSử Dụng Prompt Có Cấu Trúc - Các mẫu với ngữ cảnh, user stories và yêu cầu mang lại kết quả tốt hơn\nXuất Kế Hoạch Thành Tệp - Tạo các tài liệu sống bạn có thể tham chiếu và sửa đổi\nHướng Dẫn Tích Cực Hiệu Quả Hơn - Bảo AI phải làm gì, không phải tránh làm gì\nKinh Nghiệm Quan Trọng - Sử dụng các công cụ này thực hành để hiểu khả năng và hạn chế của chúng\nCông Cụ \u0026amp; Tài Nguyên Được Đề Xuất Amazon Q Developer - Trợ lý phát triển được hỗ trợ bởi AI tích hợp với các dịch vụ AWS Kiro IDE - Môi trường phát triển theo đặc tả với hợp tác AI AWS CodeWhisperer - Công cụ tạo mã và tối ưu hóa MCP (Model Context Protocol) - Khung công tác để tích hợp các công cụ và dịch vụ bên ngoài Kết Luận Vòng Đời Phát Triển Do AI Điều Khiển đại diện cho một mô hình mới trong kỹ thuật phần mềm nơi AI và con người hợp tác như những người bình đẳng. Thành công đòi hỏi lập kế hoạch rõ ràng, xem xét thường xuyên, yêu cầu chính xác và duy trì kiểm soát của nhà phát triển đối với hướng dự án. Các công cụ như Amazon Q Developer và Kiro đang cho phép quy trình làm việc mới này, nhưng chúng hoạt động tốt nhất khi các nhà phát triển hiểu khả năng và hạn chế của chúng, và duy trì vai trò của họ là người quản lý dự án và xác thực mã.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-api-gateway/5.4.2-add-post/",
	"title": "Thêm phương thức POST",
	"tags": [],
	"description": "",
	"content": "Cấu hình POST /hello Chọn resource /hello → Add method → POST → Lambda Function. Chọn cùng Lambda đã dùng cho GET. Dùng Lambda proxy integration để hàm nhận đầy đủ payload; trong handler parse event.body (JSON). Triển khai \u0026amp; kiểm thử Deploy lại stage dev. Gọi thử: curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;api-user\u0026#34;}\u0026#39; \\ \u0026#34;\u0026lt;invoke-url\u0026gt;/hello\u0026#34; Đảm bảo response JSON trả lời đúng tên; kiểm tra log CloudWatch nếu lỗi parse. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.2-blog2/",
	"title": "Thông báo Amazon EC2 M4 và M4 Pro Mac instances",
	"tags": [],
	"description": "",
	"content": "Thông báo Amazon EC2 M4 và M4 Pro Mac instances bởi Sébastien Stormacq vào ngày 12 THÁNG 9 2025 trong Amazon EC2 Mac Instances, Launch, News Permalink Comments\nPermalink Comments Share\nVoiced by Polly\nLà người đã sử dụng macOS từ năm 2001 và các Amazon EC2 Mac instances từ khi chúng ra mắt 4 năm trước, tôi đã giúp nhiều khách hàng mở rộng các pipeline tích hợp \u0026amp; phân phối liên tục (CI/CD) trên AWS. Hôm nay, tôi rất hào hứng chia sẻ rằng các instance Amazon EC2 M4 và M4 Pro Mac hiện đã khả dụng chính thức.\nCác nhóm phát triển xây ứng dụng cho các nền tảng Apple cần tài nguyên tính toán mạnh để xử lý các quy trình build phức tạp và chạy nhiều giả lập iOS cùng lúc. Khi các dự án phát triển ngày càng lớn và tinh vi, các nhóm cần hiệu năng và dung lượng bộ nhớ cao hơn để duy trì chu kỳ phát triển nhanh.\nApple M4 Mac mini làm lõi Các instance EC2 M4 Mac (được gọi là mac-m4.metal trong API) được xây dựng dựa trên máy Apple M4 Mac mini và sử dụng hệ thống AWS Nitro System. Chúng có chip Apple silicon M4 với 10 lõi CPU (bốn lõi hiệu năng và sáu lõi hiệu quả), GPU 10 lõi, Neural Engine 16 lõi, và bộ nhớ hợp nhất 24 GB, mang lại hiệu năng cải thiện cho các workload build ứng dụng iOS và macOS. Khi xây dựng và kiểm thử ứng dụng, các instance M4 Mac cho hiệu năng build ứng dụng tốt hơn tới 20% so với các instance EC2 M2 Mac.\nInstance EC2 M4 Pro Mac ( mac-m4pro.metal trong API ) được trang bị chip Apple silicon M4 Pro với 14 lõi CPU, 20 lõi GPU, Neural Engine 16 lõi và bộ nhớ hợp nhất 48 GB. Những instance này cung cấp hiệu năng build ứng dụng tốt hơn tới 15% so với các instance EC2 M2 Pro Mac. Thêm dung lượng bộ nhớ và công suất tính toán cho phép chạy nhiều bài kiểm thử song song bằng nhiều giả lập thiết bị.\nMỗi instance M4 và M4 Pro Mac giờ đây đi kèm với 2 TB lưu trữ nội bộ (local storage), cung cấp lưu trữ độ trễ thấp để cải thiện caching và hiệu năng build \u0026amp; test.\nCả hai loại instance đều hỗ trợ macOS Sonoma phiên bản 15.6 và mới hơn như các AMI (Amazon Machine Images (AMIs).). Hệ thống AWS Nitro cung cấp băng thông mạng Amazon Virtual Private Cloud (Amazon VPC) lên đến 10 Gbps và băng thông lưu trữ Amazon Elastic Block Store (Amazon EBS) 8 Gbps qua kết nối Thunderbolt tốc độ cao.\nCác instance EC2 Mac tích hợp liền mạch với các dịch vụ AWS, nghĩa là bạn có thể:\nXây dựng pipeline CI/CD tự động sử dụng AWS CodeBuild và AWS CodePipeline\nLưu trữ và quản lý nhiều phiên bản bí mật build của bạn, như chứng chỉ phát triển Apple và khóa, trên AWS Secrets Manager\nQuản lý hạ tầng phát triển của bạn bằng AWS CloudFormation\nGiám sát hiệu năng instance với Amazon CloudWatch\nCách bắt đầu Bạn có thể khởi chạy một instance EC2 M4 hoặc M4 Pro Mac qua AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs.\nVí dụ trong demo này, tôi sẽ khởi động một instance M4 Pro từ console. Tôi đầu tiên cấp phát một dedicated host để chạy các instance của mình. Trên AWS Management Console tôi vào EC2, rồi Dedicated Hosts, và chọn Allocate Dedicated Host.\nRồi, tôi nhập tag Name và chọn Family instance (mac‑m4pro) và loại instance (mac‑m4pro.metal). Tôi chọn một Availability Zone và bỏ chọn Host maintenance.\nEC2 Mac M$ – Dedicated hosts\nHoặc tôi có thể dùng CLI:\naws ec2 allocate-hosts \\\n--availability-zone-id \u0026ldquo;usw2-az4\u0026rdquo; \\\n--auto-placement \u0026ldquo;off\u0026rdquo; \\\n--host-recovery \u0026ldquo;off\u0026rdquo; \\\n--host-maintenance \u0026ldquo;off\u0026rdquo; \\\n--quantity 1 \\\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo;\nSau khi host dedicated được cấp cho tài khoản của tôi, tôi chọn host vừa cấp, rồi chọn menu Actions và chọn Launch instance(s) onto host.\nLưu ý console cung cấp cho bạn, bên cạnh các thông tin khác, các phiên bản macOS hỗ trợ mới nhất cho loại host này. Trong trường hợp này, là macOS 15.6.\nTrên trang Launch an instance, tôi nhập Name. Tôi chọn một AMI macOS Sequoia. Tôi đảm bảo Architecture là Arm 64-bit và loại instance là mac-m4pro.metal.\nPhần còn lại các tham số không đặc thù cho EC2 Mac: cấu hình mạng và lưu trữ. Khi khởi động một instance dùng cho phát triển, hãy chắc chọn volume tối thiểu 200 GB trở lên. Volume mặc định 100 GB không đủ để tải xuống và cài Xcode.\nKhi đã sẵn sàng, tôi nhấn nút Launch instance màu cam ở cuối trang. Instance sẽ nhanh chóng xuất hiện ở trạng thái Running trong console. Tuy nhiên, có thể mất tới 15 phút để bạn có thể kết nối qua SSH.\nHoặc tôi có thể dùng lệnh này:\naws ec2 run-instances \\\n--image-id \u0026ldquo;ami-000420887c24e4ac8\u0026rdquo; \\ # ID AMI tùy vùng !\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo; \\\n--key-name \u0026ldquo;my-ssh-key-name\u0026rdquo; \\\n--network-interfaces \u0026lsquo;{\u0026ldquo;AssociatePublicIpAddress\u0026rdquo;:true,\u0026ldquo;DeviceIndex\u0026rdquo;:0,\u0026ldquo;Groups\u0026rdquo;:[\u0026ldquo;sg-0c2f1a3e01b84f3a3\u0026rdquo;]}\u0026rsquo; \\ # Security Group ID phụ thuộc config của bạn \\\n--tag-specifications \u0026lsquo;{\u0026ldquo;ResourceType\u0026rdquo;:\u0026ldquo;instance\u0026rdquo;,\u0026ldquo;Tags\u0026rdquo;:[{\u0026ldquo;Key\u0026rdquo;:\u0026ldquo;Name\u0026rdquo;,\u0026ldquo;Value\u0026rdquo;:\u0026ldquo;My Dev Server\u0026rdquo;}]}\u0026rsquo; \\\n--placement \u0026lsquo;{\u0026ldquo;HostId\u0026rdquo;:\u0026ldquo;h-0e984064522b4b60b\u0026rdquo;,\u0026ldquo;Tenancy\u0026rdquo;:\u0026ldquo;host\u0026rdquo;}\u0026rsquo; \\ # Host ID tùy config của bạn --private-dns-name-options \u0026lsquo;{\u0026ldquo;HostnameType\u0026rdquo;:\u0026ldquo;ip-name\u0026rdquo;,\u0026ldquo;EnableResourceNameDnsARecord\u0026rdquo;:true,\u0026ldquo;EnableResourceNameDnsAAAARecord\u0026rdquo;:false}\u0026rsquo; \\\n--count \u0026ldquo;1\u0026rdquo;\nCài Xcode từ Terminal Sau khi instance có thể truy cập, tôi có thể kết nối bằng SSH và cài công cụ phát triển. Tôi dùng xcodeinstall để tải và cài Xcode 16.4.\nTừ laptop của tôi, tôi mở session với credentials Apple developer:\n# on my laptop, with permissions to access AWS Secret Manager\n» xcodeinstall authenticate -s eu-central-1\nRetrieving Apple Developer Portal credentials\u0026hellip;\nAuthenticating\u0026hellip;\n🔐 Two factors authentication is enabled, enter your 2FA code: 067785\n✅ Authenticated with MFA.\nTôi kết nối với EC2 Mac instance cái mà tôi vừa mới launched. Sau đó, tôi tải và cài đặc Xcode:\n» ssh ec2-user@44.234.115.119\nWarning: Permanently added \u0026lsquo;44.234.115.119\u0026rsquo; (ED25519) to the list of known hosts.\nLast login: Sat Aug 23 13:49:55 2025 from 81.49.207.77\n┌───┬──┐ \\_\\_| \\_\\_|\\_ ) │ ╷╭╯╷ │ \\_| ( / │ └╮ │ \\_\\_\\_|\\\\\\_\\_\\_|\\_\\_\\_| │ ╰─┼╯ │ Amazon EC2 └───┴──┘ macOS Sequoia 15.6\rec2-user@ip-172-31-54-74 ~ % brew tap sebsto/macos\n==\u0026gt; Tapping sebsto/macos\nCloning into \u0026lsquo;/opt/homebrew/Library/Taps/sebsto/homebrew-macos\u0026rsquo;\u0026hellip;\nremote: Enumerating objects: 227, done.\nremote: Counting objects: 100% (71/71), done.\nremote: Compressing objects: 100% (57/57), done.\nremote: Total 227 (delta 22), reused 63 (delta 14), pack-reused 156 (from 1)\nReceiving objects: 100% (227/227), 37.93 KiB | 7.59 MiB/s, done.\nResolving deltas: 100% (72/72), done.\nTapped 1 formula (13 files, 61KB).\nec2-user@ip-172-31-54-74 ~ % brew install xcodeinstall\n==\u0026gt; Fetching downloads for: xcodeinstall\n==\u0026gt; Fetching sebsto/macos/xcodeinstall\n==\u0026gt; Downloading https://github.com/sebsto/xcodeinstall/releases/download/v0.12.0/xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\nAlready downloaded: /Users/ec2-user/Library/Caches/Homebrew/downloads/9f68a7a50ccfdc479c33074716fd654b8528be0ec2430c87bc2b2fa0c36abb2d\u0026ndash;xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\n==\u0026gt; Installing xcodeinstall from sebsto/macos\n==\u0026gt; Pouring xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\n🍺 /opt/homebrew/Cellar/xcodeinstall/0.12.0: 8 files, 55.2MB\n==\u0026gt; Running `brew cleanup xcodeinstall`\u0026hellip;\nDisable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\nHide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n==\u0026gt; No outdated dependents to upgrade!\nec2-user@ip-172-31-54-74 ~ % xcodeinstall download -s eu-central-1 -f -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nDownloading Xcode 16.4\n100% [============================================================] 2895 MB / 180.59 MBs\n[ OK ]\n✅ Xcode 16.4.xip downloaded\nec2-user@ip-172-31-54-74 ~ % xcodeinstall install -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nInstalling\u0026hellip;\n[1/6] Expanding Xcode xip (this might take a while)\n[2/6] Moving Xcode to /Applications\n[3/6] Installing additional packages\u0026hellip; XcodeSystemResources.pkg\n[4/6] Installing additional packages\u0026hellip; CoreTypes.pkg\n[5/6] Installing additional packages\u0026hellip; MobileDevice.pkg\n[6/6] Installing additional packages\u0026hellip; MobileDeviceDevelopment.pkg\n[ OK ]\n✅ file:///Users/ec2-user/.xcodeinstall/download/Xcode%2016.4.xip installed\nec2-user@ip-172-31-54-74 ~ % sudo xcodebuild -license accept\nec2-user@ip-172-31-54-74 ~ %\nNhững điều cần biết Chọn volume EBS tối thiểu 200 GB cho mục đích phát triển. Volume mặc định 100 GB không đủ để cài Xcode. Tôi thường chọn 500 GB. Khi tăng kích thước EBS sau khi instance đã khởi chạy, nhớ to resize the APFS filesystem.\nNgoài ra, bạn có thể chọn cài công cụ phát triển và framework của bạn lên ổ SSD nội bộ 2 TB độ trễ thấp có sẵn trong Mac mini. Lưu ý rằng nội dung volume này gắn với vòng đời instance, không với dedicated host. Nghĩa là mọi thứ sẽ bị xóa khỏi ổ SSD nội bộ khi bạn dừng và khởi động lại instance.\nCác instance mac-m4.metal và mac-m4pro.metal hỗ trợ macOS Sequoia 15.6 và các phiên bản mới hơn.\nBạn có thể di chuyển các instance EC2 Mac hiện tại khi instance di chuyển đang chạy macOS 15 (Sequoia). Tạo một AMI tùy chỉnh từ instance hiện tại và khởi động một instance M4 hoặc M4 Pro từ AMI đó.\nCuối cùng, tôi gợi ý bạn xem các hướng dẫn tôi viết để giúp bạn bắt đầu với EC2 Mac:\nKhởi động một instance EC2 Mac\nKết nối tới instance EC2 Mac (tôi chỉ bạn ba cách khác nhau để kết nối)\nXây ứng dụng nhanh hơn với pipeline CI/CD trên EC2 Mac\nGiá cả và khả dụng Các instance EC2 M4 và M4 Pro Mac hiện có tại US East (N. Virginia) và US West (Oregon), dự kiến mở rộng sang các vùng khác trong tương lai.\nCác instance EC2 Mac có thể mua dưới dạng Dedicated Hosts theo mô hình giá On-Demand và Savings Plans. Việc tính phí cho EC2 Mac là theo giây với mức tối thiểu 24 giờ cấp phát để tuân theo Thỏa thuận Bản quyền phần mềm macOS của Apple. Sau khoảng thời gian tối thiểu 24 giờ, host có thể được giải phóng bất cứ lúc nào mà không cần cam kết tiếp.\nLà người làm việc chặt với các nhà phát triển Apple, tôi tò mò xem bạn sẽ dùng các instance mới này như thế nào để tăng tốc chu kỳ phát triển của bạn. Sự kết hợp giữa hiệu năng gia tăng, dung lượng bộ nhớ cải thiện và tích hợp với dịch vụ AWS mở ra nhiều khả năng mới cho các đội xây ứng dụng cho iOS, macOS, iPadOS, tvOS, watchOS, và visionOS. Ngoài phát triển ứng dụng, Neural Engine của Apple silicon khiến các instance này là ứng viên hiệu quả chi phí để chạy workload inference machine learning (ML). Tôi sẽ thảo luận chi tiết chủ đề này tại AWS re:Invent 2025, nơi tôi sẽ chia sẻ benchmark và best practices để tối ưu workload ML trên EC2 Mac.\nĐể tìm hiểu thêm về các instance EC2 M4 và M4 Pro Mac, bạn có thể truy cập trang Amazon EC2 Mac Instances hoặc tham khảo EC2 Mac documentation. Bạn có thể bắt đầu sử dụng các instance này ngay hôm nay để hiện đại hóa workflow phát triển Apple trên AWS.\n— seb\nGiới thiệu tác giả Sébastien Stormacq\rSeb đã viết code từ khi chạm Commodore 64 vào những năm tám mươi. Anh truyền cảm hứng cho cộng đồng xây dựng khai phá giá trị của AWS Cloud bằng sự kết hợp giữa đam mê, nhiệt huyết, advocacy khách hàng, tò mò và sáng tạo. Các mối quan tâm của anh là kiến trúc phần mềm, công cụ dành cho nhà phát triển và điện toán di động. Nếu muốn bán thứ gì đó cho Seb, hãy chắc chắn rằng nó có API. Theo dõi @sebsto trên Bluesky, X, Mastodon và các nền tảng khác.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/",
	"title": "Tuần 2 - Dịch vụ Mạng trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-15 đến 2025-09-19\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 2 Tuần này đào sâu các dịch vụ mạng của AWS, từ VPC cơ bản đến giải pháp kết nối nâng cao và cân bằng tải.\nNội dung chính Amazon VPC và Subnet. Security Group và Network ACL. Internet Gateway, NAT Gateway. VPC Peering và AWS Transit Gateway. Elastic Load Balancing (ALB, NLB, GWLB). Labs thực hành Lab 03: Amazon VPC \u0026amp; Networking Basics. Lab 10: Hybrid DNS (Route 53 Resolver). Lab 19: VPC Peering. Lab 20: AWS Transit Gateway. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.3-blog3/",
	"title": "Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD bởi Suyash Nadkarni và Dylan Souvage vào ngày 12 THÁNG 9 2025 trong Amazon EC2, Best Practices, Expert (400), Technical\nKhi các tổ chức di chuyển nhiều khối lượng công việc quan trọng sang đám mây, tối ưu hóa về giá — hiệu suất (price-performance) trở thành một cân nhắc chủ chốt. Các instance Amazon Elastic Compute Cloud(Amazon EC2) sử dụng bộ xử lý AMD EPYC đem lại mật độ lõi cao, băng thông bộ nhớ lớn và các tính năng bảo mật được hỗ trợ phần cứng, khiến chúng trở thành một lựa chọn mạnh mẽ cho nhiều loại khối lượng công việc tính toán, bộ nhớ hoặc I/O. Trong bài viết này, chúng tôi giải thích cách chọn loại instance Amazon EC2 dựa trên AMD phù hợp và mô tả các kỹ thuật điều chỉnh có thể giúp người dùng cải thiện hiệu quả khối lượng công việc. Cho dù bạn đang chạy mô phỏng, phân tích quy mô lớn hoặc các khối lượng inference, bài viết này cung cấp hướng dẫn thực tiễn để tối ưu hóa instance Amazon EC2 dùng AMD.\nAmazon EC2 cung cấp instances AMD dựa trên nhiều thế hệ AMD EPYC. Bài viết tập trung vào chiến lược tối ưu cho thế hệ 3 và 4, vốn tăng cường khả năng cho workload tính toán và bộ nhớ chuyên sâu.\nThế hệ 3 (M6a, R6a, C6a, Hpc6a): Cân bằng tính toán, bộ nhớ và lưu trữ — phù hợp với phân tích dữ liệu, máy chủ web và tính toán hiệu năng cao.\nThế hệ thứ 4 (M7a, R7a, C7a, Hpc7a): Cung cấp hiệu suất tốt hơn tới 50% so với các thế hệ AMD trước đó. Các instance này giới thiệu hỗ trợ AVX‑512, bộ nhớ DDR5 và Simultaneous Multithreading (SMT) bị tắt; SMT là công nghệ cho phép một lõi vật lý chạy nhiều luồng cùng lúc; với SMT bị tắt, mỗi vCPU (virtual CPU) ánh xạ trực tiếp đến một lõi vật lý, điều này có thể cải thiện tính cô lập và nhất quán trong khối lượng công việc.\nChọn loại instance Amazon EC2 dùng AMD EPYC phù hợp Việc chọn loại instance Amazon EC2 dùng AMD EPYC phù hợp bắt đầu bằng việc hiểu cách ứng dụng của bạn sử dụng tài nguyên tính toán (compute), bộ nhớ, lưu trữ và mạng. Mỗi instance family được tối ưu hóa cho những đặc tính khối lượng công việc nhất định.\nKhối lượng công việc tính toán\nNhững khối lượng này liên quan tới các phép tính quy mô lớn, mô phỏng, hoặc mã hóa, và thường cần thông lượng CPU cao và hỗ trợ tập lệnh nâng cao.\nKhuyến nghị: C7a, Hpc7a, C6a, Hpc6a\nTình huống dùng: điện toán khoa học, mô hình tài chính, chuyển mã media, mã hóa, inference ML\nBig Data \u0026amp; Analytics\nỨng dụng xử lý và phân tích tập dữ liệu lớn hưởng lợi từ băng thông bộ nhớ cao và tỷ lệ tính toán‑bộ nhớ cân bằng.\nKhuyến nghị: R7a, M7a, R6a, M6a\nTình huống dùng: xử lý luồng, phân tích thời gian thực, công cụ business intelligence, caching phân tán\nKhối lượng công việc cơ sở dữ liệu\nCông việc database thường cần hiệu suất bộ nhớ ổn định và throughput I/O cao cho các hoạt động đọc/ghi.\nKhuyến nghị: R7a, M7a, R6a, M6a\nTình huống dùng: database quan hệ (MySQL, PostgreSQL), NoSQL (MongoDB, Cassandra), database trong bộ nhớ (Redis)\nWeb và máy chủ ứng dụng\nNhững ứng dụng này xử lý các tải yêu cầu biến đổi và hưởng lợi từ sự cân bằng giữa tính toán, bộ nhớ và hiệu năng mạng.\nKhuyến nghị: C7a, M7a, C6a, M6a\nTình huống dùng: máy chủ web, hệ quản lý nội dung, nền tảng e‑commerce, các điểm cuối API\nAI/ML trên CPU\nCác tác vụ ML không cần GPU — như inference hoặc tiền xử lý — có thể chạy hiệu quả trên các instance dựa CPU.\nKhuyến nghị: M7a, R7a, C7a\nTình huống dùng: inference mô hình, xử lý ngôn ngữ tự nhiên, thị giác máy tính, hệ gợi ý\nHigh Performance Computing\nNhững khối lượng công việc này cần nhiều lõi, băng thông bộ nhớ cao và mạng độ trễ thấp cho các tính toán liên kết chặt chẽ.\nKhuyến nghị: Hpc7a, Hpc6a, R7a, M7a\nTình huống dùng: động lực chất lỏng, genomics, phân tích địa chấn, mô phỏng kỹ thuật\nViệc phù hợp hóa loại instance với nhu cầu khối lượng công việc giúp cung cấp hiệu suất dự đoán được và hiệu quả chi phí. Các dịch vụ như Amazon EC2 Auto Scaling và AWS Compute Optimizer có thể hỗ trợ trong việc lựa chọn instance và ra quyết định scale liên tục.\nTối ưu hóa các instance Amazon EC2 dùng AMD EPYC Các instance EC2 dùng bộ xử lý AMD EPYC thế hệ thứ 4 vận hành với kiến trúc “chiplet modular”, như minh họa trong hình dưới đây. Mỗi bộ xử lý bao gồm nhiều Core Complex Dies (CCD), và mỗi CCD chứa một hoặc nhiều tổ hợp lõi (core complexes, gọi là CCX). Một CCX gom tối đa tám lõi vật lý, mỗi lõi có 1 MB bộ nhớ đệm L2 riêng và tám lõi đó cùng chia sẻ 32 MB bộ nhớ đệm L3. Các CCD này được kết nối với một die I/O trung tâm, chịu trách nhiệm quản lý bộ nhớ và liên kết giữa các chip.\n(Biểu đồ 1: Sơ đồ của die CPU ‘Zen 4’ với 8 lõi mỗi die)\nKiến trúc module của các bộ xử lý AMD EPYC thế hệ thứ 4 cho phép các instance như m7a.24xlarge và m7a.48xlarge hỗ trợ số lượng lõi cao — lên đến 96 lõi vật lý mỗi socket. Ví dụ:\nm7a.24xlarge cung cấp 96 lõi vật lý từ một socket đơn\nm7a.48xlarge trải rộng hai socket, cung cấp 192 lõi vật lý\nHiểu cách các kích cỡ instance của EC2 ánh xạ tới bố cục bộ xử lý vật lý có thể giúp bạn tối ưu hóa hiệu suất và tính nhất quán bộ nhớ đệm (cache). Những khối lượng công việc liên quan tới truy cập bộ nhớ chia sẻ hoặc đồng bộ hóa luồng — như HPC hoặc database trong bộ nhớ — có thể hưởng lợi khi chọn kích cỡ instance giảm thiểu giao tiếp giữa socket và tận dụng hiệu quả bộ nhớ đệm L3.\n(Biểu đồ 2: Bố cục CPU ‘EPYC Chiplet’)\nCác instance EC2 dùng AMD EPYC thế hệ thứ 4 hoạt động với SMT bị tắt. Trong cấu hình này, mỗi vCPU ánh xạ trực tiếp tới một lõi vật lý, loại bỏ chia sẻ tài nguyên như đơn vị thực thi và bộ nhớ đệm giữa các luồng “chị/em”. Thiết kế này có thể giảm nhiễu nội lõi và giúp cung cấp hiệu suất ổn định hơn cho các khối lượng công việc nhất định. Người dùng có thể cô lập luồng ở cấp lõi và quan sát độ biến thiên thấp hơn và thông lượng ổn định hơn cho các khối lượng như HPC, inference ML và database giao dịch.\nCPU optimizations Các công cụ như htop giúp xác định mẫu sử dụng CPU, trung bình tải hệ thống, và tiêu thụ tài nguyên theo tiến trình. Việc sử dụng CPU nên được đánh giá trong ngữ cảnh khối lượng công việc và yêu cầu hiệu năng. Nếu mức sử dụng liên tục đạt 100%, điều đó có thể chỉ ra rằng khối lượng công việc bị CPU-bound và chưa được cân bằng tối ưu. Trước khi thay đổi kích thước instance, bật Auto Scaling, hoặc chuyển đổi giữa các gia đình instance, cần thực hiện đánh giá các cơ hội tuning có thể cải thiện hiệu suất mà không thay đổi hạ tầng. Trung bình tải (load averages) vượt thường xuyên hơn số lượng vCPU cũng có thể là dấu hiệu bão hòa tính toán và có thể yêu cầu tối ưu tiếp.\nSử dụng bộ nhớ đệm L3 L3 cache là lớp nhớ nhanh chia sẻ được sử dụng bởi một nhóm lõi CPU. Trên EC2 dựa AMD, các lõi được tổ chức thành các slice bộ nhớ đệm L3, mỗi slice được chia sẻ bởi một tập hợp lõi trên cùng một socket. Các luồng được lập lịch trong cùng slice có thể truy cập dữ liệu chia sẻ hiệu quả hơn, giảm độ trễ bộ nhớ. Trên các instance AMD thế hệ 4 như m7a.2xlarge hoặc r7a.2xlarge, tất cả vCPU thường ánh xạ tới các lõi nằm trong cùng một slice L3, đảm bảo tính nhất quán địa phương bộ nhớ đệm. Đối với các kích cỡ lớn hơn (ví dụ m7a.8xlarge trở lên), thread pinning — gán các luồng tới lõi vật lý cụ thể — có thể giúp duy trì tính địa phương này. Thread pinning có thể giảm biến động hiệu suất trong các khối lượng với mẫu truy cập bộ nhớ chia sẻ thường xuyên.\nBạn có thể pin luồng với lệnh:\ntaskset -c 0-3 ./your_application\nVí dụ này pin ứng dụng của bạn vào các lõi CPU 0 đến 3. Để xác định lõi nào chia sẻ cùng vùng bộ nhớ đệm L3, sử dụng các công cụ như lscpu hoặc lstopo để kiểm tra topology CPU hệ thống. Gom các luồng liên quan vào các lõi cùng chia sẻ L3 cache có thể cải thiện tính nhất quán hiệu suất cho các khối lượng có truy cập bộ nhớ chia sẻ.\nTối ưu container Docker Trong môi trường container chạy trên các instance EC2 dựa AMD, điều chỉnh các thiết lập liên quan CPU có thể cải thiện tính nhất quán và hiệu quả khối lượng công việc — đặc biệt cho các ứng dụng tính toán nặng hoặc nhạy độ trễ. Mặc dù cấu hình mặc định hoạt động cho nhiều kịch bản tổng quát, một số workload có thể lợi từ việc kiểm soát rõ ràng cách phân bổ tài nguyên CPU. Theo mặc định, runtime container như Docker cho phép hệ điều hành lập lịch container trên bất kỳ lõi CPU nào sẵn có. Việc này có thể dẫn đến biến thiên hiệu suất khi container di chuyển giữa các lõi không chia sẻ cache. Để giảm biến thiên và cải thiện hiệu quả cache, container có thể được pin vào các lõi cụ thể bằng flag --cpuset-cpus.\ndocker run --cpuset-cpus=\u0026ldquo;1,3\u0026rdquo; my-container\nThiết lập này giới hạn container chỉ dùng các lõi đã chỉ định. Trong ví dụ này, lõi 1 và 3 được dùng để minh hoạ. Lựa chọn lõi thực tế nên dựa trên topology CPU để đảm bảo lập lịch hiệu quả bộ nhớ đệm. Pin container vào các lõi chia sẻ bộ nhớ đệm L3 có thể giảm overhead lập lịch và cải thiện tính nhất quán cho các workload có mẫu truy cập bộ nhớ chia sẻ.\nThiết lập governor tần số CPU Một số hệ điều hành điều chỉnh tần số CPU động để tiết kiệm điện. Thông thường điều này được kiểm soát bởi thiết lập gọi là CPU frequency governor. Mặc dù hành vi này hiệu quả cho các workload tổng quát, nó có thể gây độ trễ hoặc biến thiên hiệu suất trong môi trường nhạy với tính toán. Với các workload cần hiệu suất CPU ổn định cao — như xử lý dữ liệu thông lượng lớn, mô phỏng, hoặc ứng dụng thời gian thực — chúng tôi khuyến nghị đặt governor của CPU về performance mode. Điều này đảm bảo CPU chạy ở tần số tối đa khi chịu tải, tránh thời gian tăng tốc từ trạng thái năng lượng thấp.\nBạn có thể áp dụng thiết lập này trên các instance bare metal hoặc Amazon EC2 Dedicated Hosts bằng lệnh:\nsudo cpupower frequency-set -g performance\nTrước khi áp dụng, hãy cân nhắc benchmark hiệu suất workload với các governor khác (như ondemand hoặc schedutil) để đảm bảo rằng chế độ performance mang lại lợi ích đo được mà không đánh đổi điện năng không cần thiết.\nDùng flag compiler kiến trúc cụ thể Khi biên dịch các ứng dụng C hoặc C++ nhạy hiệu suất, các flag kiến trúc cụ thể như -march=znverX có thể mở khóa tối ưu hóa dành riêng cho AMD EPYC, bao gồm cải thiện vectorization và hiệu suất số thực. Dù điều này có lợi cho workload tính toán nặng, nó có thể giảm tính di động giữa các kiến trúc. Để cân bằng giữa hiệu suất và tính linh hoạt, cân nhắc dùng phát hiện tính năng thời chạy (runtime feature detection) và dispatching — cách nhiều thư viện tối ưu dùng để thích ứng hành vi dựa trên CPU nền tảng.\nTrước khi dùng các flag này, xác minh rằng phiên bản compiler của bạn hỗ trợ chúng và đảm bảo kiến trúc instance EC2 đích phù hợp với flag chỉ định. Ví dụ, một binary biên dịch với -march=znver4 có thể lỗi chạy với lỗi “illegal instruction” (SIGILL) nếu chạy trên các instance thế hệ trước như M5a. Bảng dưới đây mô tả các flag phù hợp và phiên bản compiler tối thiểu hỗ trợ cho mỗi thế hệ AMD EPYC:\nThế hệ AMD EPYC Flag -march Phiên bản GCC tối thiểu Phiên bản LLVM/Clang tối thiểu Thế hệ 4 (ví dụ M7a) znver4 GCC 12 Clang 15 Thế hệ 3 (ví dụ M6a) znver3 GCC 11 Clang 13 Thế hệ 2 (ví dụ M5a) znver2 GCC 9 Clang 11 Các flag sau được hỗ trợ cho GCC 11+ hoặc LLVM Clang 13+:\nCho EPYC thế hệ 4 (M7a, R7a, C7a, Hpc7a): -march=znver4\nCho EPYC thế hệ 3 (M6a, R6a, C6a): -march=znver3\nCho EPYC thế hệ 2 (M5a, R5a, C5a): -march=znver2\nKhi nào bật AVX‑512 và VNNI Các instance EC2 dùng AMD EPYC thế hệ 4 hỗ trợ các tập lệnh SIMD tiên tiến như AVX2, AVX‑512 và VNNI. Những tập lệnh này có thể cải thiện thông lượng cho các workload vector nặng như inference ML, xử lý hình ảnh hoặc mô phỏng khoa học. Tuy nhiên, những flag này là đặc trưng theo thế hệ — việc cố chạy các binary được biên dịch với AVX‑512 trên instance không hỗ trợ (ví dụ thế hệ 2 như M5a) có thể gây lỗi thời gian chạy như “illegal instruction” (SIGILL).\nKhi biên dịch mã C hoặc C++:\ngcc -mavx2 -mavx512f -O2 your_program.c -o your_program\nĐể hiểu rõ hơn tối ưu hóa nào được áp dụng, dùng:\n-ftree-vectorizer-verbose=2 -fopt-info-vec-missed\nCách này giúp xác định các vòng lặp hưởng lợi từ vectorization và những vòng lặp không. Chỉ bật các tối ưu hóa này nếu workload của bạn hưởng lợi và bạn đã xác thực tính tương thích với thế hệ instance đang dùng. Tránh áp dụng flag AVX một cách bừa bãi, vì có thể làm giảm tính di động và tăng độ phức tạp binary.\nThư viện tối ưu CPU của AMD (AMD Optimizing CPU Libraries – AOCL) Thư viện AMD Optimizing CPU Libraries (AOCL) cung cấp các thư viện toán học được tinh chỉnh hiệu năng dành riêng cho các bộ xử lý AMD EPYC. Những thư viện này bao gồm các triển khai tối ưu của các hàm hay dùng trong khoa học, kỹ thuật và workload ML. Bạn có thể liên kết ứng dụng của bạn với AOCL để sử dụng các tối ưu phần cứng mà không cần viết lại mã. AOCL gồm các thư viện cho toán vector, scalar, tạo số ngẫu nhiên, FFT, BLAS và LAPACK, trong số những cái khác.\nCấu hình AOCL Gán biến môi trường AOCL_ROOT trỏ tới thư mục cài đặt:\nexport AOCL_ROOT=/path/to/aocl\nBiên dịch ứng dụng với đường dẫn include và library tương ứng:\ngcc -I$AOCL_ROOT/include -L$AOCL_ROOT/lib -lamdlibm -lm your_program.c -o your_program\nTối ưu toán vector và scalar: bạn có thể bật các tuning vector hóa hay scalar cụ thể cho workload:\n# Tối ưu toán vector\ngcc -lamdlibm -fveclib=AMDLIBM -lm your_program.c -o your_program\n# Toán scalar nhanh hơn\ngcc -lamdlibm -fsclrlib=AMDLIBM -lamdlibmfast -lm your_program.c -o your_program\nProfiling runtime AOCL\nAOCL hỗ trợ profiling runtime, giúp các nhà phát triển xác định các phép toán nào chiếm thời gian thực thi lớn. Để bật profiling:\nexport AOCL_PROFILE=1\n./your_program\nSau khi chạy, một file báo cáo tên aocl_profile_report.txt được sinh ra. Nó cung cấp phân tích theo hàm gồm số lần gọi, thời gian thực thi và việc sử dụng luồng. Các nhà phát triển có thể dùng nó để tập trung tối ưu hóa vào các phần có ảnh hưởng cao nhất.\nKết luận Bài viết này khảo sát cách chọn các loại instance Amazon EC2 dựa AMD phù hợp với đặc điểm khối lượng công việc, và cách áp dụng các kỹ thuật điều chỉnh tập trung vào việc sử dụng CPU, định vị luồng, hiệu quả cache và tối ưu thư viện toán học. Những phương pháp này đặc biệt quan trọng với khối lượng công việc bị giới hạn bởi CPU hoặc nhạy độ trễ, nơi hiệu suất ổn định là thiết yếu.\nSẵn sàng bắt đầu? Đăng nhập AWS Management Console và khởi động các instance Amazon EC2 dùng AMD EPYC để bắt tay vào tối ưu hóa workload của bạn ngay hôm nay.\nTAGS: AMD\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-api-gateway/5.4.3-test-api/",
	"title": "Kiểm Thử API Gateway",
	"tags": [],
	"description": "",
	"content": "Kiểm thử GET Dùng curl:\ncurl \u0026quot;\u0026lt;invoke-url\u0026gt;/\u0026quot; Kỳ vọng: HTTP 200, body {\u0026quot;message\u0026quot;:\u0026quot;Hello, tester\u0026quot;}. Nếu sai tên hoặc lỗi 5xx, xem log CloudWatch. PS C:\\Users\\Nhan\\Documents\\hugo_aws\u0026gt; curl \u0026#34;https://tyuyo8aqwd.execute-api.us-east-1.amazonaws.com/dev/hello\u0026#34; StatusCode : 200 StatusDescription : OK Content : {\u0026#34;message\u0026#34;: \u0026#34;hello\u0026#34;} RawContent : HTTP/1.1 200 OK x-amzn-RequestId: 07039249-41a2-4024-8939-5b8d676f46e7 x-amz-apigw-id: VLGW2G0loAMEm5Q= X-Amzn-Trace-Id: Root=1-693450f8-a03e8d0a53d 6c1ca901a1626;Parent=5c411597312990c1;Sample d=0;L... Forms : {} Headers : {[x-amzn-RequestId, 07039249-41a2-4024-8939-5b8d676f46e7], [x-amz-apigw-id, VLGW2G0loAMEm5Q=], [X-Amzn-Trace-Id, Root=1-693450f8-a03e8d0a53 d6c1ca901a1626;Parent=5c411597312990c1;Sampl ed=0;Lineage=1:b8101292:0], [Connection, Keep-Alive]...} Images : {} InputFields : {} Links : {} ParsedHtml : System.__ComObject RawContentLength : 20 Kiểm thử POST Dùng curl/Postman:\ncurl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; -d '{\u0026quot;name\u0026quot;:\u0026quot;tester\u0026quot;}' \u0026quot;\u0026lt;invoke-url\u0026gt;/hello\u0026quot; Kỳ vọng: JSON trả lời đúng tên, mã 200. Nếu Lambda không parse body, kiểm tra event.body và JSON.parse. C:\\Users\\Nhan\u0026gt;curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{}\u0026#39; \u0026#34;https://tyuyo8aqwd.execute-api.us-east-1.amazonaws.com/dev/hello\u0026#34; {\u0026#34;message\u0026#34;: \u0026#34;hello\u0026#34;} Xử lý lỗi (tùy chọn) Thiết lập trả 400 khi thiếu name hoặc body không phải JSON. Ghi log input để dễ debug: console.log(event) hoặc log các field cần thiết. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-lambda-basics/",
	"title": "Lambda cơ bản",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tạo và kiểm thử Lambda Hello World (Node.js/Python), truyền tham số đầu vào, và tinh chỉnh cấu hình (memory, timeout, log).\nCác bước chính Tạo hàm Node.js\nRuntime: Node.js 18.x (ví dụ).\nHandler mẫu trả JSON { message: \u0026quot;hello\u0026quot; }.\nTest event với tham số name, log kết quả ở CloudWatch. Tạo hàm Python\nRuntime: Python 3.12 (ví dụ). Handler đọc event[\u0026quot;name\u0026quot;] và trả lời tùy biến. Tham số hóa \u0026amp; lỗi thường gặp\nĐọc event (query/body sẽ được API Gateway gắn thêm ở bước sau). Bắt lỗi key thiếu, trả mã trạng thái phù hợp. Tối ưu nhẹ\nChỉnh Memory và Timeout để cân bằng chi phí/hiệu năng. Đặt CloudWatch retention. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/",
	"title": "Ngày 03 - Công cụ Quản lý AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-10 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bộ công cụ quản trị AWS AWS Management Console Đăng nhập bằng Root User hoặc IAM User (cần ID tài khoản 12 chữ số). Tìm kiếm và mở dashboard của từng dịch vụ. Support Center cho phép tạo ticket hỗ trợ trực tiếp. AWS Command Line Interface (CLI) Công cụ dòng lệnh mã nguồn mở tương tác với dịch vụ AWS. Cung cấp tính năng tương đương Console. Đặc điểm chính:\nHỗ trợ đa nền tảng (Windows, macOS, Linux). Dễ script và tự động hóa. Truy cập trực tiếp API dịch vụ AWS. Quản lý nhiều tài khoản thông qua profiles. AWS SDK (Software Development Kit) Đơn giản hóa việc tích hợp dịch vụ AWS vào ứng dụng. Tự động xử lý xác thực, retry và tuần tự hóa dữ liệu. Ngôn ngữ hỗ trợ:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go và các ngôn ngữ khác "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/",
	"title": "Ngày 08 - Bảo mật VPC &amp; Flow Logs",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-17 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bảo mật VPC Security Group (SG) Tường lửa ảo stateful kiểm soát lưu lượng vào/ra tài nguyên AWS. Quy tắc dựa trên giao thức, cổng, nguồn hoặc security group khác. Chỉ hỗ trợ rule cho phép (allow). Áp dụng ở cấp độ Elastic Network Interface (ENI). Đặc điểm của Security Group:\nStateful: lưu lượng trả về được cho phép tự động. Chỉ có rule cho phép. Đánh giá toàn bộ rule trước khi quyết định. Áp dụng ở mức instance/ENI. Network Access Control List (NACL) Tường lửa ảo stateless hoạt động ở cấp subnet. Quy tắc điều khiển lưu lượng vào/ra theo giao thức, cổng và nguồn. NACL mặc định cho phép tất cả lưu lượng. Đặc điểm của NACL:\nStateless: phải cho phép rõ ràng lưu lượng chiều về. Hỗ trợ cả rule allow và deny. Các rule được xử lý theo thứ tự số. Áp dụng ở mức subnet. VPC Flow Logs Ghi nhận metadata về lưu lượng IP đi/đến các network interface trong VPC. Log có thể gửi tới Amazon CloudWatch Logs hoặc S3. Flow Logs không ghi phần payload của gói tin. Trường hợp sử dụng:\nKhắc phục sự cố kết nối. Theo dõi mẫu lưu lượng. Phân tích bảo mật. Đáp ứng yêu cầu tuân thủ. Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (tiếp) Khởi chạy EC2 trong các subnet → 04-1 Kiểm tra kết nối giữa các instance → 04-2 Tạo NAT Gateway (Private ↔ Internet) → 04-3 EC2 Instance Connect Endpoint (không cần bastion) → 04-5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/",
	"title": "Ngày 13 - Instance Store &amp; User Data",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-24 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tính năng nâng cao của EC2 Instance Store Instance Store cung cấp lưu trữ block tạm thời gắn trực tiếp vào host EC2. Đặc điểm\nI/O và thông lượng rất cao. Dữ liệu mất khi instance dừng hoặc terminate. Không thể tách rời hoặc tạo snapshot. Tình huống sử dụng\nCache hoặc xử lý dữ liệu tạm. Ứng dụng đã có cơ chế nhân bản/replication riêng. So sánh Instance Store và EBS:\nTiêu chí Instance Store EBS Tính bền vững Tạm thời Bền vững Hiệu năng Rất cao Cao Snapshot Không Có Tháo rời Không Có Chi phí Đã bao gồm Tính riêng User Data Script User Data chạy tự động khi instance khởi tạo (mỗi lần provision AMI). Linux dùng bash script, Windows dùng PowerShell. Ví dụ User Data:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata cung cấp thông tin về instance đang chạy như IP private/public, hostname, security group. Thường dùng trong User Data để cấu hình động. Truy cập Metadata:\n# Lấy instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Lấy public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Lấy credential IAM role curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Tạo Budget từ template → 07-01 Hướng dẫn tạo Cost Budget → 07-02 Tạo Usage Budget → 07-03 Tạo Budget cho Reserved Instance → 07-04 Tạo Budget cho Savings Plans → 07-05 Dọn dẹp Budget → 07-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/",
	"title": "Ngày 18 - AWS Snow Family &amp; Hybrid Storage",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-01 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Snow Family Bộ thiết bị/dịch vụ chuyên dụng để di chuyển khối lượng dữ liệu lớn vào/ra AWS khi băng thông hạn chế hoặc dữ liệu quá khủng.\nAWS Snowcone: Thiết bị nhỏ gọn, chịu va đập (~8 TB). Phù hợp môi trường edge, vùng xa. AWS Snowball: Snowball Edge Storage Optimized: Tối đa ~80 TB lưu trữ khả dụng. Snowball Edge Compute Optimized: Thêm khả năng compute mạnh với ~42 TB lưu trữ. AWS Snowmobile: Trung tâm dữ liệu container hóa, phục vụ chuyển dữ liệu quy mô exabyte (tới 100 PB). So sánh Snow Family:\nThiết bị Lưu trữ Compute Use case Snowcone 8 TB 2 vCPU Edge, IoT Snowball Storage 80 TB 40 vCPU Di chuyển dữ liệu Snowball Compute 42 TB 52 vCPU Edge computing Snowmobile 100 PB N/A Di dời datacenter Khi nào dùng Snow Family:\nBăng thông hạn chế hoặc chi phí cao. Khối lượng dữ liệu rất lớn (TB tới PB). Địa điểm xa xôi, khó kết nối. Nhu cầu xử lý edge computing. Yêu cầu tuân thủ lưu trữ dữ liệu tại chỗ. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/",
	"title": "Ngày 23 - Amazon Cognito &amp; Organizations",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-08 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Cognito Dịch vụ managed cho xác thực/ủy quyền và quản lý người dùng cho web \u0026amp; mobile. Thành phần: User Pools: Thư mục người dùng phục vụ đăng ký/đăng nhập. Identity Pools: Danh tính liên kết (federated) cung cấp credential tạm thời để truy cập dịch vụ AWS. Tính năng của Cognito User Pools:\nĐăng ký và đăng nhập. Hỗ trợ IdP xã hội (Google, Facebook, Amazon). Hỗ trợ IdP SAML. Multi-factor authentication (MFA). Xác thực email và số điện thoại. Tùy biến luồng đăng nhập. Lambda trigger để mở rộng logic. Tính năng của Cognito Identity Pools:\nCredential AWS tạm thời. Phân biệt truy cập authenticated và unauthenticated. Kiểm soát truy cập dựa trên role. Tích hợp với User Pool. Hỗ trợ IdP bên ngoài. AWS Organizations Quản lý tập trung nhiều tài khoản AWS trong một tổ chức. Lợi ích\nQuản lý tài khoản tập trung. Consolidated Billing. Cấu trúc phân cấp bằng Organizational Unit (OU). Thiết lập guardrail bằng Service Control Policy (SCP). Organizational Unit (OU) Gom tài khoản theo phòng ban, dự án hoặc môi trường; có thể lồng OUs để áp policy theo tầng. Ví dụ cấu trúc OU:\nRoot\r├── Production OU\r│ ├── Web App Account\r│ └── Database Account\r├── Development OU\r│ ├── Dev Account\r│ └── Test Account\r└── Security OU\r└── Audit Account Consolidated Billing Một hóa đơn cho mọi tài khoản; hưởng lợi từ volume pricing; không phát sinh phí thêm. Lợi ích:\nGiảm giá theo khối lượng dùng chung giữa các tài khoản. Dễ theo dõi và lập báo cáo. Đơn giản hóa phương thức thanh toán. Chia sẻ Reserved Instance. Hands-On Labs Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Phần 2) Switch Role → 28-5.1 Truy cập EC2 Console – Tokyo → 28-5.2.1 Truy cập EC2 Console – N. Virginia → 28-5.2.2 Tạo EC2 (không đáp ứng tag) → 28-5.2.3 Chỉnh sửa tag tài nguyên EC2 → 28-5.2.4 Kiểm tra chính sách → 28-5.2.5 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Phần 1) Tạo EC2 Instance kèm Tag → 27-2.1.1 Quản lý tag cho tài nguyên AWS → 27-2.1.2 Lọc tài nguyên theo tag → 27-2.1.3 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/",
	"title": "Ngày 28 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-15 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Redshift Dịch vụ data warehouse fully-managed tối ưu cho workload phân tích quy mô lớn (OLAP).\nLưu trữ dạng cột, nén dữ liệu, thực thi song song MPP; mở rộng từ hàng trăm GB tới hàng PB. Tích hợp sâu với S3, Kinesis, DynamoDB, các công cụ BI; hỗ trợ nhiều lớp bảo mật. Concurrency Scaling tự động bổ sung compute khi lưu lượng tăng đột biến. Kiến trúc: cluster gồm leader node + compute nodes; mỗi compute node chia thành nhiều slice. Tùy chọn triển khai:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (truy vấn trực tiếp dữ liệu trên S3) Use case: BI doanh nghiệp, phân tích data lake, dashboard, phân tích xu hướng và dự báo.\nTính năng nổi bật:\nLưu trữ dạng cột: phù hợp truy vấn phân tích. Massively Parallel Processing: phân tán truy vấn trên nhiều node. Result Caching: tăng tốc truy vấn lặp lại. Automatic Compression: giảm dung lượng lưu trữ. Workload Management (WLM): ưu tiên truy vấn quan trọng. Concurrency Scaling: xử lý workload bùng nổ. Redshift vs Data Warehouse truyền thống:\nTiêu chí Redshift DW truyền thống Thiết lập Vài phút Vài tuần/tháng Mở rộng Elastic Cố định Chi phí Pay-as-you-go Đầu tư lớn ban đầu Vận hành Managed Tự quản lý Redshift Spectrum:\nTruy vấn trực tiếp dữ liệu trên S3 mà không cần load. Tách biệt compute và storage. Hỗ trợ nhiều định dạng (Parquet, ORC, JSON\u0026hellip;). Tiết kiệm chi phí cho dữ liệu truy cập không thường xuyên. Labs thực hành Lab 43 – AWS Database Migration Service (DMS) (Phần 2) Cấu hình đích MSSQL → Aurora MySQL → 43-07 Tạo project MSSQL → Aurora MySQL → 43-08 Convert schema MSSQL → Aurora MySQL → 43-09 Convert schema Oracle → MySQL (1) → 43-10 Tạo migration task \u0026amp; endpoints → 43-11 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/",
	"title": "Ngày 33 - Next.js App Router",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-22 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Next.js 16 App Router Tận dụng Server Components để fetch data trực tiếp từ server và tránh bundle dư thừa. Route /app/books/[id]/page.tsx lo việc fetch dữ liệu, trả về UI đã render sẵn. generateMetadata giúp bổ sung meta SEO dựa trên dữ liệu sách. // app/books/[id]/page.tsx import { getBook } from \u0026#34;@/lib/api\u0026#34;; export default async function BookDetail({ params }) { const book = await getBook(params.id); if (!book) return notFound(); return \u0026lt;BookPage book={book} /\u0026gt;; } Xử lý lỗi \u0026amp; not-found Chỉ cần not-found.tsx cho trường hợp không tìm được sách → coi là flow dự kiến. Không dùng error.tsx để tránh xử lý trùng; các lỗi còn lại sẽ được log ở backend. Giữ UX đồng nhất: hiển thị CTA quay về danh sách và hotline hỗ trợ. Environment \u0026amp; Config Sử dụng biến môi trường rõ ràng: NEXT_PUBLIC_API_URL cho frontend, API_URL cho route handler. Luôn cập nhật .env.example khi thêm biến mới. Gom logic build URL vào helper lib/api.ts để tránh lặp code. Insight Server Components giảm đáng kể latency khi render trang chi tiết. App Router giúp cấu trúc thư mục rõ ràng, dễ mở rộng thêm slice mới. Khi dùng mock API, chỉ cần đổi base URL để fetch từ Prism. Labs thực hành Tạo not-found.tsx tùy biến với CTA điều hướng. Viết helper getBook(id) tái sử dụng cho Server Components và tests. Chạy npm run lint \u0026amp;\u0026amp; npm run build để chắc cấu hình Next.js sạch. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.3-day38-2025-10-29/",
	"title": "Ngày 38 - Mô Hình Seq2seq &amp; LSTM Chi Tiết",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-29 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nMô Hình Seq2seq Mô hình Sequence-to-Sequence (Seq2seq) giới thiệu kiến trúc encoder-decoder hiệu quả cho các tác vụ như dịch máy và tóm tắt văn bản.\nĐặc Điểm Chính: Ánh xạ chuỗi có độ dài thay đổi thành bộ nhớ có độ dài cố định Đầu vào và đầu ra có thể có độ dài khác nhau Sử dụng LSTM và GRU để tránh vanishing và exploding gradients Encoder nhận token từ làm đầu vào → các vector trạng thái ẩn → decoder tạo ra chuỗi đầu ra Kiến Trúc LSTM: Tìm Hiểu Sâu LSTM là gì? LSTM (Long Short-Term Memory) giống như phiên bản mini của não người khi xử lý trí nhớ.\nCấu Trúc LSTM = 3 Cổng + 1 Trạng Thái Tế Bào 1. Cổng Quên – Quyết Định Quên Gì Quyết định thông tin nào cần loại bỏ khỏi trạng thái cũ.\nCông thức:\nf_t = σ(W_f · [h_{t-1}, x_t] + b_f) Ví dụ não người:\nTin nhắn vô ích từ người đã ghosting bạn → quên Công thức bạn dùng hàng ngày → giữ 2. Cổng Đầu Vào – Quyết Định Nhớ Gì Quyết định thông tin mới nào cần thêm vào bộ nhớ.\nCông thức:\ni_t = σ(W_i · [h_{t-1}, x_t] + b_i)\rĈ_t = tanh(W_C · [h_{t-1}, x_t] + b_C) Ví dụ não người:\nThông tin có giá trị → lưu vào trí nhớ dài hạn Nhiễu không liên quan → loại bỏ ngay lập tức 3. Cập Nhật Trạng Thái Tế Bào – Trí Nhớ Dài Hạn Cập nhật trí nhớ dài hạn bằng cách kết hợp cổng quên và cổng đầu vào.\nCông thức:\nC_t = f_t ⊙ C_{t-1} + i_t ⊙ Ĉ_t Trong đó:\nf_t ⊙ C_{t-1} = những gì cần giữ từ trí nhớ cũ i_t ⊙ Ĉ_t = những gì cần thêm từ đầu vào mới 4. Cổng Đầu Ra – Quyết Định Xuất Ra Gì Quyết định trí nhớ nào sử dụng cho đầu ra hiện tại.\nCông thức:\no_t = σ(W_o · [h_{t-1}, x_t] + b_o)\rh_t = o_t ⊙ tanh(C_t) Ví dụ não người:\nKhi thi NLP → nhớ lại công thức LSTM Khi nói chuyện với ai đó → nhớ ngữ cảnh cuộc trò chuyện Khi làm DevOps → nhớ thông số AWS LSTM vs Não Người Não Người LSTM Trí nhớ dài hạn Cell State Lọc thông tin không cần thiết Forget Gate Chấp nhận thông tin mới có giá trị Input Gate Truy xuất trí nhớ phù hợp để phản hồi Output Gate Học từ kinh nghiệm tuần tự RNN backbone Không quên nhanh Long-term dependencies Cổng là gì? Cổng = bộ lọc nhận thức\nMỗi cổng = một cơ chế quyết định \u0026ldquo;giữ hay bỏ\u0026rdquo;\nVí dụ: Khi Bạn Học NLP Cổng Quên: \u0026ldquo;Tôi còn cần nhớ phương pháp lỗi thời này không?\u0026rdquo; → Bỏ nếu không Cổng Đầu Vào: \u0026ldquo;Khái niệm mới này có giá trị không?\u0026rdquo; → Lưu nếu có Cổng Đầu Ra: \u0026ldquo;Tôi cần kiến thức gì ngay bây giờ?\u0026rdquo; → Truy xuất phần liên quan Giới Hạn Hidden State Hidden state không có giới hạn token, nhưng có giới hạn khả năng nhớ hiệu quả.\nGóc Độ Toán Học: Hidden state = vector có kích thước cố định (ví dụ: 128, 256, 512 chiều) Có thể xử lý 10 token hoặc 10.000 token → không bị crash Vấn đề: không thể nhớ mọi thứ Tại Sao? Ngay cả với cell state, gradient suy yếu qua nhiều bước thời gian Các phụ thuộc dài hạn bị mất Các token xa điểm bắt đầu có ảnh hưởng yếu đến đầu ra cuối cùng Giải pháp: Đây là lý do chúng ta cần cơ chế Attention!\nThrottling trong NLP Hai Nghĩa của Throttling: 1. Throttling Cấp Hệ Thống (API) Giới hạn tốc độ request hoặc xử lý token để:\nBảo vệ tài nguyên GPU Phân phối tài nguyên công bằng Tránh quá tải server Kiểm soát chi phí Ví dụ:\nOpenAI GPT: 10 requests/giây, 90k tokens/phút Anthropic Claude: 20 requests/giây HuggingFace: timeout nếu generation mất quá lâu 2. Throttling Cấp Mô Hình (Kiến Trúc) LSTM, Transformer và Attention đều có cơ chế giới hạn xử lý thông tin tại bất kỳ thời điểm nào:\n(A) LSTM Throttling → Cổng Quên Khi chuỗi quá dài:\nCổng quên tự động \u0026ldquo;throttle\u0026rdquo; thông tin cũ Chỉ cho phép một phần ý nghĩa đi qua Giống throttling mạng: \u0026ldquo;quá tải → giảm băng thông → bỏ packet\u0026rdquo; (B) Transformer Throttling → Giới Hạn Context Window\nBERT: 512 tokens GPT-3: 2048-4096 tokens GPT-4: 128k-1M tokens Claude 3.5 Sonnet: 200k-1M tokens Khi đầu vào vượt giới hạn:\nMô hình cắt dữ liệu Hoặc từ chối xử lý Hoặc hạ chất lượng attention (C) Attention Throttling → Sparse Attention Trong các mô hình ngữ cảnh dài (Longformer, BigBird, Mistral):\nKhông thể tính toán full n² attention Chỉ chú ý đến các vùng quan trọng (local attention) Hoặc global tokens Hoặc sliding window (D) Token Generation Throttling Một số decoder sẽ:\nLàm chậm tốc độ sinh token Giới hạn sampling Áp dụng kiểm soát nhiệt độ Cắt beam search Khi đầu vào nhiễu hoặc không chắc chắn, điều này hoạt động như phanh: \u0026ldquo;Không chắc → làm chậm generation → tăng chất lượng\u0026rdquo;\nTóm Tắt LSTM không chỉ là một mô hình — nó là sự bắt chước tính toán cách trí nhớ con người hoạt động. Hiểu các cổng giúp bạn hiểu tại sao một số thông tin tồn tại trong khi thông tin khác biến mất.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.3-day43-2025-11-05/",
	"title": "Ngày 43 - Sâu Hơn Về Scaled Dot-Product Attention",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-05 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nScaled Dot-Product Attention: Cơ Chế Lõi Đây là tim và linh hồn của transformers. Hiểu biết sâu về điều này là rất quan trọng.\nCông Thức Attention (Q × K^T)\rAttention(Q, K, V) = softmax(─────────) × V\r√(d_k) Trong đó:\nQ = ma trận Query (chúng ta đang tìm kiếm cái gì?) K = ma trận Key (chúng ta có thể attend tới cái gì?) V = ma trận Value (chúng ta nhận được thông tin gì?) d_k = chiều của keys (thường là 64) Tính Toán Từng Bước Hãy tính toán attention cho một ví dụ đơn giản:\nCâu Đầu Vào: \u0026ldquo;I am happy\u0026rdquo;\nGiai Đoạn Thiết Lập Bước 1: Tạo Word Embeddings\nI: [0.1, 0.2, 0.3]\ram: [0.4, 0.5, 0.6]\rhappy: [0.7, 0.8, 0.9] Bước 2: Chuyển Đổi Thành Q, K, V Trong thực tế, chúng ta học các phép chiếu tuyến tính:\nQ = Embedding × W_q\rK = Embedding × W_k\rV = Embedding × W_v Để đơn giản, hãy nói rằng:\nQ = [0.1, 0.2, 0.3] K = [0.1, 0.2, 0.3] V = [0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6] [0.4, 0.5, 0.6] [0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] [0.7, 0.8, 0.9] [0.7, 0.8, 0.9] (Trong thực tế, Q, K, V sẽ là các phép chiếu khác nhau, nhưng điều này cho thấy khái niệm)\nGiai Đoạn Tính Toán Bước 3: Tính Q × K^T (dot products)\nQ × K^T = [0.1, 0.2, 0.3] [0.1, 0.4, 0.7]\r[0.4, 0.5, 0.6] × [0.2, 0.5, 0.8]\r[0.7, 0.8, 0.9] [0.3, 0.6, 0.9]\rQ₁·K₁ = 0.1×0.1 + 0.2×0.2 + 0.3×0.3 = 0.01 + 0.04 + 0.09 = 0.14\rQ₁·K₂ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32\rQ₁·K₃ = 0.1×0.7 + 0.2×0.8 + 0.3×0.9 = 0.07 + 0.16 + 0.27 = 0.50\rMa trận kết quả:\r[0.14, 0.32, 0.50]\r[0.32, 0.77, 1.22]\r[0.50, 1.22, 1.94] Diễn Giải:\nQ₁ (query cho \u0026ldquo;I\u0026rdquo;) có điểm tương tự: [0.14, 0.32, 0.50] 0.14 với \u0026ldquo;I\u0026rdquo; chính nó 0.32 với \u0026ldquo;am\u0026rdquo; 0.50 với \u0026ldquo;happy\u0026rdquo; Bước 4: Tỷ Lệ Theo √d_k\nd_k = 3 (chiều embedding), vì vậy √d_k = √3 ≈ 1.73\nMa trận được tỷ lệ = Q×K^T / √3:\r[0.14/1.73, 0.32/1.73, 0.50/1.73] [0.08, 0.18, 0.29]\r[0.32/1.73, 0.77/1.73, 1.22/1.73] = [0.18, 0.44, 0.70]\r[0.50/1.73, 1.22/1.73, 1.94/1.73] [0.29, 0.70, 1.12] Tại Sao Tỷ Lệ?\nKhi d_k lớn (ví dụ: 64), dot products trở nên rất lớn Các số lớn khiến softmax có gradient rất nhỏ (bão hòa) Tỷ lệ theo √d_k giữ các số trong phạm vi hợp lý để huấn luyện Bước 5: Áp Dụng Softmax\nSoftmax chuyển đổi điểm thành xác suất (tổng bằng 1):\nsoftmax(x) = exp(x) / sum(exp(x))\rCho hàng đầu tiên [0.08, 0.18, 0.29]:\rexp(0.08) ≈ 1.083\rexp(0.18) ≈ 1.197\rexp(0.29) ≈ 1.336\rTổng ≈ 3.616\rXác suất:\r[1.083/3.616, 1.197/3.616, 1.336/3.616] ≈ [0.30, 0.33, 0.37] Cả ba hàng:\nTrọng số Softmax (ma trận attention):\r[0.30, 0.33, 0.37]\r[0.26, 0.37, 0.37]\r[0.25, 0.36, 0.39] Diễn Giải:\nTừ \u0026ldquo;I\u0026rdquo; chi 30% sự chú ý cho chính nó, 33% cho \u0026ldquo;am\u0026rdquo;, 37% cho \u0026ldquo;happy\u0026rdquo; Từ \u0026ldquo;am\u0026rdquo; chi 26% sự chú ý cho \u0026ldquo;I\u0026rdquo;, 37% cho chính nó, 37% cho \u0026ldquo;happy\u0026rdquo; Từ \u0026ldquo;happy\u0026rdquo; chi 25% cho \u0026ldquo;I\u0026rdquo;, 36% cho \u0026ldquo;am\u0026rdquo;, 39% cho chính nó Bước 6: Nhân Với Ma Trận Value (V)\nContext = Softmax_weights × V\rContext(cho \u0026#34;I\u0026#34;): 0.30×[0.1,0.2,0.3] + 0.33×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.03,0.06,0.09] + [0.13,0.17,0.20] + [0.26,0.30,0.33]\r= [0.42, 0.53, 0.62]\rContext(cho \u0026#34;am\u0026#34;): 0.26×[0.1,0.2,0.3] + 0.37×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.026,0.052,0.078] + [0.148,0.185,0.222] + [0.259,0.296,0.333]\r= [0.433, 0.533, 0.633]\rContext(cho \u0026#34;happy\u0026#34;): 0.25×[0.1,0.2,0.3] + 0.36×[0.4,0.5,0.6] + 0.39×[0.7,0.8,0.9]\r= [0.025,0.05,0.075] + [0.144,0.18,0.216] + [0.273,0.312,0.351]\r= [0.442, 0.542, 0.642] Ma Trận Context Đầu Ra:\n[0.42, 0.53, 0.62]\r[0.433, 0.533, 0.633]\r[0.442, 0.542, 0.642] Mỗi từ bây giờ có một vector ngữ cảnh kết hợp thông tin từ tất cả các từ được tính trọng số bằng điểm attention!\nTại Sao Scaled Dot-Product Attention? Khía Cạnh Lý Do Dot Product Thước đo tương tự hiệu quả (chỉ là phép nhân ma trận) Scaling Ngăn chặn bão hòa softmax (giữ gradient khỏe mạnh) Softmax Chuyển đổi tương tự thành trọng số chuẩn hóa [0,1] Nhân Với V Lấy thông tin thực tế (kết hợp có trọng số) Multi-Head Attention: Nhiều Quan Điểm Thay vì một đầu attention, chúng ta sử dụng h = 8 (hoặc nhiều hơn) đầu attention:\nMultiHeadAttention(Q, K, V) = Concat(Head₁, ..., Head₈) × W_o\rTrong đó:\rHead_i = Attention(Q × W_q^i, K × W_k^i, V × W_v^i) Các đầu khác nhau học các mối quan hệ khác nhau:\nĐầu 1: Mối quan hệ chủ ngữ-động từ Đầu 2: Mối quan hệ tính từ-danh từ Đầu 3: Mối quan hệ đại từ-tham chiếu Đầu 4-8: Các mô hình ngữ nghĩa khác Ví Dụ:\nCâu: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;\rĐầu 1 (chủ ngữ-động từ):\r- \u0026#34;fox\u0026#34; → \u0026#34;jumps\u0026#34;: 0.9\r- \u0026#34;dog\u0026#34; → has_property: 0.7\rĐầu 2 (tính từ-danh từ):\r- \u0026#34;quick\u0026#34; → \u0026#34;fox\u0026#34;: 0.85\r- \u0026#34;brown\u0026#34; → \u0026#34;fox\u0026#34;: 0.8\r- \u0026#34;lazy\u0026#34; → \u0026#34;dog\u0026#34;: 0.9\rĐầu 3 (không gian):\r- \u0026#34;over\u0026#34; → kết nối \u0026#34;fox\u0026#34; và \u0026#34;dog\u0026#34;: 0.8 Tất cả các quan điểm khác nhau này kết hợp lại tạo ra sự hiểu biết ngữ cảnh phong phú.\nHiệu Quả Tính Toán Tại sao scaled dot-product attention lại hiệu quả đến vậy?\nPhép Toán Ma Trận: Chỉ là phép nhân và softmax (GPU-optimized) Có Thể Song Song Hóa: Có thể xử lý toàn bộ chuỗi cùng lúc Tiết Kiệm Bộ Nhớ: Bộ nhớ O(n²) cho chuỗi độ dài n (chấp nhận được) Huấn Luyện Nhanh: GPU hiện đại có thể thực hiện hàng tỷ dot products/giây So Sánh:\nRNN: O(n) bước tuần tự → chậm Attention: O(1) độ sâu, O(n²) bộ nhớ → nhanh! Những Hiểu Biết Chính ✅ Attention được học: W_q, W_k, W_v là các tham số có thể huấn luyện ✅ Không phụ thuộc vị trí: Không có phụ thuộc tuần tự - có thể attend trên bất kỳ khoảng cách nào ✅ Có thể Diễn Giải: Có thể hình dung các từ nào attend tới từ nào ✅ Hiệu Quả: Chỉ sử dụng phép toán ma trận (GPU-friendly)\nBước Tiếp Theo Bây giờ chúng ta hiểu scaled dot-product attention, chúng ta sẽ khám phá:\nSelf-attention (query=key=value) Masked attention (decoder chỉ nhìn thấy quá khứ) Encoder-decoder attention (kết nối xuyên ngôn ngữ) Chi tiết Multi-head attention (học nhiều mô hình) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.3-day48-2025-11-12/",
	"title": "Ngày 48 - BERT và ngữ cảnh hai chiều",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-12 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nBERT học như thế nào BERT tiền huấn luyện với ngữ cảnh hai chiều, nên mỗi token nhìn được cả trái lẫn phải.\nMasked Language Modeling (MLM) Mask ngẫu nhiên ~15% token; dự đoán token gốc. Mục tiêu buộc embedding hiểu ngữ cảnh xung quanh. Input: learning from deep learning is like watching the sunset with my best [MASK] Target: friend Next Sentence Prediction (NSP) Nhiệm vụ: dự đoán câu B có theo sau câu A hay không. Hỗ trợ coherence mức câu, hữu ích cho QA và phân loại. Dùng cho downstream Bắt đầu từ trọng số đã pre-train. Cách A: đóng băng encoder, train head nhẹ (feature-based). Cách B: fine-tune encoder + head với learning rate nhỏ. Mẹo thực hành max_seq_length phải phù hợp dữ liệu; tài liệu dài cần chunk. Tránh quên thảm họa: unfreeze dần, lr nhỏ. Batch nhỏ? Dùng gradient accumulation để ổn định. Việc thực hành hôm nay Lập kế hoạch fine-tune BERT cho QA (dataset, max length, lr, epochs). Quyết định có đóng băng tầng thấp hay không dựa vào kích cỡ dữ liệu. Thêm checkpoint đánh giá (dev EM/F1) để dừng sớm khi overfit. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/1.11.3-day53-2025-11-19/",
	"title": "Ngày 53 - Hàm chạy trên LMI",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-19 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nTạo và publish hàm Tạo hàm như thường, gắn capacity provider và publish version để khởi tạo instance.\nTính năng hỗ trợ Đóng gói: ZIP hoặc OCI Runtime: Java, Python, Node, .NET Layers, extensions, function URL, response streaming, durable functions Timeout 15 phút (dài hơn nếu dùng durable) Thiết lập tài nguyên Memory/CPU ảnh hưởng lựa chọn instance Có multi-concurrency mỗi instance; cân đối throughput/chi phí Nhiều hàm có thể dùng chung một provider (pool dùng chung) Quy trình Tạo capacity provider (VPC, role, rule chọn máy) Tạo function kèm ARN provider Publish version để Lambda tạo instance và deploy execution environment "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/1.12.3-day58-2025-11-26/",
	"title": "Ngày 58 - Vector &amp; dữ liệu riêng tư",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-26 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nLưu trữ vector \u0026amp; dữ liệu tổng hợp Amazon S3 Vectors GA: tới 2B vector/index, ~100ms truy vấn, chi phí thấp hơn DB chuyên dụng Clean Rooms sinh dữ liệu tổng hợp an toàn cho ML cộng tác Cân nhắc di chuyển Kích thước index vs. store hiện tại; sharding và đặt vùng Mục tiêu độ trễ truy vấn, ước tính chi phí so với vector DB đang dùng Mẫu truy cập và bảo mật khi chia sẻ dữ liệu Việc cần làm Thiết kế POC chuyển một bộ vector sang S3 Vectors; đo hiệu năng/chi phí Xác định dataset cần synthetic để dùng chung Đặt chính sách lưu trữ/mã hóa và truy cập cho dữ liệu vector "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Phần này liệt kê và giới thiệu các blog bạn đã dịch:\nBlog 1 - Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code Blog này hướng dẫn cách kết nối Visual Studio Code cục bộ với Amazon SageMaker Unified Studio để tối ưu hóa quy trình phát triển dữ liệu và AI. Bạn sẽ học cách thiết lập môi trường phát triển tích hợp (IDE) cá nhân hóa, truy cập các dịch vụ AWS Analytics và AI/ML trong một môi trường thống nhất, đồng thời duy trì quy trình phát triển hiện có của mình. Bài viết cung cấp hướng dẫn chi tiết về cấu hình kết nối từ xa, các điều kiện tiên quyết cần thiết, và cách triển khai giải pháp để xây dựng luồng công việc dữ liệu và AI đầu-cuối.\nBlog 2 - Thông báo Amazon EC2 M4 và M4 Pro Mac instances Blog này giới thiệu về Amazon EC2 M4 và M4 Pro Mac instances mới được ra mắt, được xây dựng trên nền tảng Apple M4 Mac mini và hệ thống AWS Nitro. Bạn sẽ tìm hiểu về cấu hình phần cứng của các instance này (chip Apple silicon M4/M4 Pro, CPU đa lõi, GPU, Neural Engine), hiệu năng cải thiện so với thế hệ trước (tốt hơn 15-20% cho build ứng dụng), và các tính năng mới như 2TB lưu trữ nội bộ. Bài viết cũng hướng dẫn cách khởi chạy instance thông qua AWS Management Console hoặc CLI, và cách tích hợp với các dịch vụ AWS khác để xây dựng pipeline CI/CD tự động.\nBlog 3 - Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD Blog này cung cấp hướng dẫn chi tiết về cách tối ưu hóa hiệu suất của Amazon EC2 instances sử dụng bộ xử lý AMD EPYC. Bạn sẽ học cách chọn loại instance phù hợp dựa trên đặc điểm khối lượng công việc (compute-intensive, big data \u0026amp; analytics, database, web servers, AI/ML), hiểu về các thế hệ AMD EPYC (thế hệ 3 và 4) và tính năng của chúng. Bài viết cũng trình bày các kỹ thuật điều chỉnh thực tiễn để cải thiện hiệu quả, bao gồm tối ưu hóa cấu hình CPU, bộ nhớ, và các tham số hệ thống để đạt được tỷ lệ giá-hiệu suất tốt nhất.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/",
	"title": "Tuần 3 - Dịch vụ Compute trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-22 đến 2025-09-26\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 3 Tuần này tập trung vào các dịch vụ Compute của AWS, đặc biệt là Amazon EC2 và những dịch vụ bổ trợ.\nNội dung chính Amazon EC2 và các loại instance. AMI và chiến lược sao lưu. EBS và Instance Store. EC2 Auto Scaling. Lựa chọn mô hình giá cho EC2. Amazon Lightsail, EFS, FSx. Labs thực hành Lab 01: AWS Account \u0026amp; IAM Setup. Lab 07: AWS Budgets \u0026amp; Cost Management. Lab 09: AWS Support Plans. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-api-gateway/",
	"title": "API Gateway",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Xuất bản Lambda qua API Gateway (REST), tạo resource/method, và kiểm thử GET/POST bằng curl hoặc Postman.\nCác bước Tạo REST API, resource /hello, bật CORS nếu cần. Thêm method GET/POST, map tới Lambda (proxy integration). Deploy stage dev, ghi lại Invoke URL. Kiểm thử GET/POST, xử lý lỗi (400) khi thiếu dữ liệu. Tùy chọn bật CORS hoặc custom domain. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/",
	"title": "Các Sự Kiện Tham Gia",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập, tôi đã có cơ hội tham gia vào nhiều sự kiện có tác động lớn, giúp làm phong phú thêm hành trình chuyên nghiệp của tôi với những kiến thức quý báu và những trải nghiệm đáng nhớ.\nSự Kiện 1 Tên Sự Kiện: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nNgày \u0026amp; Giờ: 09:00 – 17:00 VNT, Thứ Năm, 18 tháng 9 năm 2025\nĐịa Điểm: Tầng 36, 2 Đường Hai Triều, Phường Sài Gòn, Thành phố Hồ Chí Minh\nVai Trò: Người tham dự\nMô Tả: Vietnam Cloud Day 2025 là một sự kiện AWS toàn diện với các bài phát biểu chính từ các nhà lãnh đạo chính phủ, các giám đốc điều hành AWS và các lãnh đạo ngành. Sự kiện bao gồm hai track chính: track phát sóng trực tiếp với các bài phát biểu chính và thảo luận bảng về cuộc cách mạng GenAI và lãnh đạo cấp cao, cùng các track riêng biệt bao gồm các chủ đề như nền tảng dữ liệu thống nhất cho AI/phân tích, lộ trình áp dụng GenAI, vòng đời phát triển do AI điều khiển, bảo mật các ứng dụng GenAI và các tác nhân AI để tăng năng suất. Sự kiện này giới thiệu các dịch vụ mới nhất của AWS và các sáng kiến chiến lược cho AI và hiện đại hóa đám mây.\nKết Quả: Hiểu rõ hơn về các chiến lược áp dụng AI ở quy mô doanh nghiệp, tìm hiểu về các dịch vụ AWS cho nền tảng dữ liệu và triển khai GenAI, và nắm vững các thực tiễn tốt nhất để bảo mật các ứng dụng AI và hiện đại hóa các hệ thống cũ.\nSự Kiện 2 Tên Sự Kiện: AWS GenAI Builder Club - AI-Driven Development Life Cycle: Reimagining Software Engineering\nNgày \u0026amp; Giờ: 14:00 (2:00 PM), Thứ Sáu, 3 tháng 10 năm 2025\nĐịa Điểm: AWS Event Hall, L26 Tòa Nhà Bitexco, Thành phố Hồ Chí Minh\nVai Trò: Người tham dự\nMô Tả: Phiên làm việc AWS GenAI Builder Club này tập trung vào Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC), khám phá cách AI tạo sinh biến đổi phát triển phần mềm từ kiến trúc đến triển khai và bảo trì. Phiên làm việc này có các bản trình diễn về Amazon Q Developer và Kiro, cho thấy cách AI có thể tự động hóa các tác vụ nặng nề không phân biệt và cho phép các nhà phát triển tập trung vào công việc sáng tạo có giá trị cao hơn. Chương trình bao gồm tổng quan về các khái niệm AI-DLC, khả năng của Amazon Q Developer và các bản trình diễn Kiro thực hành.\nKết Quả: Học được các ứng dụng thực tế của AI trong vòng đời phát triển phần mềm, có được kinh nghiệm thực hành với các công cụ Amazon Q Developer và Kiro, và hiểu cách tích hợp AI như một cộng tác viên trung tâm trong các quy trình phát triển để tăng năng suất và chất lượng mã.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-api-gateway/5.4.4-cors-custom-domain/",
	"title": "CORS &amp; custom domain",
	"tags": [],
	"description": "",
	"content": "Bật CORS Chọn method → Enable CORS → thêm header Content-Type và phương thức cần thiết. Deploy lại stage sau khi bật. Custom domain (tùy chọn) API Gateway → Custom domain names → tạo domain và map stage dev → /. Cập nhật DNS CNAME trỏ về domain API Gateway. Kiểm thử lại Gọi GET/POST qua domain mới hoặc với header CORS phù hợp. Nếu browser báo CORS, kiểm tra lại cấu hình CORS và Deploy. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/",
	"title": "Ngày 04 - Tối ưu Chi phí trên AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-11 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tối ưu chi phí trên AWS Chiến lược tối ưu chi phí Chọn đúng loại tài nguyên và Region phù hợp. Sử dụng các mô hình giá như Reserved Instances, Savings Plans, Spot Instances. Tắt hoặc lên lịch các tài nguyên không dùng. Tận dụng kiến trúc serverless để giảm chi phí vận hành. Liên tục rà soát hiệu quả chi phí bằng AWS Budgets và Cost Explorer. Gắn thẻ chi phí (Cost Allocation Tags) để theo dõi theo phòng ban. AWS Pricing Calculator calculator.aws\nTạo và chia sẻ ước tính chi phí cho các dịch vụ phổ biến. Giá thay đổi tùy Region. Tính năng chính:\nƯớc tính chi phí trước khi triển khai. So sánh giá giữa các Region. Xuất và chia sẻ bảng dự toán. Có sẵn template cho từng workload. Gói hỗ trợ AWS Support Plans Bốn cấp độ: Basic, Developer, Business, Enterprise. Có thể nâng cấp tạm thời khi gặp sự cố nghiêm trọng. So sánh các gói hỗ trợ Tính năng Basic Developer Business Enterprise Chi phí Miễn phí 29 USD/tháng 100 USD/tháng 15.000 USD/tháng Thời gian phản hồi Không áp dụng 12-24 giờ 1 giờ (khẩn) 15 phút (nghiêm trọng) Hỗ trợ kỹ thuật Diễn đàn Giờ hành chính 24/7 24/7 + TAM Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Tạo Budget từ template → 07-01 Hướng dẫn tạo Cost Budget → 07-02 Tạo Usage Budget → 07-03 Tạo Budget cho Reserved Instance (RI) → 07-04 Tạo Budget cho Savings Plans → 07-05 Dọn dẹp các Budget → 07-06 Lab 09 – AWS Support Plans Các gói hỗ trợ AWS → 09-01 Phân loại yêu cầu hỗ trợ → 09-02 Thay đổi gói hỗ trợ → 09-03 Quản lý ticket hỗ trợ → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/",
	"title": "Ngày 09 - Kết nối VPC &amp; Cân bằng tải",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-18 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học VPC Peering \u0026amp; Transit Gateway VPC Peering Cho phép kết nối riêng tư trực tiếp giữa hai VPC mà không qua Internet. Không hỗ trợ định tuyến chuyển tiếp (transitive) và không chấp nhận CIDR trùng nhau. Hạn chế của VPC Peering:\nKhông có peering chuyển tiếp. Không được phép trùng CIDR. Mỗi VPC tối đa 125 kết nối peering. Hỗ trợ peering liên vùng (cross-region). AWS Transit Gateway (TGW) Hoạt động như một hub kết nối nhiều VPC và mạng on-premises, đơn giản hóa kiến trúc mesh phức tạp. TGW Attachment gắn các subnet thuộc một AZ cụ thể vào TGW. Các subnet trong cùng AZ có thể truy cập TGW sau khi được attach. Lợi ích của Transit Gateway:\nHub kết nối tập trung. Đơn giản hóa kiến trúc mạng. Mở rộng tới hàng nghìn VPC. Hỗ trợ peering giữa các region. VPN \u0026amp; Direct Connect Site-to-Site VPN Thiết lập kết nối IPSec bảo mật giữa data center on-premises và AWS VPC. Bao gồm: Virtual Private Gateway (VGW): endpoint đa AZ do AWS quản lý. Customer Gateway (CGW): thiết bị hoặc appliance do khách hàng quản lý. AWS Direct Connect Cung cấp đường truyền riêng giữa data center on-prem và AWS. Độ trễ điển hình: 20–30 ms. Tại Việt Nam hiện có thông qua Hosted Connection (đối tác). Có thể điều chỉnh băng thông linh hoạt. Hands-On Labs Lab 10 – Hybrid DNS (Route 53 Resolver) Tạo Key Pair → 10-02.1 Khởi tạo CloudFormation Template → 10-02.2 Cấu hình Security Group → 10-02.3 Thiết lập hệ thống DNS → 10-05 Tạo Route 53 Outbound Endpoint → 10-05.1 Tạo Resolver Rules → 10-05.2 Tạo Inbound Endpoints → 10-05.3 Lab 19 – VPC Peering Khởi tạo CloudFormation Templates → 19-02.1 Tạo Security Group → 19-02.2 Tạo EC2 instance (test peering) → 19-02.3 Tạo kết nối Peering → 19-04 Cấu hình Route Table (Cross-VPC) → 19-05 Bật Cross-Peer DNS → 19-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/",
	"title": "Ngày 14 - EC2 Auto Scaling",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-25 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon EC2 Auto Scaling EC2 Auto Scaling tự động điều chỉnh số lượng instance EC2 dựa trên nhu cầu. Lợi ích\nCo giãn năng lực linh hoạt. Tăng tính sẵn sàng cho ứng dụng. Tối ưu chi phí. Thành phần chính\nAuto Scaling Group (ASG): nhóm logic chứa các EC2 instance. Launch Template / Configuration: định nghĩa thông số instance. Scaling Policy: quy tắc thêm/bớt instance. Scaling Policy Simple / Step Scaling: thêm/bớt instance khi vượt ngưỡng. Target Tracking: duy trì một metric (ví dụ CPU = 50%). Scheduled Scaling: scale theo lịch định sẵn. Predictive Scaling: dùng ML dự đoán và scale chủ động. Ví dụ Target Tracking:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Tích hợp với Load Balancer ASG thường đi kèm Elastic Load Balancer (ELB). Instance mới sẽ tự đăng ký, instance bị hủy sẽ tự hủy đăng ký. Best practices cho Auto Scaling:\nDùng nhiều AZ để tăng độ sẵn sàng. Thiết lập cooldown hợp lý. Giám sát metric trên CloudWatch. Sử dụng lifecycle hook cho tác vụ tùy chỉnh. Kiểm thử policy trước khi đưa vào production. Các mô hình giá của EC2 On-Demand: Trả theo giờ/giây. Linh hoạt nhất nhưng chi phí cao. Reserved Instances: Cam kết 1 hoặc 3 năm để được giảm giá; gắn với loại/family cụ thể. Savings Plans: Cam kết 1 hoặc 3 năm; linh hoạt hơn giữa các family. Spot Instances: Dùng công suất dư thừa, giảm tới 90%; có thể bị thu hồi sau 2 phút báo trước. Nên kết hợp nhiều mô hình giá trong Auto Scaling Group để tối ưu chi phí.\nSo sánh giá:\nMô hình Giảm giá Linh hoạt Cam kết On-Demand 0% Cao Không Reserved 40-60% Thấp 1-3 năm Savings Plans 40-60% Trung bình 1-3 năm Spot 50-90% Thấp Không Hands-On Labs Lab 09 – AWS Support Plans Các gói hỗ trợ AWS → 09-01 Phân loại yêu cầu hỗ trợ → 09-02 Thay đổi gói hỗ trợ → 09-03 Quản lý ticket hỗ trợ → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/",
	"title": "Ngày 19 - Disaster Recovery trên AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-02 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Disaster Recovery (DR) trên AWS Disaster Recovery là quá trình khôi phục dịch vụ CNTT sau sự cố lớn (mất điện, thiên tai, phần cứng hỏng, tấn công mạng).\nRTO (Recovery Time Objective): Thời gian cần để khôi phục dịch vụ. RPO (Recovery Point Objective): Mức dữ liệu tối đa có thể mất (theo thời gian). Chiến lược DR (tăng dần theo độ phức tạp \u0026amp; chi phí) Backup \u0026amp; Restore\nChỉ lưu backup (snapshot EBS/RDS, S3/Glacier). Khôi phục hạ tầng mới khi gặp sự cố. RTO: vài giờ tới vài ngày. RPO: phụ thuộc tần suất backup. Chi phí: thấp nhất. Pilot Light\nDuy trì các dịch vụ lõi ở trạng thái thu nhỏ trên AWS. Scale lên toàn bộ sản xuất khi DR. RTO: hàng giờ. RPO: vài phút. Chi phí: trung bình. Warm Standby\nHệ thống hoàn chỉnh chạy ở quy mô giảm trên AWS. Scale lên khi failover. RTO: phút – giờ. RPO: giây – phút. Chi phí: cao hơn. Multi-Site (Active/Active hoặc Active/Passive)\nMôi trường production chạy song song giữa on-prem và AWS, hoặc giữa nhiều Region AWS. Có thể chuyển hướng traffic ngay lập tức (Route 53, Global Accelerator). RTO/RPO: gần như bằng 0. Chi phí: cao nhất. So sánh chiến lược DR:\nChiến lược RTO RPO Chi phí Độ phức tạp Backup \u0026amp; Restore Giờ – Ngày Giờ $ Thấp Pilot Light Giờ Phút $$ Trung bình Warm Standby Phút Giây $$$ Trung bình-Cao Multi-Site Giây Gần 0 $$$$ Cao Best Practices cho DR Lập kế hoạch Xác định yêu cầu RTO và RPO. Tài liệu hóa quy trình khôi phục. Nhận diện hệ thống và phụ thuộc quan trọng. Thiết lập kế hoạch truyền thông. Triển khai Tự động hóa quy trình khôi phục. Sử dụng nhiều AZ và Region. Triển khai cơ chế sao chép dữ liệu. Kiểm thử backup định kỳ. Kiểm thử Thực hiện diễn tập DR thường xuyên. Thử nghiệm quy trình khôi phục. Đo lường RTO/RPO thực tế. Cập nhật tài liệu. Hands-On Labs Lab 14 – AWS VM Import/Export (Phần 2) Import máy ảo lên AWS → 14-02.3 Deploy instance từ AMI → 14-02.4 Thiết lập ACL cho S3 Bucket → 14-03.1 Export máy ảo từ instance → 14-03.2 Dọn dẹp tài nguyên trên AWS → 14-05 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/",
	"title": "Ngày 24 - SCPs, Identity Center &amp; KMS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-09 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Service Control Policy (SCP) Xác định quyền tối đa cho tài khoản; chỉ giới hạn chứ không cấp quyền. Áp dụng cho tài khoản hoặc OU; ảnh hưởng tất cả user/role, kể cả root; Deny ghi đè Allow. Ví dụ SCP (cấm xóa bucket):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Tình huống dùng SCP:\nNgăn tài khoản rời khỏi organization. Giới hạn region được phép tạo tài nguyên. Ép buộc yêu cầu mã hóa. Ngăn tắt các dịch vụ bảo mật. Bắt buộc gắn tag nhất định cho tài nguyên. Best practice:\nBắt đầu với least privilege. Thử nghiệm ở môi trường non-prod trước. Dùng explicit deny cho kiểm soát quan trọng. Ghi chú mục đích từng SCP. Rà soát và cập nhật định kỳ. AWS Identity Center (trước là AWS SSO) Tập trung hóa truy cập vào tài khoản AWS và ứng dụng bên ngoài. Nguồn danh tính: built-in, AWS Managed Microsoft AD, AD on-prem (trust/AD Connector), hoặc IdP ngoài. Permission Set định nghĩa quyền cho user/group trên tài khoản đích (Identity Center tạo IAM role tương ứng). Có thể cấp nhiều permission set cho một người dùng. Tính năng Identity Center:\nSingle sign-on cho nhiều tài khoản AWS. Tích hợp Microsoft Active Directory. Hỗ trợ SAML 2.0. MFA. Quản lý permission tập trung. Ghi log audit qua CloudTrail. AWS Key Management Service (KMS) Dịch vụ quản lý khóa bảo vệ dữ liệu, tích hợp sâu với các dịch vụ AWS và hỗ trợ audit đầy đủ. Điểm nổi bật\nTạo/quản lý khóa mà không cần tự vận hành HSM. Kiểm soát truy cập chi tiết với IAM \u0026amp; key policy; mọi thao tác được log trong CloudTrail. Các nhóm khóa\nKhóa do khách hàng quản lý (CMK), khóa do AWS quản lý và khóa thuộc AWS-owned. Loại khóa KMS:\nSymmetric: Một khóa duy nhất để mã hóa/giải mã (AES-256). Asymmetric: Cặp khóa public/private (RSA, ECC). Tính năng KMS:\nTự động xoay vòng khóa. Key policy và grant. Envelope encryption. Tích hợp với dịch vụ AWS. Log CloudTrail. Khóa multi-region. Hands-On Labs Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Phần 1) Tạo Policy và Role → 33-2.1 Tạo Group và User → 33-2.2 Tạo KMS Key → 33-3 Tạo S3 Bucket → 33-4.1 Upload dữ liệu lên S3 → 33-4.2 Lab 30 – IAM Restriction Policy Tạo Restriction Policy → 30-3 Tạo IAM Limited User → 30-4 Kiểm tra giới hạn của IAM User → 30-5 Dọn dẹp tài nguyên → 30-6 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/",
	"title": "Ngày 29 - Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-16 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon ElastiCache Dịch vụ cache in-memory managed cho Redis và Memcached giúp giảm độ trễ và giảm tải database.\nĐộ trễ micro giây, Multi-AZ với failover, scale đơn giản, tích hợp mã hóa/xác thực, vận hành tự động. Redis: hỗ trợ cấu trúc dữ liệu phong phú, backup, replication, cluster mode. Memcached: cache key-value đơn giản, mở rộng ngang với auto-discovery. Use case điển hình: tăng tốc web/mobile, cache truy vấn DB, session store, leaderboard, pub/sub, queue.\nElastiCache for Redis – điểm nổi bật:\nCấu trúc dữ liệu: strings, lists, sets, sorted sets, hashes, bitmap, hyperloglog. Persistence: snapshot và AOF. Replication: mô hình primary-replica tự failover. Cluster Mode: phân mảnh dữ liệu trên nhiều shard. Pub/Sub: nhắn tin thời gian thực. Lua Scripting: chạy logic phía server. Geospatial: truy vấn tọa độ. ElastiCache for Memcached – điểm nổi bật:\nMulti-threaded: tận dụng đa lõi. Auto Discovery: client tự nhận node mới. Horizontal Scaling: thêm/bớt node dễ dàng. Đơn giản: không persistence, cấu hình nhẹ. So sánh Redis vs Memcached:\nTiêu chí Redis Memcached Cấu trúc dữ liệu Phong phú Đơn giản (key-value) Persistence Có Không Replication Có Không Multi-AZ Có Không Backup/Restore Có Không Pub/Sub Có Không Đa luồng Không Có Chiến lược Caching Cache-Aside (Lazy Loading) Ứng dụng kiểm tra cache trước, nếu miss thì đọc DB và ghi lại vào cache. Ưu: chỉ cache dữ liệu thực sự cần. Nhược: cache miss gây trễ, dữ liệu có thể cũ. Write-Through Ghi đồng thời vào cache và database. Ưu: dữ liệu đọc luôn tươi. Nhược: ghi chậm hơn, có thể lưu trữ dữ liệu ít dùng. Write-Behind (Write-Back) Ghi vào cache tức thì, đồng bộ xuống DB bất đồng bộ. Ưu: ghi rất nhanh, giảm tải DB. Nhược: rủi ro mất dữ liệu nếu cache lỗi, phức tạp hơn. Tình huống sử dụng Session Store:\n# Lưu session user vào Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Lấy session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Ghi điểm vào sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Lấy top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Đếm số lần gọi trong 60 giây pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Labs thực hành Lab 43 – AWS Database Migration Service (DMS) (Phần 3) Kiểm tra dữ liệu trên S3 → 43-12 Tạo migration serverless → 43-13 Tạo thông báo sự kiện → 43-14 Theo dõi log → 43-15 Xử lý sự cố: Memory Pressure → 43-16 Xử lý sự cố: Table Error → 43-17 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.4-day34-2025-10-23/",
	"title": "Ngày 34 - FastAPI Clean Architecture",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-23 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tổng quan Clean Architecture Tách rõ phần cấu hình, model, route và core logic để dễ mở rộng/kiểm thử. Giữ main.py nhẹ: chỉ khởi tạo app, load config và mount router. Dùng Pydantic model để chuẩn hóa request/response, đảm bảo contract trùng OpenAPI. backend/\r├── main.py\r├── core/\r│ └── config.py\r├── models/\r│ └── book.py\r├── routes/\r│ └── books.py\r└── services/\r└── books.py Cấu hình \u0026amp; Dependency core/config.py đọc biến môi trường, gom cấu hình CORS, API prefix, debug flag. Tận dụng dependency injection của FastAPI để truyền service vào router. Giúp thay datasource (in-memory → PostgreSQL) mà không đổi interface hàm. CORS \u0026amp; Độ ổn định API CORS chỉ mở cho origin cần thiết (http://localhost:3000 trong giai đoạn dev). Bật allow_methods=[\u0026quot;GET\u0026quot;] cho slice đầu tiên để giảm bề mặt tấn công. Đảm bảo /openapi.json luôn truy cập được nhằm phục vụ công cụ contract testing. Bắt đầu đơn giản, refactor sau Dùng repository in-memory để demo nhanh, sau đó mới thêm DB thật. Ghi chú TODO rõ ràng để không quên khi sang sprint mới. Logging tối giản, tập trung các lỗi quan trọng (timeout, data mismatch). Labs thực hành Refactor main.py chỉ còn khởi tạo app và register router. Viết service get_book_detail(id) trả dữ liệu giả theo spec. Cấu hình CORSMiddleware khớp URL của frontend mock/production. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.4-day39-2025-10-30/",
	"title": "Ngày 39 - NMT &amp; Tóm Tắt Văn Bản",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-30 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nDịch Máy Neuron (NMT) Tổng Quan Kiến Trúc Câu đầu vào được chuyển đổi thành biểu diễn số và được mã hóa thành biểu diễn sâu bởi một encoder 6 lớp, sau đó được giải mã bởi một decoder 6 lớp thành bản dịch ở ngôn ngữ đích.\nCác Lớp Encoder và Decoder Các lớp bao gồm:\nSelf-attention: Giúp mô hình tập trung vào các phần khác nhau của đầu vào Các lớp feed-forward: Xử lý thông tin Lớp encoder-decoder attention (chỉ decoder): Sử dụng biểu diễn sâu từ lớp encoder cuối cùng Ví Dụ Cơ Chế Attention Tác Vụ Dịch: \u0026ldquo;The woman took the empty magazine out of her gun\u0026rdquo;\nNgôn Ngữ Đích: Czech\nTrực Quan Hóa Self-Attention Khi dịch \u0026ldquo;magazine\u0026rdquo;, cơ chế attention:\nTạo liên kết attention mạnh giữa \u0026lsquo;magazine\u0026rsquo; và \u0026lsquo;gun\u0026rsquo; Điều này giúp dịch chính xác \u0026ldquo;magazine\u0026rdquo; thành \u0026ldquo;zásobník\u0026rdquo; (hộp đạn súng) Thay vì \u0026ldquo;časopis\u0026rdquo; (tạp chí tin tức) Tại Sao Attention Quan Trọng Attention = cơ chế giúp mô hình tập trung vào các phần quan trọng nhất của đầu vào khi tạo ra đầu ra\nNói cách khác: Attention = xử lý thông tin có chọn lọc thay vì tiêu thụ mọi thứ cùng một lúc\nTrong NLP, attention cho phép mô hình quyết định từ nào ảnh hưởng mạnh nhất đến việc hiểu một từ khác trong câu.\nChi Tiết Triển Khai NMT Các Thành Phần Kiến Trúc Mô Hình: Đầu Vào: Input tokens (ngôn ngữ nguồn) Target tokens (ngôn ngữ đích) Bước 1: Tạo Bản Sao Tạo hai bản sao cho mỗi input và target tokens (cần thiết ở các vị trí khác nhau của mô hình)\nBước 2: Encoder Một bản sao của input tokens → encoder Chuyển đổi thành vector key và value Đi qua lớp embedding → LSTM Bước 3: Pre-attention Decoder Một bản sao của target tokens → pre-attention decoder Dịch chuỗi sang phải + thêm token start-of-sentence (teacher forcing) Đi qua lớp embedding → LSTM Đầu ra trở thành vector query Lưu ý: Encoder và pre-attention decoder có thể chạy song song (không có phụ thuộc)\nBước 4: Chuẩn Bị cho Attention Lấy các vector query, key, value Tạo padding mask để xác định padding tokens Sử dụng bản sao của input tokens cho bước này Bước 5: Lớp Attention Truyền queries, keys, values và mask vào lớp attention\nĐầu ra là context vectors và mask Bước 6: Post-attention Decoder Bỏ mask, truyền context vectors qua:\nLSTM Lớp Dense LogSoftmax Bước 7: Đầu Ra Mô hình trả về:\nLog probabilities Bản sao của target tokens (để tính loss) Tóm Tắt Văn Bản Tóm tắt = nén nội dung trong khi vẫn giữ các ý chính\nHai Loại: 1. Tóm Tắt Extractive Khái niệm: Chọn các câu quan trọng nhất từ văn bản gốc\nĐặc Điểm:\nKhông viết lại văn bản Giữ nguyên từ ngữ gốc Giống như \u0026ldquo;đánh dấu các câu chính\u0026rdquo; Quy Trình (TextRank Cổ Điển):\nTách thành các câu Chuyển đổi câu thành embeddings Tính toán độ tương tự (cosine) Tạo đồ thị (câu là nodes) Xếp hạng sử dụng TextRank Chọn các câu xếp hạng cao nhất Kết Quả: Tập con của văn bản gốc\n2. Tóm Tắt Abstractive Khái niệm: Viết lại các ý chính trong các câu mới\nĐặc Điểm:\nTạo ra các câu chưa từng xuất hiện trong bản gốc Hiểu nội dung → paraphrase Yêu cầu mô hình mạnh (seq2seq, Transformer) Ví Dụ: Bài báo gốc thảo luận về quy trình điều tra của công tố viên\u0026hellip;\nTóm tắt được tạo:\n\u0026ldquo;Công tố viên: Cho đến nay không có video nào được sử dụng trong cuộc điều tra vụ tai nạn.\u0026rdquo;\nCâu này không tồn tại trong bản gốc nhưng nắm bắt ý chính.\nTóm Tắt Extractive vs Abstractive Đặc Điểm Extractive Abstractive Cách tiếp cận Chọn câu hiện có Tạo câu mới Sáng tạo Thấp Cao Độ phức tạp Đơn giản hơn Phức tạp hơn Độ chính xác Trung thành hơn với nguồn Có thể gây lỗi Mô hình TextRank, dựa trên đồ thị Seq2seq, Transformer Pipeline TextRank Tóm tắt extractive từng bước:\nKết hợp các bài báo → văn bản đầy đủ Tách các câu Chuyển đổi câu → vectors (embeddings) Tạo ma trận tương tự Xây dựng đồ thị (câu = nodes, cạnh = tương tự) Xếp hạng nodes sử dụng thuật toán TextRank Chọn các câu xếp hạng cao nhất → Tóm tắt Đây là thuật toán cổ điển thống trị trước deep learning!\nÔn Tập Cú Pháp và Ngữ Nghĩa Cú Pháp – Cấu Trúc Câu Cú Pháp kiểm tra cách các từ kết hợp để tạo thành các câu chính xác ngữ pháp.\nBao Gồm: Thứ tự từ: Tiếng Anh sử dụng S–V–O (Chủ Từ–Động Từ–Tân Từ) Cấu trúc cụm từ: NP (Cụm Danh Từ), VP (Cụm Động Từ), PP (Cụm Giới Từ) Các mối quan hệ phụ thuộc: Cách các từ liên quan đến nhau Liên Quan NLP: Gắn nhãn POS Phân tích cú pháp Nhận dạng thực thể Dịch máy Trả lời câu hỏi Ngữ Nghĩa – Ý Nghĩa của Từ và Câu Ngữ Nghĩa tập trung vào ý nghĩa độc lập với ngữ cảnh bên ngoài.\nBao Gồm: Ngữ nghĩa từ vựng: Ý nghĩa của từ Ngữ nghĩa thành phần: Ý nghĩa của câu Từ đồng nghĩa / trái nghĩa: Ý nghĩa tương tự/đối lập Cấp tính / hạ tính: Mối quan hệ chung/cụ thể Liên Quan NLP: Nhúng từ Các biện pháp tương tự Tìm kiếm ngữ nghĩa Phân loại văn bản Thực Dụng – Ý Định Có Ngữ Cảnh Thực Dụng nghiên cứu ý nghĩa từ ngữ cảnh, ý định của người nói và kiến thức thế giới thực.\nBao Gồm: Hàm ý: Ý nghĩa ẩn Chỉ dẫn: Tham chiếu phụ thuộc ngữ cảnh (cái này/cái kia/ở đây/bạn) Hành động nói: Hứa, yêu cầu, xin lỗi Lịch sự, tính chính thức, châm biếm: Tông điệu và ý định Liên Quan NLP: Hệ thống hội thoại Chatbots Phát hiện cảm xúc và châm biếm Mô hình ngôn ngữ ngữ cảnh (BERT, GPT) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.4-day44-2025-11-06/",
	"title": "Ngày 44 - Các Loại Attention: Self, Masked, Encoder-Decoder",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-06 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nBa Loại Attention trong Transformers Transformer sử dụng attention theo ba cách khác nhau. Hiểu biết từng cách là rất quan trọng.\nLoại 1: Self-Attention (Encoder) Định Nghĩa: Mỗi vị trí attend tới tất cả các vị trí trong chuỗi tương tự.\nTrường Hợp Sử Dụng: Trong encoder, chúng ta muốn mỗi từ hiểu bối cảnh của nó bằng cách nhìn vào tất cả các từ khác.\nVí Dụ:\nCâu: \u0026#34;The cat sat on the mat\u0026#34;\rCho từ \u0026#34;cat\u0026#34;:\r- Attend tới \u0026#34;The\u0026#34;: 0.15 (mạo từ)\r- Attend tới \u0026#34;cat\u0026#34;: 0.40 (chính nó)\r- Attend tới \u0026#34;sat\u0026#34;: 0.20 (động từ)\r- Attend tới \u0026#34;on\u0026#34;: 0.10\r- Attend tới \u0026#34;the\u0026#34;: 0.08\r- Attend tới \u0026#34;mat\u0026#34;: 0.07\rKết quả: Ngữ cảnh \u0026#34;cat\u0026#34; = kết hợp có trọng số của cả 6 từ Tại Sao Hữu Ích:\nNắm bắt ngữ cảnh câu đầy đủ Có thể xác định các mối quan hệ (chủ ngữ-động từ, tính từ-danh từ, v.v.) Mỗi từ nhận thông tin từ toàn bộ câu Triển Khai:\nQ = K = V = Đầu vào (cùng một nguồn!)\rattention(Q, K, V) = softmax(Q×K^T / √d_k) × V Vì Q, K, V đến từ cùng một nơi, nó được gọi là \u0026ldquo;self-attention\u0026rdquo;.\nLoại 2: Masked Self-Attention (Decoder) Vấn Đề: Trong quá trình huấn luyện, nếu decoder có thể \u0026ldquo;nhìn thấy\u0026rdquo; các từ tương lai, nó gian lận!\nVí Dụ - Vấn Đề:\nTác Vụ: Dịch \u0026#34;Je suis heureux\u0026#34; → \u0026#34;I am happy\u0026#34;\rHuấn Luyện:\rBước 1: Dự đoán \u0026#34;am\u0026#34; bằng cách sử dụng... \u0026#34;am\u0026#34; (nó có thể nhìn thấy câu trả lời!)\rBước 2: Dự đoán \u0026#34;happy\u0026#34; bằng cách sử dụng \u0026#34;I am happy\u0026#34; (biết câu trả lời!)\rBước 3: Dự đoán \u0026#34;happy\u0026#34; đã xong (gian lận!)\rKết Quả: Mô hình huấn luyện hoàn hảo nhưng thất bại vào thời gian kiểm tra! Giải Pháp: Che (ẩn) các vị trí tương lai trong quá trình self-attention.\nMasked Self-Attention:\nThay vì: Chúng ta làm:\r[0.30, 0.33, 0.37] [0.30, -∞, -∞]\r[0.26, 0.37, 0.37] → [0.26, 0.37, -∞]\r[0.25, 0.36, 0.39] [0.25, 0.36, 0.39]\rSau softmax:\r[1.00, 0.00, 0.00]\r[0.30, 0.70, 0.00]\r[0.25, 0.36, 0.39]\r(chuẩn hóa) Ma Trận Mask:\nMask = [1, 0, 0]\r[1, 1, 0]\r[1, 1, 1]\rHoặc: -∞ cho các vị trí được che Hiệu Ứng:\nVị Trí 0: Attend tới vị trí 0 chỉ\rVị Trí 1: Attend tới vị trí 0, 1 chỉ\rVị Trí 2: Attend tới vị trí 0, 1, 2\rDecoder chỉ có thể sử dụng thông tin quá khứ! Tại Sao Điều Này Hoạt Động:\nTrong quá trình huấn luyện, có thể sử dụng tạo hình autoregressive Trong quá trình suy luận, tạo từ từng từ một một cách tự nhiên Ngăn chặn mô hình \u0026ldquo;nhìn thấy câu trả lời\u0026rdquo; Loại 3: Encoder-Decoder Attention Mục Đích: Decoder attend tới đầu ra encoder.\nVí Dụ:\nEncoder xử lý: \u0026#34;Je suis heureux\u0026#34; (Tiếng Pháp)\rTạo ra: Context vectors C\rDecoder xử lý: \u0026#34;\u0026#34; (bắt đầu rỗng)\rĐể tạo ra từ đầu tiên:\r- Query: từ decoder (tôi nên dịch cái gì?)\r- Key, Value: từ encoder (tôi nên nhìn vào các từ tiếng Pháp nào?)\rKết Quả: Decoder attend tới các từ tiếng Pháp để tạo ra tiếng Anh Khác Biệt Chính So Với Self-Attention:\nSelf-Attention: Encoder-Decoder:\rQ, K, V đều từ đầu vào Q từ decoder\rCùng chuỗi K, V từ encoder\rAttend trong bản thân Attend tới chuỗi khác Trường Hợp Sử Dụng:\nDecoder nhìn lại đầu ra encoder Cho phép dịch: Tiếng Pháp → Tiếng Anh Cho phép tóm tắt: Tài Liệu → Tóm Tắt Nói chung hữu ích cho các tác vụ seq2seq So Sánh: Cả Ba Loại Loại Nguồn Q Nguồn K, V Mục Đích Self-Attention Đầu vào Đầu vào Hiểu ngữ cảnh trong chuỗi tương tự Masked Self-Attention Đầu vào Đầu vào (tương lai che) Tạo hình autoregressive, ngăn gian lận Encoder-Decoder Decoder Encoder Hiểu xuyên chuỗi Masked Attention Chi Tiết Toán Học Trước khi che:\nAttention = softmax(Q×K^T / √d_k) × V Với che:\nScores = Q×K^T / √d_k\rMa trận Mask M:\rM[i,j] = 0 nếu j \u0026lt;= i (được phép)\rM[i,j] = -∞ nếu j \u0026gt; i (che tương lai)\rMasked_scores = Scores + M\rAttention = softmax(Masked_scores) × V Ví Dụ với Các Số Thực Điểm attention gốc (3×3):\n[0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] Ma trận mask:\n[0, -∞, -∞]\r[0, 0, -∞]\r[0, 0, 0] Sau khi thêm mask:\n[0.1, -∞, -∞]\r[0.4, 0.5, -∞]\r[0.7, 0.8, 0.9] Sau softmax (áp dụng exp và chuẩn hóa):\nexp(0.1) / exp(0.1) = 1.0, softmax([0.1]) = [1.0]\rVì vậy:\rHàng 0: [1.0, 0, 0]\rexp(0.4) ≈ 1.49, exp(0.5) ≈ 1.65\rHàng 1: [1.49/(1.49+1.65), 1.65/(1.49+1.65), 0] ≈ [0.47, 0.53, 0]\rHàng 2: softmax([0.7, 0.8, 0.9]) (tất cả được phép) Trọng số attention cuối cùng:\n[1.0, 0.0, 0.0]\r[0.47, 0.53, 0.0]\r[0.25, 0.33, 0.42] Hiểu Biết Chính: Vị trí 2 chỉ có thể sử dụng thông tin từ các vị trí 0, 1, 2 (không phải tương lai)\nLuồng Attention Transformer Hoàn Chỉnh ĐẦU VÀO: \u0026#34;Je suis heureux\u0026#34;\r↓\rCÁC LỚP ENCODER (lặp 6 lần):\r├─ Self-Attention: Mỗi từ tiếng Pháp attend tới tất cả các từ tiếng Pháp\r├─ Feed-Forward\r→ Đầu ra: C (vector ngữ cảnh tiếng Pháp)\rCÁC LỚP DECODER (lặp 6 lần):\r├─ Masked Self-Attention: Mỗi từ được tạo attend tới các từ trước đó\r├─ Encoder-Decoder Attention: Từ được tạo attend tới ngữ cảnh tiếng Pháp\r├─ Feed-Forward\r→ Đầu ra: Logits cho dự đoán từ tiếp theo\rĐẦU RA: \u0026#34;I am happy\u0026#34; Những Hiểu Biết Chính ✅ Self-Attention: Hiểu lưỡng chiều (encoder) ✅ Masked Attention: Tạo hình một chiều (decoder) ✅ Encoder-Decoder: Chuyển giao xuyên chuỗi ✅ Masking ngăn gian lận: Mô hình không thể sử dụng thông tin tương lai\nTại Sao Không Luôn Sử Dụng Cả Ba? BERT (Encoder-only): Chỉ sử dụng self-attention (lưỡng chiều, tốt cho phân loại) GPT (Decoder-only): Chỉ sử dụng masked self-attention (autoregressive, tốt cho tạo hình) T5 (Đầy Đủ): Sử dụng cả ba (cân bằng, tốt cho seq2seq) Tiếp Theo: Triển Khai Bây giờ chúng ta hiểu ba loại attention, chúng ta sẽ thấy cách triển khai chúng trong code!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.4-day49-2025-11-13/",
	"title": "Ngày 49 - T5 đa nhiệm dạng text-to-text",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-13 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nMột mô hình, nhiều tác vụ T5 coi mọi thứ là text-to-text, nên cùng một mô hình xử lý QA, tóm tắt, dịch, phân loại thông qua prompt.\nPrompt hóa tác vụ Ví dụ: question: When is Pi Day? context: ... -\u0026gt; March 14 summarize: \u0026lt;bài viết\u0026gt; translate English to German: \u0026lt;câu\u0026gt; Định dạng thống nhất giúp mô hình chia sẻ biểu diễn giữa các nhiệm vụ. Quy mô dữ liệu Pre-train trên C4 (~800 GB) so với Wikipedia (~13 GB). Corpora lớn, sạch giúp tổng quát hóa tốt hơn. Lợi ích đa nhiệm Encoder-decoder dùng chung, chuyển giao giữa tác vụ tốt hơn. Cải thiện bài toán ít dữ liệu nhờ tín hiệu chéo tác vụ. Ghi chú thực tế Kiểm soát độ dài output bằng decoder max_length và repetition penalty. Với QA, prompt rõ ràng phần question và context. Fine-tune đa nhiệm: cân bằng tỉ lệ batch tránh một task lấn át. Việc thực hành hôm nay Soạn prompt cho QA và tóm tắt của bạn. Chọn kích thước model theo GPU (T5-small/base/large). Lập kế hoạch pha trộn tác vụ (tỉ lệ mỗi task) khi fine-tune. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/1.11.4-day54-2025-11-20/",
	"title": "Ngày 54 - Mạng &amp; quan sát",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-20 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nĐường đi lưu lượng và log Mọi traffic từ LMI đi qua ENI của instance trong VPC provider; cần thiết kế kết nối và monitoring phù hợp.\nEgress \u0026amp; đích Dependency phải truy cập được từ VPC provider (NAT/Transit/Peering/PrivateLink) Log CloudWatch cũng đi qua ENI; mở đường tới endpoint (public hoặc PrivateLink) Security Group Không cần inbound; giữ inbound đóng Outbound phải cover dependency và CloudWatch Quan sát Log CloudWatch như thường, miễn có đường kết nối Theo dõi metric instance (EC2 billing, vCPU) + Lambda (invokes, errors) Lưu ý Bỏ qua cấu hình VPC ở mức function cho LMI Giới hạn băng thông/ENI theo loại instance vẫn áp dụng "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/1.12.4-day59-2025-11-27/",
	"title": "Ngày 59 - Nền tảng SageMaker AI",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-27 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nTraining serverless \u0026amp; resilient SageMaker AI serverless MLflow cho thử nghiệm nhanh (không cần hạ tầng, auto scale) HyperPod training thêm checkpointless recovery và elastic scaling theo tài nguyên Lợi ích Chu kỳ thử nghiệm nhanh hơn, không phải dựng cluster Giảm chi phí khôi phục lỗi; tận dụng tài nguyên không đồng nhất tốt hơn Việc cần làm Thiết lập workspace MLflow serverless cho thí nghiệm hiện tại Thử checkpointless/elastic training trên một model đại diện; ghi nhận chi phí/thời gian Cập nhật playbook MLOps với mode training và xử lý lỗi mới "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/",
	"title": "Tuần 4 - Dịch vụ Lưu trữ trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-29 đến 2025-10-03\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 4 Tuần này tập trung vào các dịch vụ lưu trữ của AWS, từ S3 cho tới giải pháp hybrid và chiến lược DR.\nNội dung chính Amazon S3 và các lớp lưu trữ. S3 Static Website Hosting. S3 Glacier cho lưu trữ dài hạn. AWS Snow Family. AWS Storage Gateway. Chiến lược Disaster Recovery. AWS Backup. Labs thực hành Lab 13: AWS Backup. Lab 14: AWS VM Import/Export. Lab 24: AWS Storage Gateway. Lab 25: Amazon FSx. Lab 57: Amazon S3 \u0026amp; CloudFront. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/",
	"title": "Ngày 05 - AWS Well-Architected Framework",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-12 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nKhám phá AWS Well-Architected Framework Bộ nguyên tắc thiết kế và phương pháp thực hành tốt nhất để xây dựng kiến trúc cloud tin cậy, bảo mật, hiệu quả và tiết kiệm chi phí. Công cụ Well-Architected trên Console hỗ trợ tự đánh giá và đề xuất hướng cải thiện. Sáu trụ cột của Well-Architected Framework 1. Vận hành xuất sắc (Operational Excellence) Tập trung vận hành và giám sát hệ thống. Liên tục cải thiện quy trình. Tự động hóa thay đổi. Phản ứng kịp thời trước sự kiện. 2. Bảo mật (Security) Bảo vệ thông tin và hệ thống. Quản lý danh tính và quyền truy cập. Thiết lập cơ chế giám sát phát hiện. Bảo vệ hạ tầng. Giữ an toàn cho dữ liệu. 3. Độ tin cậy (Reliability) Tự động phục hồi khi gặp sự cố. Mở rộng ngang để tăng khả năng chịu lỗi. Kiểm thử kịch bản khôi phục. Quản lý thay đổi bằng tự động hóa. 4. Hiệu năng (Performance Efficiency) Sử dụng tài nguyên tính toán hiệu quả. Chọn đúng loại tài nguyên. Giám sát hiệu năng. Đưa ra quyết định dựa trên dữ liệu. 5. Tối ưu chi phí (Cost Optimization) Tránh chi tiêu không cần thiết. Hiểu rõ mô hình sử dụng. Chọn dịch vụ phù hợp. Tối ưu liên tục theo thời gian. 6. Phát triển bền vững (Sustainability) Giảm tác động đến môi trường. Hiểu dấu chân carbon của hệ thống. Tối đa hóa mức sử dụng tài nguyên. Ưu tiên dịch vụ managed. Ôn lại Best Practices Nguyên tắc thiết kế Ngừng đoán dung lượng: Dùng auto scaling. Kiểm thử ở quy mô sản xuất: Dễ dàng nhân bản môi trường. Tự động hóa thử nghiệm kiến trúc: Áp dụng hạ tầng như mã (IaC). Cho phép kiến trúc tiến hóa: Thiết kế linh hoạt để thay đổi. Ra quyết định dựa trên dữ liệu: Luôn giám sát \u0026amp; đo lường. Cải thiện qua game day: Luyện tập kịch bản sự cố. Tổng kết Tuần 1 Nền tảng cloud, lợi ích và mô hình dịch vụ; AWS Global Infrastructure. Công cụ quản trị (console/CLI), Support Plans, chiến lược tối ưu chi phí. Khung Well-Architected và các trụ cột. Labs: IAM Setup, Budgets, Support Plans. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/",
	"title": "Ngày 10 - Elastic Load Balancing",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-19 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Elastic Load Balancing (ELB) Tổng quan Dịch vụ fully-managed phân phối lưu lượng tới nhiều target (EC2, container, v.v.). Hỗ trợ giao thức HTTP, HTTPS, TCP, TLS. Có thể triển khai ở subnet public hoặc private. Cung cấp DNS name; chỉ Network Load Balancer hỗ trợ IP tĩnh. Tích hợp health check và ghi log truy cập (lưu S3). Hỗ trợ sticky session (session affinity). Các loại chính: Application, Network, Classic và Gateway Load Balancer. Application Load Balancer (ALB) Hoạt động ở tầng 7 (HTTP/HTTPS). Hỗ trợ định tuyến theo path (ví dụ /mobile vs /desktop). Target: EC2, Lambda, địa chỉ IP, container (ECS/EKS). Tính năng nổi bật của ALB:\nĐịnh tuyến theo host name. Định tuyến theo path. Định tuyến dựa trên HTTP header. Định tuyến theo query string parameter. Hỗ trợ WebSocket. Hỗ trợ HTTP/2. Network Load Balancer (NLB) Hoạt động ở tầng 4 (TCP/TLS). Hỗ trợ IP tĩnh, xử lý hàng triệu request/giây. Target: EC2, địa chỉ IP, container (ECS/EKS). Điểm mạnh của NLB:\nĐộ trễ cực thấp. Cung cấp địa chỉ IP tĩnh. Giữ nguyên nguồn IP truy cập. Hỗ trợ kết nối TCP dài hạn. Có thể chấm dứt TLS (TLS termination). Gateway Load Balancer (GWLB) Hoạt động ở tầng 3 (gói IP). Sử dụng giao thức GENEVE trên cổng 6081. Định tuyến lưu lượng đến các virtual appliance như firewall, công cụ monitor. Danh sách đối tác: aws.amazon.com/elasticloadbalancing/partners Khám phá AWS Advanced Networking – Specialty Study Guide Sách hướng dẫn chính thức bao quát chủ đề kỳ thi, nguyên tắc thiết kế mạng trên AWS và các tình huống kiến trúc thực tế. Hands-On Labs Lab 20 – AWS Transit Gateway Chuẩn bị môi trường → 20-02 Tạo Transit Gateway → 20-03 Tạo TGW Attachment → 20-04 Tạo TGW Route Table → 20-05 Thêm route TGW vào Route Table của VPC → 20-06 Tổng kết Tuần 2 VPC/Subnet, Security Group/NACL. Kết nối liên VPC (Peering, Transit Gateway) và hybrid (VPN, Direct Connect). Các loại Load Balancer (ALB, NLB, GWLB) và tình huống sử dụng. Labs: VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/",
	"title": "Ngày 15 - Lightsail, EFS &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-26 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Lightsail Dịch vụ compute đơn giản với giá cố định hàng tháng (bắt đầu ~3,5 USD/tháng). Bao gồm băng thông đi kèm với giá thấp hơn EC2. Lý tưởng cho workload nhỏ, môi trường dev/test. Hỗ trợ snapshot để sao lưu. Chạy trong VPC được quản lý và có thể kết nối VPC tiêu chuẩn qua peering (một lần nhấp). Trường hợp dùng Lightsail:\nỨng dụng web đơn giản. Trang WordPress. Môi trường phát triển/thử nghiệm. Ứng dụng doanh nghiệp nhỏ. Học tập và thử nghiệm. So sánh Lightsail và EC2:\nTiêu chí Lightsail EC2 Giá Cố định hàng tháng Trả theo dùng Độ phức tạp Đơn giản Nhiều tùy chọn Khả năng mở rộng Giới hạn Không giới hạn Đối tượng Dự án nhỏ Doanh nghiệp Amazon EFS (Elastic File System) Dịch vụ hệ thống file NFSv4 do AWS quản lý, nhiều EC2 có thể mount đồng thời. Tự động scale tới hàng petabyte. Trả tiền theo dung lượng thực tế sử dụng (khác với EBS phải provision). Có thể mount từ on-prem thông qua VPN hoặc Direct Connect. Tính năng EFS:\nTruy cập đồng thời từ nhiều instance. Tự động mở rộng. Dịch vụ cấp độ Region (đa AZ). Quản lý vòng đời. Mã hóa dữ liệu khi lưu trữ và truyền tải. Các lớp lưu trữ EFS:\nStandard: File truy cập thường xuyên. Infrequent Access (IA): Chi phí thấp cho file ít truy cập. One Zone: Một AZ để tiết kiệm chi phí. Amazon FSx Các hệ thống file được quản lý, mở rộng cho Windows, Lustre, NetApp ONTAP. AWS lo phần thiết lập, mở rộng, sao lưu. Truy cập từ EC2, máy chủ on-prem hoặc người dùng qua giao thức SMB/NFS. Các biến thể FSx:\nFSx for Windows File Server Hệ thống file Windows gốc. Hỗ trợ giao thức SMB. Tích hợp Active Directory. Hỗ trợ DFS namespace. FSx for Lustre Phù hợp workload HPC. Machine Learning, mô phỏng. Độ trễ dưới mili giây. Tích hợp S3. FSx for NetApp ONTAP Hỗ trợ đa giao thức (NFS, SMB, iSCSI). Giảm trùng lặp dữ liệu, nén. Snapshots và replication. AWS Application Migration Service (MGN) Dịch vụ migrate/replicate máy chủ vật lý hoặc ảo lên AWS để DR hoặc hiện đại hóa. Liên tục sao chép máy nguồn sang instance staging EC2 nhẹ. Khi cut-over, MGN tạo EC2 đầy đủ chức năng từ dữ liệu đã replicate. Các giai đoạn migration:\nCài agent lên máy nguồn. Sao chép liên tục vào AWS. Kiểm thử bằng instance test không ảnh hưởng. Cutover sang production. Khám phá Microsoft Workloads on AWS Playlist tuyển chọn về triển khai, tối ưu và best practices khi chạy workload Microsoft trên AWS. Tổng kết Tuần 3 EC2 và các loại instance; AMI, EBS, Instance Store. Auto Scaling và mô hình giá EC2. Dịch vụ liên quan: Lightsail, EFS, FSx. Labs: (ghi chú các lab thực hiện trong tuần). "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/",
	"title": "Ngày 20 - AWS Backup &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-03 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Backup Dịch vụ backup tập trung giúp tự động hóa và quản trị bảo vệ dữ liệu ở quy mô lớn. Khả năng chính Quản lý tập trung: Định nghĩa và áp chính sách backup cho nhiều dịch vụ. Hỗ trợ đa dịch vụ: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3,\u0026hellip; Lịch và vòng đời: Tự động hóa lịch backup và retention. Tuân thủ: Đáp ứng yêu cầu governance và audit. Lợi ích Đơn giản vận hành: Không cần script tùy biến hay công cụ rời rạc. Tiết kiệm thời gian: Tự động bảo vệ dựa trên policy. Báo cáo \u0026amp; audit: Theo dõi trạng thái backup và tuân thủ. Backup Vault Lock Cơ chế đảm bảo tính bất biến, ngăn chỉnh sửa/xóa backup đã bảo vệ nhằm đáp ứng yêu cầu tuân thủ nghiêm ngặt. Tính năng nổi bật của AWS Backup:\nSao chép backup liên vùng. Backup chéo tài khoản. Backup plan (chính sách) linh hoạt. Quản lý vòng đời (lưu trữ lạnh, xóa theo hạn). Mã hóa dữ liệu khi lưu trữ. Gắn thẻ để điều khiển chính sách backup theo tag. Ví dụ Backup Plan:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Khám phá AWS Skill Builder Các learning plan chọn lọc và nội dung chuyên sâu dành cho chuyên gia lưu trữ: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 – AWS Backup Tạo S3 Bucket → 13-02.1 Triển khai hạ tầng mẫu → 13-02.2 Tạo Backup Plan → 13-03 Thiết lập thông báo → 13-04 Kiểm tra khôi phục → 13-05 Dọn dẹp tài nguyên → 13-06 Lab 25 – Amazon FSx (File Systems) Tạo File System SSD Multi-AZ → 25-2.2 Tạo File System HDD Multi-AZ → 25-2.3 Tạo File Share mới → 25-3 Kiểm thử hiệu năng → 25-4 Giám sát hiệu năng → 25-5 Bật tính năng Data Deduplication → 25-6 Bật Shadow Copies → 25-7 Quản lý phiên người dùng \u0026amp; file mở → 25-8 Bật quota người dùng → 25-9 Scale thông lượng → 25-11 Mở rộng dung lượng lưu trữ → 25-12 Xóa môi trường → 25-13 Tổng kết Tuần 4 S3 và các lớp lưu trữ; static website, CORS. Snow Family, Storage Gateway; chiến lược DR và AWS Backup. Labs: Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/",
	"title": "Ngày 25 - AWS Security Hub &amp; Automation",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-10 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Security Hub Tổng hợp và ưu tiên hóa các phát hiện bảo mật, posture across account/dịch vụ. Khả năng\nKiểm tra tự động, chuẩn hóa findings, workflow xử lý ưu tiên. Hỗ trợ chuẩn tuân thủ: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Tích hợp\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer và nhiều công cụ đối tác. Kết quả\nGiảm thời gian thu thập, tập trung khắc phục; cái nhìn hợp nhất và nâng cao hygiene bảo mật. Tính năng Security Hub:\nTheo dõi posture bảo mật liên tục. Kiểm tra tuân thủ tự động. Tổng hợp findings đa tài khoản. Tích hợp 50+ dịch vụ AWS \u0026amp; đối tác. Custom insight và dashboard. Tự động khắc phục qua EventBridge. Chuẩn bảo mật được hỗ trợ:\nAWS Foundational Security Best Practices: hơn 50 control. CIS AWS Foundations Benchmark: chuẩn ngành. PCI DSS: tiêu chuẩn thẻ thanh toán. NIST: khung bảo mật NIST. Security Automation Dịch vụ AWS phục vụ tự động hóa:\nAWS Config: Theo dõi thay đổi cấu hình tài nguyên. Amazon EventBridge: Tự động hóa dựa trên sự kiện. AWS Lambda: Hàm serverless xử lý remediation. AWS Systems Manager: Tự động vá lỗi và quản lý tuân thủ. Mẫu tự động hóa phổ biến:\nTự động khắc phục tài nguyên không tuân thủ. Ứng phó sự cố tự động. Kiểm tra quy tắc security group. Cưỡng chế mã hóa. Đảm bảo tuân thủ tag. Khám phá AWS Certified Security – Specialty: All-in-One Exam Guide (SCS-C01) Tài liệu ôn luyện toàn diện cho chứng chỉ Security Specialty. Hands-On Labs Lab 18 – AWS Security Hub Bật Security Hub → 18-02 Đánh giá từng bộ tiêu chí → 18-03 Dọn dẹp tài nguyên → 18-04 Lab 22 – AWS Lambda Automation with Slack Tạo VPC → 22-2.1 Tạo Security Group → 22-2.2 Tạo EC2 Instance → 22-2.3 Cấu hình Slack Incoming Webhook → 22-2.4 Tạo tag cho instance → 22-3 Tạo role cho Lambda → 22-4 Hàm Stop Instance → 22-5.1 Hàm Start Instance → 22-5.2 Kiểm tra kết quả → 22-6 Dọn dẹp → 22-7 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Phần 2) Sử dụng tag với CLI → 27-2.2 Tạo Resource Group → 27-3 Dọn dẹp tài nguyên → 27-4 Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Phần 2) Tạo CloudTrail → 33-5.1 Ghi log vào CloudTrail → 33-5.2 Tạo Amazon Athena → 33-5.3 Query bằng Athena → 33-5.4 Kiểm tra \u0026amp; chia sẻ dữ liệu S3 đã mã hóa → 33-6 Dọn dẹp → 33-7 Lab 44 – IAM Advanced Role Control Tạo IAM Group → 44-2 Tạo IAM User → 44-3.1 Kiểm tra quyền → 44-3.2 Tạo IAM Role Admin → 44-4.1 Cấu hình Switch Role → 44-4.2 Giới hạn Switch Role theo IP → 44-4.3.1 Giới hạn Switch Role theo thời gian → 44-4.3.2 Dọn dẹp → 44-5 Tổng kết Tuần 5 Shared Responsibility Model; IAM (user/group/role/policy), KMS. Cognito, Organizations/SCP, Identity Center, Security Hub. Labs: Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/",
	"title": "Ngày 30 - Database Migration &amp; Best Practices",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-17 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Database Migration Service (DMS) AWS DMS hỗ trợ di chuyển cơ sở dữ liệu lên AWS nhanh chóng, bảo mật và giảm tối đa downtime.\nĐiểm nổi bật:\nHomogeneous Migration: cùng engine (ví dụ Oracle → Oracle). Heterogeneous Migration: khác engine (Oracle → Aurora). Continuous Replication: giữ đồng bộ nguồn và đích. Schema Conversion: dùng AWS Schema Conversion Tool (SCT). Kiểu migration:\nFull Load: di chuyển toàn bộ dữ liệu hiện tại một lần. Full Load + CDC: tải ban đầu và đồng bộ thay đổi liên tục (Change Data Capture). CDC Only: chỉ replicate phần thay đổi mới. Nguồn hỗ trợ:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Đích hỗ trợ:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Best Practices cho Database Tối ưu hiệu năng RDS/Aurora:\nChọn đúng loại instance. Bật Enhanced Monitoring. Tối ưu câu truy vấn và index. Dùng Read Replica cho workload đọc nhiều. Bật Performance Insights để phân tích bottleneck. Redshift:\nChọn distribution key phù hợp. Dùng sort key cho cột hay lọc. Vacuum \u0026amp; analyze định kỳ. Tận dụng nén cột. Cấu hình workload management (WLM). ElastiCache:\nChọn node type đúng nhu cầu. Dùng cluster mode cho Redis. Cài đặt chính sách loại bỏ (eviction). Theo dõi cache hit rate. Dùng connection pooling. Bảo mật Mã hóa khi lưu (Encryption at Rest): bật cho toàn bộ database. Mã hóa khi truyền: dùng kết nối SSL/TLS. Cô lập mạng: triển khai trong private subnet. IAM Authentication: tận dụng cho RDS/Aurora khi có thể. Secrets Manager: lưu thông tin đăng nhập an toàn. Security Group: giới hạn truy cập tối thiểu. Audit Logging: bật CloudWatch Logs và CloudTrail. High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nBật Multi-AZ cho môi trường production. Cấu hình backup tự động. Thường xuyên kiểm tra quy trình khôi phục. Sử dụng Aurora Global Database cho DR đa vùng. Tạo read replica ở vùng khác nếu cần. Redshift:\nBật snapshot tự động. Sao chép snapshot sang vùng khác. Kết hợp Redshift Spectrum với data lake. Thiết lập cơ chế copy snapshot cross-region. ElastiCache:\nRedis: bật Multi-AZ với failover tự động. Kích hoạt backup/restore. Dùng cluster mode để mở rộng. Thêm retry logic ở mức ứng dụng. Tối ưu chi phí Right-sizing: chọn instance đúng tải. Reserved Instances: cam kết 1–3 năm. Aurora Serverless: cho workload biến thiên. Redshift Serverless: phù hợp phân tích không liên tục. Tối ưu lưu trữ: chọn loại storage thích hợp. Lifecycle Policy: lưu trữ lạnh lên S3/Glacier. Giám sát chi phí: dùng Cost Explorer \u0026amp; Budgets. Khám phá thêm The Data Warehouse Toolkit Tài liệu kinh điển về dimensional modeling và các mẫu thiết kế data warehouse. Tổng kết Tuần 6 Nền tảng Database (RDBMS, NoSQL, OLTP vs OLAP). Dịch vụ chính: RDS/Aurora, Redshift, ElastiCache, DMS. Labs: RDS \u0026amp; EC2 Integration, Database Migration Service. Tổng kết 6 tuần đầu (08/09 - 17/10/2025) Week 1: Cloud fundamentals, hạ tầng, công cụ quản trị, tối ưu chi phí. Week 2: Networking (VPC, kiểm soát bảo mật, load balancing, hybrid). Week 3: Compute (EC2/AMI/storage, auto scaling, pricing). Week 4: Storage (S3/Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR). Week 5: Security \u0026amp; Identity (IAM, Cognito, Organizations/SCP, KMS, Security Hub). Week 6: Databases (RDS, Aurora, Redshift, ElastiCache, DMS). Labs đã hoàn thành: 25+ labs. Kế hoạch tiếp theo: bắt đầu Week 7 từ 20/10/2025. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.5-day35-2025-10-24/",
	"title": "Ngày 35 - Contract Testing &amp; Retrospective",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-24 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Contract Testing với Schemathesis Chạy schemathesis run --checks all --workers 4 --url http://127.0.0.1:8000/openapi.yaml để tạo bộ test tự động. Schemathesis sinh nhiều trường hợp ngẫu nhiên (happy path, edge case, thiếu field). Đảm bảo backend không trả sai schema khi frontend chuyển sang gọi API thật. Lợi ích Không cần viết tay test case phức tạp. Giảm rủi ro mismatch sau refactor. Hoạt động như quality gate trong CI pipeline. Sai sót \u0026amp; cách khắc phục Sai lầm Nguyên nhân Cách xử lý Tạo cả error.tsx và not-found.tsx Thừa xử lý Chỉ giữ not-found.tsx Dùng --base-url trong Schemathesis Sai câu lệnh Dùng --url đúng chuẩn Timeout khi đọc /openapi.json CORS hoặc phản hồi chậm Dùng file YAML trực tiếp Over-engineer backend Tách quá nhiều file sớm Bắt đầu đơn giản, refactor sau Workflow chuẩn đã được validate 1. Define Contract (OpenAPI)\r2. Mock API (Prism)\r3. Build Frontend với mock data\r4. Implement Backend theo spec\r5. Chuyển sang API thật\r6. Contract Testing (Schemathesis) Hỗ trợ frontend và backend phát triển song song. Giảm xung đột, tăng tốc demo và giữ chất lượng ổn định. Key Insights Contract-first giữ spec đồng bộ, giảm lỗi integration. Vertical slice cho phép release từng phần và lấy feedback sớm. Tự động hóa (Prism, Schemathesis) giảm effort test thủ công. Bắt đầu đơn giản, refactor dần khi nhu cầu mở rộng. Labs thực hành Chạy Schemathesis với spec mới nhất và ghi nhận kết quả. Cập nhật README workflow để cả team tham khảo. Chuẩn bị backlog cho vertical slice tiếp theo dựa trên feedback demo. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.5-day40-2025-10-31/",
	"title": "Ngày 40 - Đánh Giá MT &amp; Chiến Lược Decoding",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-31 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nĐiểm BLEU – Đánh Giá Dựa Trên Precision BLEU (Bilingual Evaluation Understudy) là thuật toán được thiết kế để đánh giá chất lượng dịch máy.\nCách BLEU Hoạt Động Khái Niệm Cốt Lõi: So sánh bản dịch ứng viên với một hoặc nhiều bản dịch tham chiếu (thường là bản dịch của con người)\nPhạm Vi Điểm: 0 đến 1\nGần 1 = bản dịch tốt hơn Gần 0 = bản dịch tệ hơn Tính Toán Điểm BLEU BLEU Vanilla (Có Vấn Đề) Ví Dụ:\nỨng viên: \u0026ldquo;I I am I\u0026rdquo; Tham chiếu 1: \u0026ldquo;Eunice said I\u0026rsquo;m hungry\u0026rdquo; Tham chiếu 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; Quy Trình:\nĐếm có bao nhiêu từ của ứng viên xuất hiện trong bất kỳ tham chiếu nào Chia cho tổng số từ của ứng viên Kết quả: 4/4 = 1.0 (điểm hoàn hảo!)\nVấn Đề: Bản dịch này tệ nhưng lại được điểm hoàn hảo! Một mô hình chỉ xuất ra các từ phổ biến sẽ đạt điểm tốt.\nBLEU Được Sửa Đổi (Tốt Hơn) Thay Đổi Chính: Sau khi khớp một từ, loại bỏ nó khỏi tham chiếu\nVí Dụ Tương Tự:\n\u0026ldquo;I\u0026rdquo; (đầu tiên) → khớp → loại bỏ \u0026ldquo;I\u0026rdquo; khỏi tham chiếu → đếm = 1 \u0026ldquo;I\u0026rdquo; (thứ hai) → không còn khớp → đếm = 1 \u0026ldquo;am\u0026rdquo; → khớp → loại bỏ \u0026ldquo;am\u0026rdquo; → đếm = 2 \u0026ldquo;I\u0026rdquo; (thứ ba) → không còn khớp → đếm = 2 Kết quả: 2/4 = 0.5 (thực tế hơn!)\nHạn Chế của BLEU ❌ Không xem xét ý nghĩa ngữ nghĩa\nChỉ kiểm tra khớp từ ❌ Không xem xét cấu trúc câu\n\u0026ldquo;Ate I was hungry because\u0026rdquo; vs \u0026ldquo;I ate because I was hungry\u0026rdquo; Cả hai đều được điểm giống nhau! ✅ Vẫn là metric được áp dụng rộng rãi nhất mặc dù có hạn chế\nĐiểm ROUGE – Đánh Giá Dựa Trên Recall ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nBLEU vs ROUGE Metric Tập Trung Tính Toán BLEU Precision Bao nhiêu từ ứng viên có trong tham chiếu? ROUGE Recall Bao nhiêu từ tham chiếu có trong ứng viên? Tính Toán Điểm ROUGE-N Ví Dụ:\nỨng viên: \u0026ldquo;I I am I\u0026rdquo; Tham chiếu 1: \u0026ldquo;Younes said I am hungry\u0026rdquo; (5 từ) Tham chiếu 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; (5 từ) Quy Trình cho Tham chiếu 1:\n\u0026ldquo;Younes\u0026rdquo; → không khớp → đếm = 0 \u0026ldquo;said\u0026rdquo; → không khớp → đếm = 0 \u0026ldquo;I\u0026rdquo; → khớp → đếm = 1 \u0026ldquo;am\u0026rdquo; → khớp → đếm = 2 \u0026ldquo;hungry\u0026rdquo; → không khớp → đếm = 2 Điểm ROUGE cho Ref 1: 2/5 = 0.4\nNếu có nhiều tham chiếu: Tính cho mỗi cái, lấy giá trị lớn nhất\nĐiểm F1 – Kết Hợp BLEU và ROUGE Vì BLEU = precision và ROUGE = recall, chúng ta có thể tính điểm F1:\nCông Thức:\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\rF1 = 2 × (BLEU × ROUGE) / (BLEU + ROUGE) Ví Dụ:\nBLEU = 0.5 ROUGE = 0.4 F1 = 2 × (0.5 × 0.4) / (0.5 + 0.4) = 4/9 ≈ 0.44 Beam Search Decoding Vấn Đề: Chọn từ có xác suất cao nhất ở mỗi bước không đảm bảo chuỗi tốt nhất tổng thể\nGiải Pháp: Beam search tìm các chuỗi có khả năng cao nhất trên một cửa sổ cố định\nCách Beam Search Hoạt Động Độ Rộng Beam (B): Số lượng chuỗi cần giữ ở mỗi bước\nQuy Trình: Bước 1: Bắt đầu với SOS token Lấy xác suất cho từ đầu tiên:\nI: 0.5 am: 0.4 hungry: 0.1 Giữ top B=2: \u0026ldquo;I\u0026rdquo; và \u0026ldquo;am\u0026rdquo;\nBước 2: Tính Xác Suất Có Điều Kiện Cho \u0026ldquo;I\u0026rdquo;:\nI am: 0.5 × 0.5 = 0.25 I I: 0.5 × 0.1 = 0.05 Cho \u0026ldquo;am\u0026rdquo;:\nam I: 0.4 × 0.7 = 0.28 am hungry: 0.4 × 0.2 = 0.08 Giữ top B=2: \u0026ldquo;am I\u0026rdquo; (0.28) và \u0026ldquo;I am\u0026rdquo; (0.25)\nBước 3: Lặp Lại Tiếp tục cho đến khi tất cả B chuỗi đạt EOS token\nBước 4: Chọn Tốt Nhất Chọn chuỗi có xác suất tổng thể cao nhất\nĐặc Điểm Beam Search Ưu Điểm:\nTốt hơn greedy decoding (B=1) Tìm các chuỗi tốt hơn toàn cục Được sử dụng rộng rãi trong production Nhược Điểm:\nTốn bộ nhớ (lưu trữ B chuỗi) Tốn kém về mặt tính toán (chạy mô hình B lần mỗi bước) Phạt các chuỗi dài (tích của nhiều xác suất) Giải Pháp cho Chuỗi Dài: Chuẩn hóa theo độ dài: chia xác suất cho số từ\nMinimum Bayes Risk (MBR) Decoding Khái Niệm: Tạo nhiều mẫu và tìm sự đồng thuận\nQuy Trình MBR: Bước 1: Tạo Nhiều Mẫu Tạo ~30 mẫu ngẫu nhiên từ mô hình\nBước 2: So Sánh Tất Cả Các Cặp Với mỗi mẫu, so sánh với tất cả các mẫu khác sử dụng metric tương tự (ví dụ: ROUGE)\nBước 3: Tính Độ Tương Tự Trung Bình Với mỗi ứng viên, tính độ tương tự trung bình với tất cả các ứng viên khác\nBước 4: Chọn Tốt Nhất Chọn mẫu có độ tương tự trung bình cao nhất (rủi ro thấp nhất)\nCông Thức MBR E* = argmax_E [ trung bình ROUGE(E, E\u0026#39;) cho tất cả E\u0026#39; ] Trong đó:\nE = bản dịch ứng viên E\u0026rsquo; = tất cả các ứng viên khác Mục tiêu: Tìm E tối đa hóa ROUGE trung bình với mọi E' Ví Dụ MBR (4 Ứng Viên) Bước 1: Tính điểm ROUGE theo cặp\nROUGE(C1, C2), ROUGE(C1, C3), ROUGE(C1, C4) Trung bình = R1 Bước 2: Lặp lại cho C2, C3, C4\nLấy R2, R3, R4 Bước 3: Chọn cao nhất\nChọn ứng viên có max(R1, R2, R3, R4) Đặc Điểm MBR Ưu Điểm:\nChính xác hơn về mặt ngữ cảnh so với random sampling Tìm bản dịch đồng thuận Có thể vượt trội beam search Nhược Điểm:\nYêu cầu tạo nhiều mẫu (tốn kém) Yêu cầu so sánh O(n²) Khi Nào Sử Dụng:\nKhi cần bản dịch chất lượng cao Khi chi phí tính toán chấp nhận được Khi đầu ra beam search không nhất quán Tóm Tắt: Các Chiến Lược Decoding Phương Pháp Mô Tả Ưu Điểm Nhược Điểm Greedy Chọn xác suất cao nhất mỗi bước Nhanh, đơn giản Chuỗi không tối ưu Beam Search Giữ top-B chuỗi Chất lượng tốt hơn Chi phí bộ nhớ + tính toán Random Sampling Lấy mẫu từ phân phối Đầu ra đa dạng Chất lượng không nhất quán MBR Đồng thuận từ các mẫu Chất lượng cao Rất tốn kém Tóm Tắt Các Metric Đánh Giá Metric Loại Tập Trung Tốt Nhất Cho BLEU Precision Ứng viên → Tham chiếu MT chung ROUGE Recall Tham chiếu → Ứng viên Tóm tắt F1 Trung bình điều hòa Cả precision \u0026amp; recall Cái nhìn cân bằng Lưu Ý Quan Trọng: Tất cả các metric này:\n❌ Không xem xét ngữ nghĩa ❌ Không xem xét cấu trúc câu ✅ Chỉ đếm khớp n-gram Thay Thế Hiện Đại: Sử dụng các metric neural hoặc đánh giá của con người cho các ứng dụng quan trọng!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.5-day45-2025-11-07/",
	"title": "Ngày 45 - Decoder Transformer &amp; Triển Khai GPT2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-07 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nXây Dựng Decoder Transformer: Kiến Trúc GPT2 Bây giờ hãy xem cách tất cả những phần này kết hợp lại trong mã thực tế!\nCấu Trúc Transformer Decoder (kiểu GPT2) Đầu Vào: Câu được tokenize [1, 2, 3, 4, 5]\r↓\rLớp Embedding: Chuyển đổi tokens thành vectors\r↓\rThêm Positional Encoding: Thêm thông tin vị trí\r↓\r┌──────────────────────────────────┐\r│ Decoder Block (N lần) │\r│ ├─ Masked Self-Attention │\r│ ├─ Residual + LayerNorm │\r│ ├─ Feed-Forward │\r│ └─ Residual + LayerNorm │\r└──────────────────────────────────┘\r↓\rLớp Linear: Chiếu tới kích thước vocab\r↓\rSoftmax: Chuyển đổi thành xác suất\r↓\rĐầu Ra: Xác suất cho từ tiếp theo Triển Khai PyTorch Bước 1: Word Embedding + Positional Encoding import torch import torch.nn as nn class TransformerDecoder(nn.Module): def __init__(self, vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1): super().__init__() # 1. Lớp embedding self.embedding = nn.Embedding(vocab_size, d_model) # 2. Positional encoding (được học) self.positional_encoding = nn.Embedding(max_seq_len, d_model) # 3. Các decoder blocks (lặp N lần) self.decoder_layers = nn.ModuleList([ DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]) # 4. Lớp đầu ra self.final_layer = nn.Linear(d_model, vocab_size) self.softmax = nn.Softmax(dim=-1) self.d_model = d_model def forward(self, input_ids, mask=None): # input_ids shape: [batch_size, seq_length] batch_size, seq_len = input_ids.shape # 1. Nhúng các tokens x = self.embedding(input_ids) # [batch, seq_len, d_model] # 2. Thêm positional encoding positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) pos_encoding = self.positional_encoding(positions) x = x + pos_encoding # [batch, seq_len, d_model] # 3. Truyền qua các lớp decoder for decoder_layer in self.decoder_layers: x = decoder_layer(x, mask) # 4. Chiếu tới vocab logits = self.final_layer(x) # [batch, seq_len, vocab_size] return logits Bước 2: Decoder Block class DecoderBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super().__init__() # 1. Masked multi-head attention self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) self.norm1 = nn.LayerNorm(d_model) # 2. Feed-forward network self.feed_forward = FeedForward(d_model, d_ff, dropout) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask=None): # x shape: [batch, seq_len, d_model] # Masked Self-Attention + Residual + Norm attn_output = self.self_attention(x, x, x, mask) # Q=K=V x = x + self.dropout(attn_output) x = self.norm1(x) # Feed-Forward + Residual + Norm ff_output = self.feed_forward(x) x = x + self.dropout(ff_output) x = self.norm2(x) return x Bước 3: Multi-Head Attention class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads, dropout): super().__init__() assert d_model % num_heads == 0 self.num_heads = num_heads self.d_k = d_model // num_heads # Các phép chiếu tuyến tính cho Q, K, V self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) # Phép chiếu đầu ra self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, Q, K, V, mask=None): batch_size = Q.shape[0] # 1. Các phép chiếu tuyến tính và chia thành nhiều đầu Q = self.W_q(Q) # [batch, seq_len, d_model] K = self.W_k(K) V = self.W_v(V) # Reshape cho multi-head: [batch, seq_len, num_heads, d_k] Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Bây giờ: [batch, num_heads, seq_len, d_k] # 2. Scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, num_heads, seq_len, seq_len] # 3. Áp dụng mask (cho causal masking trong decoder) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 4. Softmax attention_weights = torch.softmax(scores, dim=-1) attention_weights = self.dropout(attention_weights) # 5. Nhân với value context = torch.matmul(attention_weights, V) # [batch, num_heads, seq_len, d_k] # 6. Ghép các đầu context = context.transpose(1, 2).contiguous() context = context.view(batch_size, -1, self.d_model) # 7. Phép chiếu tuyến tính cuối cùng output = self.W_o(context) return output Bước 4: Feed-Forward Network class FeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout): super().__init__() self.linear1 = nn.Linear(d_model, d_ff) # 512 → 2048 self.linear2 = nn.Linear(d_ff, d_model) # 2048 → 512 self.relu = nn.ReLU() self.dropout = nn.Dropout(dropout) def forward(self, x): x = self.linear1(x) # Mở rộng x = self.relu(x) # Non-linearity x = self.dropout(x) # Regularization x = self.linear2(x) # Nén return x Bước 5: Causal Mask (để ngăn chặn attend vào tương lai) def create_causal_mask(seq_len, device): \u0026#34;\u0026#34;\u0026#34; Tạo một mask ngăn chặn attention tới các vị trí tương lai. Đầu Ra: [1, 0, 0, 0] [1, 1, 0, 0] [1, 1, 1, 0] [1, 1, 1, 1] Vị trí i chỉ có thể attend tới các vị trí 0...i \u0026#34;\u0026#34;\u0026#34; mask = torch.tril(torch.ones(seq_len, seq_len, device=device)) return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len] # Sử Dụng: mask = create_causal_mask(seq_len=10, device=\u0026#39;cuda\u0026#39;) Vòng Lặp Huấn Luyện def train_transformer(model, train_loader, epochs=10, learning_rate=0.0001): optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): total_loss = 0 for batch_idx, (input_ids, target_ids) in enumerate(train_loader): # Forward pass logits = model(input_ids) # logits: [batch, seq_len, vocab_size] # target_ids: [batch, seq_len] # Tính toán loss loss = loss_fn( logits.view(-1, vocab_size), target_ids.view(-1) ) # Backward pass optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\u0026#34;Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\u0026#34;) Suy Luận (Tạo Văn Bản) def generate_text(model, start_token, max_length=50, device=\u0026#39;cuda\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Tạo văn bản tự động sử dụng transformer được huấn luyện. \u0026#34;\u0026#34;\u0026#34; model.eval() generated = [start_token] with torch.no_grad(): for _ in range(max_length): # Chuẩn bị đầu vào input_ids = torch.tensor(generated, device=device).unsqueeze(0) # Forward pass logits = model(input_ids) # Lấy logits cho vị trí cuối cùng last_logits = logits[0, -1, :] # [vocab_size] # Lấy mẫu hoặc tham lam next_token = torch.argmax(last_logits).item() # Tham lam # Hoặc: next_token = torch.multinomial(softmax(last_logits), 1).item() # Lấy mẫu generated.append(next_token) if next_token == end_token: break return generated Ví Dụ Hoàn Chỉnh Hoạt Động # Khởi tạo mô hình model = TransformerDecoder( vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1 ).to(\u0026#39;cuda\u0026#39;) # Tạo dữ liệu giả batch_size, seq_len = 32, 128 input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(\u0026#39;cuda\u0026#39;) # Forward pass output = model(input_ids) print(f\u0026#34;Output shape: {output.shape}\u0026#34;) # [32, 128, 10000] # Tạo causal mask mask = create_causal_mask(seq_len, \u0026#39;cuda\u0026#39;) # Forward với mask output_masked = model(input_ids, mask) print(f\u0026#34;Masked output shape: {output_masked.shape}\u0026#34;) # Tạo văn bản generated = generate_text(model, start_token=101, max_length=20) print(f\u0026#34;Generated sequence: {generated}\u0026#34;) Tóm Tắt Các Thành Phần Chính Thành Phần Mục Đích Kích Thước Embedding Token → Vector vocab_size → d_model Positional Encoding Thêm thông tin vị trí d_model Multi-Head Attention Học mối quan hệ d_model → d_model Feed-Forward Phép biến đổi phi tuyến d_model → d_ff → d_model LayerNorm Ổn định huấn luyện per-element Output Layer Chiếu tới vocab d_model → vocab_size Tại Sao Kiến Trúc Này Hoạt Động ✅ Xử Lý Song Song: Tất cả các vị trí được xử lý cùng nhau (nhanh!) ✅ Phụ Thuộc Dài Hạn: Attention trực tiếp tới bất kỳ vị trí nào (không có vanishing gradients!) ✅ Có Thể Diễn Giải: Có thể hình dung các mô hình attention ✅ Có Thể Mở Rộng: Có thể lớn lên tới hàng tỷ tham số\nCác Biến Thể GPT2 GPT-2 Small: 117M tham số GPT-2 Medium: 345M tham số GPT-2 Large: 762M tham số GPT-2 XL: 1.5B tham số Tất cả sử dụng kiến trúc decoder giống nhau, chỉ được mở rộng!\nBước Tiếp Theo Huấn Luyện Trước: Huấn luyện trên kho ngữ liệu văn bản lớn (Wikipedia, Sách, v.v.) Tinh Chỉnh: Điều chỉnh cho các tác vụ cụ thể (dịch, phân loại, v.v.) Đánh Giá: Đo chất lượng (perplexity, BLEU, đánh giá của con người) Triển Khai: Sử dụng cho các ứng dụng thực tế Tóm Tắt Tuần 9 Chúng ta đã bao quát:\n✅ Tại sao transformers thay thế RNNs ✅ Kiến trúc transformer hoàn chỉnh ✅ Cơ chế scaled dot-product attention ✅ Self, masked, và encoder-decoder attention ✅ Chi tiết triển khai và code Đây là nền tảng của NLP hiện đại! Tất cả các mô hình tiên tiến (BERT, GPT, T5, Claude, ChatGPT) đều dựa trên kiến trúc transformer.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.5-day50-2025-11-14/",
	"title": "Ngày 50 - Thực hành fine-tuning",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-14 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nCông thức fine-tuning Tập trung vào các nút chỉnh: đóng băng tầng nào, lên lịch learning rate ra sao, và đánh giá setup transfer như thế nào.\nĐóng băng vs. train Đóng băng tầng thấp khi dữ liệu ít hoặc miền gần pre-train. Mở dần (từ trên xuống) nếu accuracy đứng yên. Thêm head đặc thù (classification/span QA/seq2seq) và bắt đầu từ đó. Hyperparameter cơ bản Learning rate: 1e-5 đến 3e-5 cho fine-tune toàn encoder; cao hơn nếu đa phần đóng băng. Warmup: ~5-10% số bước để ổn định giai đoạn đầu. Max sequence length: khớp tác vụ; chunk tài liệu dài cho QA. Vòng đánh giá Theo dõi loss + metric (EM/F1 cho QA, ROUGE cho tóm tắt, accuracy/F1 cho phân loại). Early stop theo dev; giữ checkpoint tốt nhất, không chỉ checkpoint cuối. So sánh baseline feature-based vs. fine-tune full trên một tập nhỏ. Khi triển khai Distill hoặc quantize nếu cần giảm độ trễ. Cache tokenizer và quy tắc cắt/truncate để tránh lệch train/serve. Log prompt/input để debug hành vi closed-book vs. có ngữ cảnh. Việc thực hành hôm nay Chạy (hoặc lập kế hoạch) grid nhỏ: learning rate, chiến lược đóng băng, max length. Đánh giá trên tập dev và ghi nhận EM/F1 hoặc ROUGE. Quyết định bước hậu huấn luyện: distillation, quantization, hoặc caching. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/1.11.5-day55-2025-11-21/",
	"title": "Ngày 55 - Scaling &amp; vận hành",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-21 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nScaling và kiểm soát chi phí Lên kế hoạch scale ở mức instance và multi-concurrency cho traffic ổn định nhưng giới hạn chi phí.\nGuardrail scale Giới hạn max vCPU cho capacity provider Chọn họ máy theo workload (compute/memory/network) Traffic ổn định: ưu tiên máy lớn + multi-concurrency; nếu quá biến động, cân nhắc ở lại Lambda mặc định Chi phí \u0026amp; giá Áp dụng Savings Plan/Reserved Instance cho công suất LMI Theo dõi sử dụng so với cam kết; điều chỉnh mix instance khi cần Checklist rollout Capacity provider đúng role/VPC/allowlist máy Function version đã publish; đường warm OK (không cold start) CloudWatch truy cập được; cảnh báo lỗi/độ trễ/concurrency Chạy benchmark so sánh LMI vs. Lambda mặc định cho workload của bạn "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/1.12.5-day60-2025-11-28/",
	"title": "Ngày 60 - Compute mới",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-28 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nGraviton5 và Trainium3 Graviton5: CPU thế hệ mới, giá/hiệu năng tốt hơn cho nhiều workload EC2 Trainium3 UltraServers (3nm): throughput train/inference cao hơn, chi phí thấp hơn Phân tích phù hợp Dịch vụ nào chuyển sang Graviton5 (web/app, xử lý dữ liệu) và mức tiết kiệm kỳ vọng Bài train/inference nào hưởng lợi từ Trainium3 so với GPU/CPU hiện tại Việc cần làm Chọn 1–2 dịch vụ benchmark Graviton5; theo dõi perf/chi phí Lập POC Trainium3 cho một model mục tiêu; so throughput và ngân sách Cập nhật kế hoạch capacity với họ máy và giá mới "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/",
	"title": "Tuần 5 - Bảo mật &amp; Danh tính trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-06 đến 2025-10-10\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 5 Tuần này tập trung vào bảo mật và quản lý danh tính trên AWS.\nNội dung chính Mô hình Trách nhiệm chia sẻ. AWS IAM (Users, Groups, Roles, Policies). Amazon Cognito. AWS Organizations \u0026amp; SCPs. AWS Identity Center (SSO). AWS KMS. AWS Security Hub. Labs thực hành Lab 18: AWS Security Hub. Lab 22: AWS Lambda Automation with Slack. Lab 27: AWS Resource Groups \u0026amp; Tagging. Lab 28: IAM Cross-Region Role \u0026amp; Policy. Lab 30: IAM Restriction Policy. Lab 33: AWS KMS \u0026amp; CloudTrail Integration. Lab 44: IAM Advanced Role Control. Lab 48: IAM Access Keys \u0026amp; Roles. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.5-sample-app/",
	"title": "Ứng dụng mẫu: Gợi ý in-app purchase",
	"tags": [],
	"description": "",
	"content": "Bài tập: Gợi ý in-app purchase Xây dựng hàm Lambda nhận danh sách item đã mua và trả về gợi ý các item chưa mua (phép trừ tập hợp đơn giản).\nYêu cầu đầu vào { \u0026#34;allItems\u0026#34;: [\u0026#34;starter_pack\u0026#34;,\u0026#34;booster_pack\u0026#34;,\u0026#34;data_pack\u0026#34;,\u0026#34;golden_apples\u0026#34;,\u0026#34;skins\u0026#34;], \u0026#34;owned\u0026#34;: [\u0026#34;data_pack\u0026#34;,\u0026#34;starter_pack\u0026#34;] } Xử lý Dùng tập hợp để lấy phần còn lại: suggestions = allItems - owned. Trả JSON { \u0026quot;suggestions\u0026quot;: [...] } và mã 200. Log input/output để dễ debug. Kiểm thử Trong console: tạo test event với nhiều giá trị owned. Qua API Gateway: POST /suggest body JSON như trên. Thử lỗi: thiếu allItems hoặc owned → trả 400 với thông báo rõ ràng. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Workshop Tổng quan Workshop serverless với AWS Lambda và API Gateway: tạo hàm Hello World (Node.js/Python), thêm tham số, xuất bản API GET/POST, và xây dựng hàm gợi ý in-app purchase. Tập trung triển khai nhanh, không quản lý server, chú ý chi phí và vận hành.\nNội dung Giới thiệu Chuẩn bị Lambda cơ bản API Gateway Hàm gợi ý (sample app) Dọn dẹp "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Xin chúc mừng bạn đã hoàn thành xong lab này! Trong lab này, bạn đã học về các mô hình kiến trúc để truy cập Amazon S3 mà không sử dụng Public Internet.\nBằng cách tạo Gateway endpoint, bạn đã cho phép giao tiếp trực tiếp giữa các tài nguyên EC2 và Amazon S3, mà không đi qua Internet Gateway. Bằng cách tạo Interface endpoint, bạn đã mở rộng kết nối S3 đến các tài nguyên chạy trên trung tâm dữ liệu trên chỗ của bạn thông qua AWS Site-to-Site VPN hoặc Direct Connect. Dọn dẹp Điều hướng đến Hosted Zones trên phía trái của bảng điều khiển Route 53. Nhấp vào tên của s3.us-east-1.amazonaws.com zone. Nhấp vào Delete và xác nhận việc xóa bằng cách nhập từ khóa \u0026ldquo;delete\u0026rdquo;. Disassociate Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. 4.Mở console của CloudFormation và xóa hai stack CloudFormation mà bạn đã tạo cho bài thực hành này:\nPLOnpremSetup PLCloudSetup Xóa các S3 bucket Mở bảng điều khiển S3 Chọn bucket chúng ta đã tạo cho lab, nhấp chuột và xác nhận là empty. Nhấp Delete và xác nhận delete. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/",
	"title": "Tuần 6 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-13 đến 2025-10-17\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 6 Tuần này tập trung vào hệ sinh thái cơ sở dữ liệu trên AWS: từ dịch vụ quan hệ managed, NoSQL chuyên dụng, bộ nhớ đệm in-memory cho tới data warehouse phân tích.\nNội dung chính Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Labs thực hành Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập tại First Cloud Journey (FCJ) - AWS từ ngày 8 tháng 9 năm 2025 đến ngày 28 tháng 11 năm 2025 (12 tuần), tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia nhiều trình độ học tập AWS và các dự án thực hành, qua đó cải thiện kỹ năng thiết kế kiến trúc đám mây, triển khai dịch vụ AWS, lập tài liệu kỹ thuật, dịch blog và phát triển hội thảo.\nCác thành tựu chính Trong suốt thời gian thực tập, tôi đã hoàn thành những công việc chính sau:\nHành trình học tập AWS (12 tuần): Nghiên cứu có hệ thống các dịch vụ AWS cơ bản bao gồm EC2, S3, RDS, Lambda, API Gateway, VPC, IAM, CloudWatch và các phương pháp hay nhất về bảo mật.\nĐề xuất thách thức: Phát triển các đề xuất toàn diện cho các thách thức và giải pháp cơ sở hạ tầng đám mây.\nDịch blog: Dịch 3+ bài đăng blog AWS kỹ thuật từ tiếng Anh sang tiếng Việt, bao gồm các chủ đề như Amazon SageMaker, phiên bản EC2 Mac và hướng dẫn tối ưu hóa AMD.\nPhát triển hội thảo: Tạo hội thảo thực hành về tích hợp dịch vụ AWS với các ví dụ thực tế và hướng dẫn từng bước cho các phòng thí nghiệm thực hành.\nTham dự sự kiện: Tham dự 2 sự kiện AWS chính:\nVietnam Cloud Day 2025 (ngày 18 tháng 9 năm 2025) AWS GenAI Builder Club - AI-Driven Development Life Cycle (ngày 3 tháng 10 năm 2025) Về tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc X 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh X 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn X 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng X 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc X 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân X 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng X 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm X 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc X 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo X 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team X 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập X Những điểm mạnh Nền tảng kỹ thuật vững chắc: Đã học tập và áp dụng thành công nhiều dịch vụ AWS trên các lĩnh vực tính toán, lưu trữ, mạng, cơ sở dữ liệu và bảo mật. Kỹ năng lập tài liệu: Sản xuất tài liệu kỹ thuật chất lượng cao bao gồm các đề xuất, blog dịch và tài liệu hội thảo. Hợp tác nhóm: Tích cực tham gia các hoạt động nhóm và sự kiện AWS, kết nối với các thành viên FCJ và chuyên gia trong ngành. Khả năng tự học: Thể hiện khả năng nghiên cứu độc lập các khái niệm đám mây phức tạp và thực hiện các phòng thí nghiệm thực hành. Cần cải thiện Nâng cao tính kỷ luật, chấp hành nghiêm chỉnh nội quy của công ty hoặc bất kỳ tổ chức nào Cải thiện cách tư duy giải quyết vấn đề và tiếp cận các thách thức một cách có hệ thống Học cách giao tiếp tốt hơn trong giao tiếp hằng ngày và trong công việc, xử lý tình huống hiệu quả Quản lý thời gian tốt hơn để tuân thủ các thời hạn một cách nhất quán "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": "Đánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Đề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Bạn có muốn tiếp tục chương trình này trong tương lai? Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/",
	"title": "Tuần 7 - Vertical Slice Delivery",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-20 đến 2025-10-24\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 7 Tuần này hoàn thiện vertical slice 0 cho dự án Ebook Demo, tập trung vào contract-first development và tự động hóa kiểm thử để có thể demo end-to-end sớm.\nNội dung chính Vertical Slice Architecture \u0026amp; phạm vi slice 0 Contract-first development với OpenAPI + Prism mock Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture và cấu hình CORS Schemathesis contract testing \u0026amp; retrospective Labs thực hành Checklist demo vertical slice 0 Mock API bằng Prism và kết nối Next.js Refactor backend FastAPI theo clean architecture Chạy Schemathesis và cập nhật workflow chuẩn "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Lê Trọng Nhân\nSố điện thoại: SE190515\nEmail: nhanle221199@gmail.com\nTrường: Đại học FPT TP.HCM\nNgành: Công nghệ thông tin\nLớp: SE190525\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 12/08/2025 đến ngày 12/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/",
	"title": "Tuần 8 - Xử Lý Ngôn Ngữ Tự Nhiên &amp; Deep Learning",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-27 to 2025-10-31\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTổng Quan Tuần 8 Tuần này cung cấp cái nhìn sâu rộng về Xử Lý Ngôn Ngữ Tự Nhiên (NLP), bao gồm nền tảng ngôn ngữ học, ứng dụng NLP hiện đại, kiến trúc sequence-to-sequence, và phương pháp đánh giá. Từ hiểu biết về âm vị học đến triển khai hệ thống dịch máy, tuần này kết nối lý thuyết và thực hành trong NLP.\nCác Chủ Đề Chính Nền Tảng Ngôn Ngữ Học Các thành phần cốt lõi: Âm Vị Học, Âm Vị Học, Hình Thái Học, Cú Pháp, Ngữ Nghĩa, Thực Dụng Hiểu cấu trúc ngôn ngữ ảnh hưởng đến thiết kế NLP như thế nào Ứng Dụng NLP Công cụ tìm kiếm và nhận dạng ý định Quảng cáo trực tuyến với NER và trích xuất mối quan hệ Trợ lý giọng nói và nhận dạng giọng nói Chatbot với pipeline NLU/NLG Hệ thống dịch máy Tóm tắt văn bản (extractive \u0026amp; abstractive) Kiến Trúc Deep Learning Mô hình Seq2seq với kiến trúc encoder-decoder LSTM chi tiết: forget gate, input gate, cell state, output gate Cơ chế Attention và self-attention Triển khai Neural Machine Translation (NMT) Đánh Giá \u0026amp; Decoding Điểm BLEU (dựa trên precision) Điểm ROUGE (dựa trên recall) Điểm F1 để đánh giá MT Beam search decoding Minimum Bayes Risk (MBR) sampling Phòng Thí Nghiệm Thực Hành Xây dựng workflow voicebot và chatbot Triển khai LSTM cho sequence modeling Tạo encoder-decoder với attention Neural machine translation end-to-end Đánh giá chất lượng dịch với BLEU/ROUGE Triển khai beam search và MBR "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/",
	"title": "Tuần 9 - Kiến Trúc &amp; Triển Khai Transformer",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-03 to 2025-11-07\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTóm Tắt Tuần 9 Tuần này khám phá kiến trúc Transformer, một mô hình cách mạng thay thế RNNs trong NLP. Chúng ta sẽ hiểu tại sao cần transformers, cách chúng hoạt động bên trong, và triển khai chúng từ đầu. Từ các cơ chế attention đến thiết kế encoder-decoder đầy đủ, tuần này kết nối lý thuyết và triển khai thực tế.\nCác Chủ Đề Chính Hạn Chế của RNNs \u0026amp; Giới Thiệu Transformer Thắt cổ chai xử lý tuần tự trong RNNs Vấn đề Vanishing Gradient Thắt cổ chai thông tin với chuỗi dài Tại sao Attention là tất cả bạn cần Kiến Trúc Transformer Cấu trúc Encoder-Decoder Lớp Multi-head Attention Positional Encoding Residual Connections \u0026amp; Layer Normalization Feed-forward Networks Cơ Chế Attention Scale Dot-product Attention (cơ chế lõi) Self-attention (cùng một câu) Masked Attention (Decoder) Encoder-Decoder Attention Multi-head Attention để tính toán song song Transformer Decoder \u0026amp; GPT2 Positional Embeddings Decoder Block Implementation Feed-forward Layer Design Tính toán Xác suất Output Ứng Dụng \u0026amp; Các Mô Hình GPT-2 (Generative Pre-trained Transformer) BERT (Bidirectional Encoder Representations) T5 (Text-to-Text Transfer Transformer) Ứng dụng: Dịch, Phân loại, QA, Tóm tắt, Phân tích Cảm xúc Mục Tiêu Học Tập ✅ Hiểu hạn chế của RNN và tại sao transformers giải quyết chúng ✅ Nắm bắt kiến trúc transformer hoàn chỉnh ✅ Triển khai các cơ chế attention từ đầu ✅ Xây dựng transformer decoder (kiểu GPT2) ✅ Nhận biết các ứng dụng transformer và các mô hình tiên tiến Chia Nhỏ Theo Ngày Ngày Tập Trung Chủ Đề 41 Vấn Đề RNN Xử lý tuần tự, Vanishing Gradients, Thắt cổ chai thông tin 42 Tổng Quan Kiến Trúc Encoder-Decoder, Multi-head Attention, Positional Encoding 43 Lõi Attention Công thức Scale Dot-product, Phép toán ma trận, Hiệu quả GPU 44 Các Loại Attention Self-attention, Masked Attention, Encoder-Decoder Attention 45 Triển Khai Decoder Kiến trúc GPT2, Các khối xây dựng, Hướng dẫn Code Điều Kiện Tiên Quyết Hiểu biết sâu về RNNs, LSTMs, và attention từ Tuần 8 Thoải mái với phép toán ma trận và đại số tuyến tính Kiến thức PyTorch hoặc TensorFlow rất hữu ích Các Bước Tiếp Theo Học bài báo \u0026ldquo;Attention is All You Need\u0026rdquo; (Vaswani et al., 2017) Triển khai các thành phần transformer từng bước Thử nghiệm với các mô hình được huấn luyện trước (BERT, GPT-2, T5) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/",
	"title": "Tuần 10 - Transfer Learning, BERT &amp; T5",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-10 đến 2025-11-14\nTrạng thái: \u0026ldquo;Đang thực hiện\u0026rdquo;\nTổng quan Tuần 10 Tuần này tập trung vào transfer learning cho NLP và cách QA hiện đại tận dụng transformer tiền huấn luyện. Ta so sánh huấn luyện truyền thống với reuse đặc trưng và fine-tuning, rồi đi vào hai mô hình tiêu biểu: BERT (ngữ cảnh hai chiều) và T5 (text-to-text đa nhiệm), kèm cách thiết lập QA có ngữ cảnh vs. closed-book.\nChủ đề chính Nền tảng Transfer Learning Pipeline truyền thống vs. pipeline transfer Tái dùng trọng số tiền huấn luyện để hội tụ nhanh hơn So sánh feature-based với fine-tuning Lợi ích: nhanh hơn, chính xác hơn, ít dữ liệu nhãn Hai chế độ Question Answering QA có ngữ cảnh (trích span/sinh ngắn dựa vào đoạn văn) QA closed-book (sinh câu trả lời không có context) Ảnh hưởng của chất lượng tiền huấn luyện lên QA BERT và ngữ cảnh hai chiều Masked Language Modeling cho embedding ngữ cảnh Next Sentence Prediction cho coherence mức câu Dùng cả trái và phải để dự đoán token Ứng dụng: QA, sentiment, phân loại T5 đa nhiệm text-to-text Định dạng text-to-text cho nhiều tác vụ Prompt chung cho rating, QA, tóm tắt, dịch Mở rộng dữ liệu (C4 so với Wikipedia) Chuyển giao đa nhiệm để tổng quát hóa tốt hơn Chiến lược dữ liệu \u0026amp; huấn luyện Pha trộn dữ liệu có nhãn/không nhãn; self-supervised masking Đóng băng backbone vs. thêm head Công thức fine-tuning cho QA/tóm tắt/dịch Mục tiêu học tập Giải thích khi nào nên dùng transfer thay vì huấn luyện từ đầu Phân biệt reuse đặc trưng và fine-tuning toàn bộ So sánh QA có ngữ cảnh và QA closed-book Tóm lược cách BERT và T5 tiền huấn luyện và chuyển giao Nêu vì sao transfer giúp giảm dữ liệu nhãn và thời gian train Lịch trong tuần Ngày Trọng tâm Chủ đề 46 Giới thiệu Transfer Pipeline truyền thống vs. transfer, reuse trọng số, feature-based vs. fine-tuning, lợi ích 47 Question Answering QA có ngữ cảnh vs. closed-book, nhu cầu dữ liệu, cách đánh giá 48 BERT hai chiều Masked LM, NSP, tận dụng ngữ cảnh hai phía cho dự đoán token 49 Mô hình T5 Prompt text-to-text, chia sẻ đa nhiệm, mở rộng dữ liệu (C4 vs. Wikipedia) 50 Thực hành fine-tuning Đóng băng/lộ trình unfreeze, downstream: QA, tóm tắt, dịch Yêu cầu nền tảng Nắm vững kiến trúc transformer từ Tuần 9 Thoải mái với attention và luồng encoder-decoder Cơ bản PyTorch/TensorFlow để fine-tune Bước tiếp theo Đọc paper BERT và T5 để hiểu mục tiêu pre-train Fine-tune thử model BERT QA (kiểu SQuAD span) Thử prompt T5 cho QA, tóm tắt, sentiment So sánh hiệu năng feature-based vs. fine-tune trên dữ liệu của bạn "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.11-week11/",
	"title": "Tuần 11 - Lambda Managed Instances &amp; ghi chú re:Invent",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-17 đến 2025-11-21\nTrạng thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nTổng quan Tuần 11 Tóm tắt session AWS re:Invent 2025 (CNS382) về Lambda Managed Instances (LMI): chạy hàm Lambda trên EC2 do Lambda quản lý, giữ nguyên trải nghiệm serverless nhưng chọn được loại máy, giá EC2, và loại bỏ cold start. Trọng tâm: khi nào chọn LMI vs. Lambda mặc định, cách cấu hình capacity provider, và lưu ý test/ops cho workload lưu lượng ổn định.\nChủ đề chính Vì sao LMI Giữ trải nghiệm Lambda nhưng tự chọn họ máy EC2 và tận dụng Savings Plan/Reserved Instance Không cold start, hỗ trợ multi-concurrency trên instance Dễ dự báo chi phí với traffic ổn định Kiến trúc \u0026amp; Thiết lập Capacity Provider: cấu hình VPC, loại máy (C/M/R, x86/Graviton), guardrail scaling Quy trình: tạo capacity provider -\u0026gt; tạo function gắn provider -\u0026gt; publish version để khởi tạo instance Vòng đời do Lambda quản lý: patch OS/runtime, routing/auto scaling; thấy instance nhưng không can thiệp được Mạng \u0026amp; Bảo mật Toàn bộ egress đi qua ENI của instance trong VPC provider; không cấu hình VPC ở mức function Đóng inbound SG; đảm bảo đường ra tới dependency/CloudWatch (Internet hoặc PrivateLink) Mặc định mã hóa EBS, có thể dùng KMS của bạn Tính năng hàm \u0026amp; Scaling Hỗ trợ ZIP/OCI; runtime Java/Python/Node/.NET; layers, extensions, function URL, response streaming, durable functions Memory/CPU quyết định chọn instance; có thể giới hạn/loại trừ loại máy Guardrail scaling ở mức instance (vd max vCPU) để kiểm soát chi phí Phù hợp \u0026amp; Đánh đổi Dùng LMI cho workload lưu lượng cao, ổn định, hoặc cần compute/memory/network đặc thù Giữ Lambda mặc định cho traffic đột biến, ngắn Multi-concurrency + billing kiểu EC2 thay đổi bức tranh cost/perf Mục tiêu học tập Nêu khi nào chọn LMI thay vì Lambda mặc định Cấu hình capacity provider (VPC, role, loại máy, guardrail) Mô tả mô hình mạng và đường log của LMI Ghép tính năng Lambda (đóng gói, runtime, URL, streaming, durable) với LMI Đặt giới hạn scale/chi phí và chọn họ máy phù hợp workload Lịch trong tuần Ngày Trọng tâm Chủ đề 51 Tổng quan \u0026amp; use case Lợi ích LMI, tận dụng giá EC2, khi nào không dùng 52 Capacity Provider VPC, IAM operator role, shortlist loại máy, KMS, guardrail 53 Hàm trên LMI Đóng gói/runtime, ánh xạ memory/CPU, multi-concurrency, publish version 54 Mạng \u0026amp; quan sát Đường egress, CloudWatch, SG, logging, monitoring 55 Scaling \u0026amp; vận hành Max vCPU, kế hoạch traffic ổn định, kiểm soát chi phí, checklist rollout Yêu cầu nền tảng Nắm trải nghiệm Lambda và transformer từ tuần trước Kiến thức cơ bản EC2/VPC, IAM, CloudWatch Hiểu Savings Plan/Reserved Instance cho EC2 Bước tiếp theo Phác capacity provider cho workload mục tiêu (VPC, shortlist instance, guardrail) Lập kế hoạch benchmark LMI vs. Lambda mặc định theo traffic mẫu Vẽ đường giám sát/log (CloudWatch endpoint, PrivateLink nếu cần) Quyết định dùng SP/RI và mục tiêu multi-concurrency cho từng hàm "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.12-week12/",
	"title": "Tuần 12 - Thông báo AWS re:Invent 2025",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-24 đến 2025-11-28\nTrạng thái: \u0026ldquo;Kế hoạch\u0026rdquo;\nTổng quan Tuần 12 Tóm tắt re:Invent 2025: dòng Nova mới (speech-to-speech, đa phương thức, reasoning giá rẻ), Bedrock mở rộng (model open-weight, reinforcement fine-tuning), vector/AI hạ tầng (S3 Vectors GA), và phần cứng compute mới như Graviton5, Trainium3 UltraServers. Trọng tâm là lập kế hoạch áp dụng thực tế.\nChủ đề chính GenAI \u0026amp; Model Nova 2: Sonic (speech-to-speech), Lite (nhanh/rẻ), Omni (đa phương thức), Forge để huấn luyện frontier model tùy biến Nova Act cho agent UI ổn định; Bedrock AgentCore thêm policy/quality cho agent Bedrock bổ sung model open-weight (Mistral Large 3, Ministral 3) và reinforcement fine-tuning Vector \u0026amp; Dữ liệu Amazon S3 Vectors GA: tới 2B vector/index, ~100ms truy vấn, chi phí thấp hơn DB chuyên dụng Clean Rooms sinh dữ liệu tổng hợp bảo vệ riêng tư cho ML cộng tác Nền tảng AI Dev SageMaker AI với serverless MLflow; training checkpointless và elastic trên HyperPod Compute \u0026amp; Hardware Graviton5 CPU giá/hiệu năng tốt hơn trên EC2 Trainium3 UltraServers (3nm) cho train/inference nhanh và rẻ hơn Mục tiêu học tập Xác định launch AI/compute nào tác động tới workload hiện tại Lên pilot cho Nova và tính năng Bedrock mới Phác lộ trình chuyển sang S3 Vectors cho lưu trữ/search vector Đánh giá phù hợp Graviton5/Trainium3 cho mục tiêu cost/perf Lịch trong tuần Ngày Trọng tâm Chủ đề 56 Model mới Nova 2 (Sonic, Lite, Omni), Forge, Nova Act 57 Bedrock \u0026amp; Agent Model open-weight, reinforcement fine-tuning, AgentCore policy/quality 58 Vector \u0026amp; dữ liệu S3 Vectors GA, Clean Rooms synthetic, kế hoạch scale/chi phí 59 SageMaker Serverless MLflow, checkpointless \u0026amp; elastic training trên HyperPod 60 Compute mới Graviton5, Trainium3 UltraServers, checklist fit/migration Yêu cầu nền tảng Hiểu catalog Bedrock và khả năng agent Nắm kiến trúc vector search cơ bản Biết các họ EC2 và lựa chọn accelerator Bước tiếp theo Chọn 1 pilot cho Nova (speech/đa phương thức/reasoning) và lập kế hoạch đánh giá Lập POC chuyển sang S3 Vectors so với vector store hiện tại Đặt mục tiêu benchmark cho Graviton5/Trainium3 so với máy đang dùng Xác định yêu cầu governance/policy trước khi dùng AgentCore và Nova Act "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
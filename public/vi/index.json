[
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Chuẩn bị tài nguyên",
	"tags": [],
	"description": "",
	"content": "Để chuẩn bị cho phần này của workshop, bạn sẽ cần phải:\nTriển khai CloudFormation stack Sửa đổi bảng định tuyến VPC. Các thành phần này hoạt động cùng nhau để mô phỏng DNS forwarding và name resolution.\nTriển khai CloudFormation stack Mẫu CloudFormation sẽ tạo các dịch vụ bổ sung để hỗ trợ mô phỏng môi trường truyền thống:\nMột Route 53 Private Hosted Zone lưu trữ các bản ghi Bí danh (Alias records) cho điểm cuối PrivateLink S3 Một Route 53 Inbound Resolver endpoint cho phép \u0026ldquo;VPC Cloud\u0026rdquo; giải quyết các yêu cầu resolve DNS gửi đến Private Hosted Zone Một Route 53 Outbound Resolver endpoint cho phép \u0026ldquo;VPC On-prem\u0026rdquo; chuyển tiếp các yêu cầu DNS cho S3 sang \u0026ldquo;VPC Cloud\u0026rdquo; Click link sau để mở AWS CloudFormation console. Mẫu yêu cầu sẽ được tải sẵn vào menu. Chấp nhận tất cả mặc định và nhấp vào Tạo stack. Có thể mất vài phút để triển khai stack hoàn tất. Bạn có thể tiếp tục với bước tiếp theo mà không cần đợi quá trình triển khai kết thúc.\nCập nhật bảng định tuyến private on-premise Workshop này sử dụng StrongSwan VPN chạy trên EC2 instance để mô phỏng khả năng kết nối giữa trung tâm dữ liệu truyền thống và môi trường cloud AWS. Hầu hết các thành phần bắt buộc đều được cung cấp trước khi bạn bắt đầu. Để hoàn tất cấu hình VPN, bạn sẽ sửa đổi bảng định tuyến \u0026ldquo;VPC on-prem\u0026rdquo; để hướng lưu lượng đến cloud đi qua StrongSwan VPN instance.\nMở Amazon EC2 console\nChọn instance tên infra-vpngw-test. Từ Details tab, copy Instance ID và paste vào text editor của bạn để sử dụng ở những bước tiếp theo\nĐi đến VPC menu bằng cách gõ \u0026ldquo;VPC\u0026rdquo; vào Search box\nClick vào Route Tables, chọn RT Private On-prem route table, chọn Routes tab, và click Edit Routes.\nClick Add route. Destination: CIDR block của Cloud VPC Target: ID của infra-vpngw-test instance (bạn đã lưu lại ở bước trên) Click Save changes "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.1-workshop-overview/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu về VPC Endpoint Điểm cuối VPC (endpoint) là thiết bị ảo. Chúng là các thành phần VPC có thể mở rộng theo chiều ngang, dự phòng và có tính sẵn sàng cao. Chúng cho phép giao tiếp giữa tài nguyên điện toán của bạn và dịch vụ AWS mà không gây ra rủi ro về tính sẵn sàng. Tài nguyên điện toán đang chạy trong VPC có thể truy cập Amazon S3 bằng cách sử dụng điểm cuối Gateway. Interface Endpoint PrivateLink có thể được sử dụng bởi tài nguyên chạy trong VPC hoặc tại TTDL. Tổng quan về workshop Trong workshop này, bạn sẽ sử dụng hai VPC.\n\u0026ldquo;VPC Cloud\u0026rdquo; dành cho các tài nguyên cloud như Gateway endpoint và EC2 instance để kiểm tra. \u0026ldquo;VPC On-Prem\u0026rdquo; mô phỏng môi trường truyền thống như nhà máy hoặc trung tâm dữ liệu của công ty. Một EC2 Instance chạy phần mềm StrongSwan VPN đã được triển khai trong \u0026ldquo;VPC On-prem\u0026rdquo; và được cấu hình tự động để thiết lập đường hầm VPN Site-to-Site với AWS Transit Gateway. VPN này mô phỏng kết nối từ một vị trí tại TTDL (on-prem) với AWS cloud. Để giảm thiểu chi phí, chỉ một phiên bản VPN được cung cấp để hỗ trợ workshop này. Khi lập kế hoạch kết nối VPN cho production workloads của bạn, AWS khuyên bạn nên sử dụng nhiều thiết bị VPN để có tính sẵn sàng cao. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/",
	"title": "Ngày 01 - Giới thiệu về Điện toán Đám mây",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-08 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Điện toán Đám mây là gì? Hình thức cung cấp tài nguyên CNTT theo nhu cầu thông qua Internet với mô hình trả phí theo mức sử dụng. Lợi ích của Điện toán Đám mây Chỉ trả tiền cho phần tài nguyên thực tế sử dụng, tối ưu chi phí. Tăng tốc phát triển nhờ dịch vụ quản lý và tự động hóa. Mở rộng/thu hẹp tài nguyên linh hoạt theo nhu cầu. Triển khai ứng dụng toàn cầu chỉ trong vài phút. Vì sao chọn AWS? AWS giữ vị trí dẫn đầu thị trường cloud toàn cầu 13 năm liên tiếp (tính đến 2023). Văn hóa, tầm nhìn và sự ám ảnh khách hàng rất khác biệt. Triết lý giá: khách hàng nên trả ít hơn theo thời gian cho cùng một lượng tài nguyên. Mọi Nguyên tắc Lãnh đạo của AWS đều hướng đến giá trị thực cho khách hàng. Bắt đầu với AWS như thế nào? Có nhiều lộ trình học – tự học hoàn toàn khả thi. Đăng ký tài khoản AWS Free Tier để khám phá dịch vụ. Gợi ý nền tảng khóa học: Udemy A Cloud Guru Tham khảo thêm các lộ trình chính thức của AWS: AWS Learning Paths "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/",
	"title": "Ngày 06 - Kiến thức Cơ bản về Amazon VPC",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-15 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Dịch vụ mạng trên AWS Amazon Virtual Private Cloud (VPC) Amazon VPC cho phép triển khai tài nguyên AWS vào một mạng ảo do chính bạn định nghĩa. Mỗi VPC tồn tại trong phạm vi một Region. Khi tạo VPC, cần định nghĩa dải IPv4 CIDR (bắt buộc) và có thể thêm IPv6. Giới hạn mặc định: 5 VPC mỗi Region cho mỗi tài khoản. Thường dùng để tách biệt môi trường Production, Development, Staging. Muốn cô lập hoàn toàn tài nguyên, nên dùng các AWS Account khác nhau thay vì nhiều VPC cùng tài khoản. Subnet Mỗi subnet nằm trong một Availability Zone. CIDR của subnet phải là tập con của CIDR VPC cha. AWS dành sẵn 5 địa chỉ IP trong mỗi subnet: network, broadcast, router, DNS và một địa chỉ dự phòng. Ví dụ các IP được dành trước (10.0.0.0/24):\n10.0.0.0 – Địa chỉ mạng 10.0.0.1 – Router của VPC 10.0.0.2 – Máy chủ DNS 10.0.0.3 – Dành cho sử dụng trong tương lai 10.0.0.255 – Địa chỉ broadcast Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking Basics Tạo VPC → 03-03.1 Tạo Subnet → 03-03.2 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/",
	"title": "Ngày 11 - Kiến thức cơ bản về Amazon EC2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-22 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Compute trên AWS Amazon Elastic Compute Cloud (EC2) Amazon EC2 cung cấp năng lực tính toán có thể co giãn trên cloud, tương tự máy chủ ảo hoặc vật lý. Phù hợp cho các workload như web hosting, ứng dụng, cơ sở dữ liệu, dịch vụ xác thực và nhiều tác vụ máy chủ tổng quát khác. Instance Type\nCấu hình EC2 được xác định bởi instance type, không phải phần cứng tự chọn. Mỗi loại quy định: CPU (Intel, AMD, ARM – Graviton 1/2/3) / GPU Bộ nhớ Kết nối mạng Lưu trữ Nhóm instance tiêu biểu:\nGeneral Purpose: T3, T4g, M5, M6i (cân bằng giữa compute, memory, network). Compute Optimized: C5, C6i, C7g (hiệu năng xử lý cao). Memory Optimized: R5, R6i, X2 (tối ưu cho workload dùng nhiều RAM). Storage Optimized: I3, D2, H1 (tốc độ đọc/ghi tuần tự cao trên storage cục bộ). Accelerated Computing: P4, G5, Inf1 (GPU/FPGA cho ML, đồ họa). Amazon Machine Image (AMI) AMI (Amazon Machine Image) là mẫu chứa cấu hình phần mềm của instance, gồm hệ điều hành, ứng dụng và thiết lập. Các loại AMI: AMI do AWS cung cấp (Amazon Linux, Windows, Ubuntu, v.v.). AMI trên AWS Marketplace. AMI tùy chỉnh do người dùng tạo. Lợi ích của AMI tùy chỉnh\nLaunch và cấu hình instance nhanh hơn. Đơn giản hóa sao lưu và khôi phục. Đảm bảo môi trường nhất quán trên nhiều instance. Thành phần AMI:\nMẫu ổ đĩa gốc (OS và ứng dụng). Quyền launch. Ánh xạ thiết bị khối (block device mapping). Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Tạo tài khoản AWS → 01-01 Cấu hình thiết bị MFA ảo → 01-02 Tạo nhóm Admin và người dùng Admin → 01-03 Cập nhật hỗ trợ xác thực tài khoản → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/",
	"title": "Ngày 16 - Kiến thức cơ bản về Amazon S3",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-29 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Dịch vụ lưu trữ trên AWS Amazon Simple Storage Service (S3) Amazon S3 là dịch vụ lưu trữ đối tượng, cho phép lưu và truy xuất dữ liệu với quy mô gần như không giới hạn, độ sẵn sàng cao, bảo mật mạnh và hiệu năng tốt.\nTính năng cốt lõi của S3 Bucket và Object: Dữ liệu được lưu dưới dạng object trong bucket. Mỗi object tối đa 5 TB. Availability \u0026amp; Durability: Thiết kế đạt 99,99% availability và 99,999999999% (11 số 9) durability. Bảo mật: Nhiều lớp bảo vệ như IAM, bucket policy, ACL, mã hóa. Khả năng mở rộng: Tự động scale dung lượng và throughput mà không giảm hiệu năng. Cấu trúc object S3:\nKey: Tên/đường dẫn object. Value: Dữ liệu object. Version ID: Dùng khi bật versioning. Metadata: Metadata hệ thống và người dùng. Access Control: Quyền truy cập. S3 Access Points Access Point giúp đơn giản hóa việc quản lý truy cập cho dataset dùng chung.\nKiểm soát theo ứng dụng: Mỗi access point có policy riêng. Đơn giản vận hành: Dễ quản lý quyền cho dataset dùng chung nhiều ứng dụng. Kiểm soát mạng: Có thể cấu hình chỉ cho phép truy cập từ các VPC cụ thể. Các lớp lưu trữ S3 Chọn storage class phù hợp với mô hình truy cập và chi phí:\nS3 Standard: Dữ liệu truy cập thường xuyên; availability và hiệu năng cao nhất. S3 Intelligent-Tiering: Tự động di chuyển object giữa các tier để tối ưu chi phí. S3 Standard-IA: Dữ liệu ít truy cập, vẫn truy xuất mili-giây. S3 One Zone-IA: Tương tự Standard-IA nhưng lưu ở một AZ. S3 Glacier Flexible Retrieval: Lưu trữ chi phí thấp, truy xuất phút–giờ. S3 Glacier Deep Archive: Chi phí thấp nhất, truy xuất ~12 giờ. So sánh storage class:\nClass Độ bền Availability Lưu tối thiểu Thời gian truy xuất Standard 11 số 9 99,99% Không Tức thời Intelligent-Tiering 11 số 9 99,9% Không Tức thời Standard-IA 11 số 9 99,9% 30 ngày Tức thời One Zone-IA 11 số 9 99,5% 30 ngày Tức thời Glacier Flexible 11 số 9 99,99% 90 ngày Phút-giờ Glacier Deep Archive 11 số 9 99,99% 180 ngày 12 giờ Hands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Phần 1) Tạo S3 Bucket → 57-2.1 Tải dữ liệu lên → 57-2.2 Bật Static Website → 57-3 Cấu hình Public Access Block → 57-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/",
	"title": "Ngày 21 - Shared Responsibility &amp; Kiến thức IAM cơ bản",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-06 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bảo mật Mô hình Trách nhiệm chia sẻ (Shared Responsibility Model) Trên môi trường cloud, bảo mật là trách nhiệm được chia sẻ giữa nhà cung cấp và khách hàng. Khách hàng cần cấu hình dịch vụ an toàn, áp dụng best practice và triển khai các kiểm soát bảo mật từ lớp hypervisor trở lên (ứng dụng/dữ liệu). Phạm vi trách nhiệm thay đổi tùy mô hình dịch vụ: Dịch vụ cấp hạ tầng Dịch vụ được quản lý một phần Dịch vụ fully-managed Trách nhiệm của AWS (Security OF the Cloud):\nBảo mật vật lý trung tâm dữ liệu. Hạ tầng phần cứng và mạng. Lớp ảo hóa. Vận hành các dịch vụ managed. Trách nhiệm của khách hàng (Security IN the Cloud):\nMã hóa dữ liệu. Cấu hình mạng. Quản lý truy cập. Bảo mật ứng dụng. Vá OS (đối với EC2). AWS Identity and Access Management (IAM) Root Account Có quyền không giới hạn với mọi dịch vụ/tài nguyên AWS và có thể gỡ mọi quyền đã cấp. Best practice: Tạo và dùng IAM Administrator cho tác vụ hằng ngày. Lưu trữ thông tin root an toàn (dual control). Duy trì email và domain của root hợp lệ, được gia hạn. Bật MFA cho tài khoản root. Tổng quan IAM IAM kiểm soát quyền truy cập dịch vụ/tài nguyên trong tài khoản. Principal bao gồm: root user, IAM user, federated user, IAM role, phiên assumed-role, dịch vụ AWS và người dùng ẩn danh. Ghi nhớ: IAM user không phải là tài khoản AWS riêng. Người dùng mới tạo không có quyền mặc định. Cấp quyền bằng cách gắn policy vào user, group hoặc role. Dùng IAM group để quản lý nhiều user (group không lồng nhau được). Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Phần 1) Tạo EC2 Instance → 48-1.1 Tạo S3 Bucket → 48-1.2 Tạo IAM User và Access Key → 48-2.1 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/",
	"title": "Ngày 26 - Kiến thức Cơ bản về Cơ sở dữ liệu",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-13 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Ôn tập khái niệm cơ sở dữ liệu Một cơ sở dữ liệu là tập hợp dữ liệu có tổ chức (hoặc bán cấu trúc) được lưu trên thiết bị lưu trữ, cho phép nhiều người dùng/chương trình truy cập đồng thời cho các mục đích khác nhau. Phiên làm việc (Session) Phiên bắt đầu khi client kết nối tới DBMS và kết thúc khi đóng kết nối. Khóa chính (Primary Key) Primary key dùng để định danh duy nhất từng dòng trong bảng quan hệ. Khóa ngoại (Foreign Key) Foreign key ở một bảng sẽ trỏ đến primary key của bảng khác để tạo quan hệ dữ liệu. Chỉ mục (Index) Index giúp truy vấn nhanh hơn nhưng phải trả giá bằng ghi chép bổ sung và dung lượng lưu trữ để duy trì cấu trúc chỉ mục. Nhờ index, hệ quản trị có thể tìm bản ghi mà không phải quét toàn bộ bảng; có thể định nghĩa trên một hoặc nhiều cột. Các loại chỉ mục phổ biến:\nB-Tree: Cây cân bằng dùng cho hầu hết tình huống. Hash: Truy vấn theo giá trị chính xác cực nhanh. Bitmap: Hợp lý với cột có số lượng giá trị ít. Full-Text: Tối ưu cho tìm kiếm văn bản. Phân vùng (Partitioning) Partitioning chia một bảng lớn thành nhiều phần nhỏ độc lập (partition) và có thể đặt trên nhiều thiết bị lưu trữ khác nhau. Lợi ích: tăng tốc truy vấn, dễ bảo trì và mở rộng. Các kiểu thông dụng: Range (ví dụ theo ngày/tháng) List Hash Composite (kết hợp nhiều tiêu chí) Ví dụ partition theo range:\n-- Partition theo năm CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan Query plan mô tả cách DBMS thực thi câu lệnh SQL (đường truy cập, kiểu join, sort\u0026hellip;). Có hai dạng: Estimated plan: ước lượng trước khi chạy. Actual plan: sinh ra sau khi thực thi. Thành phần chính: table scan, index seek/scan, nested loop, hash/merge join, sort, aggregate, filter. Database Logs Database log ghi lại mọi thay đổi (INSERT/UPDATE/DELETE) và thao tác hệ thống. Thông dụng: log giao dịch, redo, undo, binary log\u0026hellip; Công dụng: khôi phục dữ liệu, đảm bảo toàn vẹn (ACID), hỗ trợ replicate, phân tích hiệu năng. Bộ đệm (Buffer) Buffer pool lưu các trang dữ liệu được đọc từ đĩa nhằm giảm I/O. Chiến lược quản lý: Thu hồi trang: LRU, FIFO, Clock\u0026hellip; Chính sách ghi: ghi ngay hay trì hoãn. Prefetching để sưởi ấm bộ đệm trước. Labs thực hành Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Phần 1) Tạo VPC → 05-2.1 Tạo Security Group cho EC2 → 05-2.2 Tạo Security Group cho RDS → 05-2.3 Tạo DB Subnet Group → 05-2.4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/",
	"title": "Ngày 31 - Khởi động Vertical Slice",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-20 (Thứ Hai)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bối cảnh dự án Ebook Demo – Vertical Slice 0 Mục tiêu: demo trọn vẹn luồng xem chi tiết sách end-to-end trước khi xây toàn bộ hệ thống. Cách tiếp cận: Vertical Slice Architecture để phát triển từng lát cắt hoàn chỉnh thay vì chia theo tầng kỹ thuật. Lợi ích: trình diễn sớm, phát hiện lỗi sớm, tạo nhịp phối hợp giữa frontend/backend. Kiến trúc slice User → Frontend → API → Database → Response → UI Mỗi slice bao gồm UI, contract API, logic backend và dữ liệu giả phục vụ demo. Có thể thay thế từng thành phần độc lập mà không ảnh hưởng toàn bộ hệ thống. Vertical Slice Architecture Nguyên tắc chính Phát triển theo flow người dùng thay vì bóc tách theo tầng. Giữ scope nhỏ để demo nhanh và nhận feedback liên tục. Xác định rõ trách nhiệm của từng slice để dễ mở rộng. Lợi ích Tăng tốc đóng gói giá trị: có thể show cho stakeholder ngay. Giảm rủi ro tích hợp vì mỗi slice tự kiểm chứng được. Cho phép nhiều slice phát triển song song. Insight chính Vertical slice là nền tảng trước khi mở rộng feature khác. Mỗi slice cần checklist rõ (UI hoàn thiện, contract chuẩn, backend trả dữ liệu đúng). Xem slice như “mini product” với vòng đời riêng giúp giữ được chất lượng. Labs thực hành Xác định phạm vi slice 0 (luồng xem chi tiết sách, dữ liệu tối thiểu). Vẽ sơ đồ dòng dữ liệu và ranh giới giữa frontend/backend. Chuẩn hóa checklist demo (contract, mock, UI, backend). "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.1-day36-2025-10-27/",
	"title": "Ngày 36 - Nền Tảng NLP &amp; Ứng Dụng",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-27 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nXử Lý Ngôn Ngữ Tự Nhiên là gì? Xử Lý Ngôn Ngữ Tự Nhiên (NLP) là một lĩnh vực của Trí Tuệ Nhân Tạo tập trung vào việc giúp máy tính có thể hiểu, giải thích, tạo ra và tương tác với ngôn ngữ con người.\nNLP kết hợp ngôn ngữ học tính toán, học máy và học sâu để xử lý dữ liệu văn bản và lời nói quy mô lớn.\nCác Tác Vụ NLP Điển Hình: Phân loại văn bản Phân tích cảm xúc Nhận dạng thực thể (NER) Dịch máy Gắn nhãn từ loại (POS tagging) Nhận dạng giọng nói Các Thành Phần Ngôn Ngữ Cốt Lõi trong NLP Âm Vị Học – Những Âm Thanh của Lời Nói Con Người Âm Vị Học nghiên cứu các đặc tính vật lý của những âm thanh lời nói.\nBa Nhánh Chính: Âm vị học phát âm: cách những âm thanh được tạo ra (lưỡi, môi, dây thanh…) Âm vị học âm học: các đặc tính vật lý của âm thanh (tần số, biên độ, thời lượng) Âm vị học thính giác: con người cảm nhận âm thanh như thế nào Liên Quan NLP: Được sử dụng trong nhận dạng giọng nói, tổng hợp giọng nói (TTS), mô hình âm thanh.\nÂm Vị Học – Hệ Thống Âm Thanh của Ngôn Ngữ Âm Vị Học nghiên cứu cách những âm thanh hoạt động trong một ngôn ngữ cụ thể. Nó đề cập đến phonemes, mô hình nhấn mạnh, các tổ hợp âm thanh được phép.\nLiên Quan NLP: Chuyển đổi grapheme sang phoneme, mô hình phát âm.\nHình Thái Học – Cấu Trúc của Từ Hình Thái Học nghiên cứu cách những từ được hình thành từ những đơn vị nhỏ hơn gọi là morphemes.\nVí Dụ: Tiền tố: un-, re-, pre- Hậu tố: -ing, -ed, -ness Gốc/thân từ: run, happy, form Liên Quan NLP:\nStemming Lemmatization Tokenization Xây dựng từ vựng cho mô hình BoW Các Ứng Dụng NLP Công Cụ Tìm Kiếm Những tìm kiếm hàng ngày của bạn trên các công cụ tìm kiếm được tạo thuận lợi bởi NLP để hiểu truy vấn và xếp hạng kết quả.\nVí Dụ Nhận Dạng Ý Định Tìm Kiếm Khi ai đó tìm kiếm \u0026ldquo;glass coffee tables\u0026rdquo; (bàn cà phê mặt kính), công cụ nhận dạng ý định xác định rằng từ \u0026ldquo;glass\u0026rdquo; có khả năng đề cập đến giá trị của thuộc tính \u0026lsquo;Top Material\u0026rsquo; (Chất Liệu Bề Mặt) trong bàn cà phê. Sau đó, nó chỉ dẫn công cụ tìm kiếm hiển thị danh mục bàn cà phê với thuộc tính \u0026lsquo;Top Material\u0026rsquo; được đặt thành \u0026lsquo;glass\u0026rsquo;.\nQuảng Cáo Trực Tuyến NLP cho phép quảng cáo được nhắm mục tiêu bằng cách phân tích hành vi trực tuyến thông qua nhiều thành phần:\n1. Nhận Dạng Thực Thể (NER) Xác định các yếu tố thông tin được chọn gọi là Thực Thể. Do không có dữ liệu được gắn nhãn, các phương pháp bán giám sát được áp dụng để phát hiện các thực thể cụ thể cho từng trường hợp sử dụng.\n2. Trích Xuất Mối Quan Hệ Một trong những tác vụ NLP cổ điển nhằm trích xuất các mối quan hệ ngữ nghĩa từ các tài liệu văn bản không có cấu trúc hoặc bán cấu trúc.\n3. Nhận Dạng Khoảnh Khắc (MoRec) Cho phép các nhà phân tích hiểu các cuộc thảo luận trên diễn đàn trong giai đoạn khám phá kiến thức bằng cách xử lý văn bản thảo luận không có cấu trúc và trích xuất kiến thức dưới dạng sự kiện. Các sự kiện có thể được xác định và cấu hình tùy thuộc vào trường hợp sử dụng đang được điều tra.\nTrợ Lý Giọng Nói Siri, Alexa và Google Assistant sử dụng NLP để hiểu và trả lời các lệnh giọng nói của bạn.\nDịch Máy Các dịch vụ như Google Translate dựa vào NLP để chuyển đổi văn bản từ một ngôn ngữ sang ngôn ngữ khác.\nChatbot Chatbot dịch vụ khách hàng sử dụng NLP để tương tác với người dùng và cung cấp hỗ trợ.\nTóm Tắt Văn Bản Các thuật toán NLP có thể nén các bài viết dài thành những bản tóm tắt ngắn gọn.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.1-day41-2025-11-03/",
	"title": "Ngày 41 - Vấn Đề RNN &amp; Tại Sao Cần Transformers",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-03 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nVấn Đề với RNNs: Thắt Cổ Chai Xử Lý Tuần Tự RNNs đã thống trị NLP trong những năm qua, nhưng chúng có những hạn chế cơ bản mà transformers giải quyết. Hãy khám phá những vấn đề này.\nVấn Đề 1: Tính Toán Tuần Tự Cách RNNs Xử Lý Thông Tin RNNs phải xử lý đầu vào từng bước một, theo tuần tự:\nVí Dụ Dịch (Tiếng Anh → Tiếng Pháp):\nĐầu vào: \u0026#34;I am happy\u0026#34;\rBước Thời Gian 1: Xử lý \u0026#34;I\u0026#34;\rBước Thời Gian 2: Xử lý \u0026#34;am\u0026#34; Bước Thời Gian 3: Xử lý \u0026#34;happy\u0026#34; Tác Động:\nNếu câu của bạn có 5 từ → cần 5 bước tuần tự Nếu câu của bạn có 1000 từ → cần 1000 bước tuần tự Không thể song song hóa! Phải đợi bước t-1 trước khi tính bước t Tại Sao Điều Này Quan Trọng GPU và TPU hiện đại được thiết kế cho tính toán song song RNNs tuần tự không thể tận dụng song song hóa này Huấn luyện trở nên chậm hơn nhiều so với cần thiết Chuỗi dài hơn = thời gian huấn luyện dài hơn theo hàm mũ Vấn Đề 2: Vanishing Gradient Problem Nguyên Nhân Gốc Rễ Khi RNNs backpropagate qua nhiều time step, gradients được nhân lặp đi lặp lại:\nLuồng Gradient Qua T Bước:\n∂Loss/∂h₀ = ∂Loss/∂hₜ × (∂hₜ/∂hₜ₋₁) × (∂hₜ₋₁/∂hₜ₋₂) × ... × (∂h₁/∂h₀) Nếu mỗi ∂hᵢ/∂hᵢ₋₁ \u0026lt; 1 (điều này thường xảy ra):\nSau T phép nhân: gradient ≈ 0.5^100 ≈ 0 (với T=100) Gradient biến mất thành không Mô hình không thể học các phụ thuộc dài hạn Ví Dụ Cụ Thể Câu: \u0026ldquo;The students who studied hard\u0026hellip; passed the exam\u0026rdquo;\nTừ đầu \u0026ldquo;students\u0026rdquo; cần ảnh hưởng đến dự đoán \u0026ldquo;passed\u0026rdquo; Nhưng gradient đã biến mất khi tiếp cận \u0026ldquo;students\u0026rdquo; Mô hình không học được mối quan hệ này! Giải Pháp Hiện Tại LSTMs và GRUs giúp một chút với gates, nhưng:\nVẫn có vấn đề với chuỗi rất dài (\u0026gt;100-200 từ) Không thể hoàn toàn giải quyết vấn đề Vẫn yêu cầu xử lý tuần tự Vấn Đề 3: Thắt Cổ Chai Thông Tin Vấn Đề Nén Dữ Liệu Kiến Trúc Sequence-to-Sequence:\nEncoder: Word₁ → h₁ → h₂ → h₃ → hₜ (hidden state cuối cùng)\rDecoder: hₜ → Word₁\u0026#39; → Word₂\u0026#39; → Word₃\u0026#39; → ... Thắt Cổ Chai: Tất cả thông tin từ toàn bộ chuỗi đầu vào được nén vào một vector duy nhất hₜ (hidden state cuối cùng).\nTại Sao Điều Này Không Hoạt Động Ví Dụ Câu: \u0026ldquo;The government of the United States of America announced\u0026hellip;\u0026rdquo;\nKhi mã hóa câu 8 từ này:\nTừ đầu tiên \u0026ldquo;The\u0026rdquo; được xử lý Thông tin chảy qua các state: h₁ → h₂ → h₃ → \u0026hellip; → h₈ Bởi h₈, thông tin về \u0026ldquo;The\u0026rdquo; đã bị pha loãng/mất Chỉ h₈ được truyền đến decoder Decoder có thông tin ngữ cảnh hạn chế về các từ đầu Hậu Quả Chuỗi dài mất thông tin Ngữ cảnh quan trọng ở đầu bị pha loãng Mô hình gặp khó khăn với tài liệu dài Chất lượng dịch giảm cho các câu dài Tóm Tắt: Tại Sao RNNs Có Những Vấn Đề Cơ Bản Vấn Đề Tác Động Giải Pháp Hiện Tại Xử Lý Tuần Tự Không thể song song hóa, đào tạo chậm N/A - Cơ bản của thiết kế RNN Vanishing Gradients Không thể học phụ thuộc dài hạn LSTM/GRU gates (sửa chữa một phần) Thắt Cổ Chai Thông Tin Thông tin sớm bị mất Cơ chế Attention (sửa chữa một phần) Giải Pháp Transformer: \u0026ldquo;Attention is All You Need\u0026rdquo; Giới thiệu năm 2017 bởi các nhà nghiên cứu Google (Vaswani et al.), transformers hoàn toàn thay thế RNNs bằng các cơ chế attention.\nNhững Khác Biệt Chính Khía Cạnh RNNs Transformers Xử Lý Tuần tự (từng từ một lần) Song Song (tất cả từ cùng lúc) Phụ Thuộc Mỗi từ phụ thuộc vào state trước Tất cả từ attend tới tất cả từ Tốc Độ Huấn Luyện Chậm (tuần tự) Nhanh (song song) Chuỗi Dài Vanishing gradients Không thắt cổ chai tuần tự Phụ Thuộc Dài Hạn Khó khăn Dễ dàng (attention trực tiếp) Cách Transformers Hoạt Động (Tóm Tắt Ngắn) Không RNN: Loại bỏ hoàn toàn các hidden state tuần tự Attention Thuần: Để mỗi từ \u0026ldquo;attend tới\u0026rdquo; tất cả các từ khác Positional Encoding: Thêm thông tin vị trí vì chúng ta không có thứ tự tuần tự Xử Lý Song Song: Xử lý toàn bộ chuỗi cùng lúc Ví Dụ:\nCâu: \u0026#34;I am happy\u0026#34;\rThay vì:\rBước 1: Xử lý \u0026#34;I\u0026#34; → h₁\rBước 2: Xử lý \u0026#34;am\u0026#34; với h₁ → h₂\rBước 3: Xử lý \u0026#34;happy\u0026#34; với h₂ → h₃\rTransformer Làm:\rSong Song: \u0026#34;I\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rSong Song: \u0026#34;am\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rSong Song: \u0026#34;happy\u0026#34; attend tới {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rTất cả cùng lúc! ⚡ Tại Sao Mọi Người Nói Về Transformers ✅ Tốc Độ: Có thể huấn luyện nhanh hơn trên GPU/TPU (song parallel) ✅ Khả Năng Mở Rộng: Có thể xử lý chuỗi rất dài (không thắt cổ chai) ✅ Dài Hạn: Attention trực tiếp giải quyết các vấn đề gradient ✅ Tính Linh Hoạt: Hoạt động cho dịch, phân loại, QA, tóm tắt, chatbots\u0026hellip; ✅ Hiệu Năng: Đạt kết quả tiên tiến trên gần như mọi tác vụ NLP\nỨng Dụng của Transformers Transformers được sử dụng cho:\nDịch (Neural Machine Translation) - Chất lượng cao, nhanh Tóm Tắt Văn Bản (Abstractive \u0026amp; Extractive) Named Entity Recognition (NER) - Nhận dạng thực thể tốt hơn Hỏi Đáp - Hiểu ngữ cảnh tốt hơn Chatbots \u0026amp; Trợ Lý Thoại Phân Tích Cảm Xúc - Hiểu cảm xúc/ý kiến Tự Động Hoàn Thành - Đề xuất thông minh Phân Loại - Phân loại văn bản vào các danh mục Trí Tuệ Thị Trường - Phân tích cảm xúc thị trường Các Mô Hình Transformer Tiên Tiến Nhất GPT-2 (Generative Pre-trained Transformer) Tạo bởi: OpenAI Loại: Decoder-only transformer Chuyên môn: Tạo văn bản Nổi tiếng vì: Tạo văn bản giống con người (thậm chí đánh lừa các nhà báo năm 2019!) BERT (Bidirectional Encoder Representations from Transformers) Tạo bởi: Google AI Loại: Encoder-only transformer Chuyên môn: Hiểu văn bản \u0026amp; đại diện Sử dụng: Phân loại, NER, QA T5 (Text-to-Text Transfer Transformer) Tạo bởi: Google Loại: Full encoder-decoder (giống transformer gốc) Chuyên môn: Học đa tác vụ Rất linh hoạt: Một mô hình xử lý dịch, phân loại, QA, tóm tắt, hồi quy Bước Tiếp Theo Bây giờ chúng ta hiểu tại sao transformers là cách mạng, chúng ta sẽ học:\nKiến trúc transformer hoàn chỉnh Cách các cơ chế attention hoạt động về mặt toán học Cách triển khai một transformer decoder Cách sử dụng các mô hình được huấn luyện trước (BERT, GPT-2, T5) Thông Tin Chính: Transformer thay thế RNNs vì nó giải quyết ba vấn đề cơ bản thông qua attention thuần và xử lý song song. Đây là một sự thay đổi mô hình trong NLP.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.1-day46-2025-11-10/",
	"title": "Ngày 46 - Cơ Bản Transfer Learning",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-10 (Thứ Hai)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTransfer Learning: Tại Sao Quan Trọng Transfer learning là kỹ thuật QUAN TRỌNG NHẤT trong NLP hiện đại. Nó là lý do tại sao một mô hình được huấn luyện trên Wikipedia có thể được fine-tune trong vài ngày thay vì vài tháng.\nPhương Pháp Cổ Điển vs Transfer Learning Học Máy Cổ Điển Tác Vụ: Dự đoán xếp hạng bộ phim từ đánh giá\rBước 1: Thu thập dữ liệu có gắn nhãn (bộ phim + xếp hạng)\rBước 2: Khởi tạo mô hình ngẫu nhiên\rBước 3: Huấn luyện từ đầu trong vài tuần/tháng\rBước 4: Triển khai\rVấn Đề: Chậm và yêu cầu rất nhiều dữ liệu có gắn nhãn Phương Pháp Transfer Learning Tác Vụ: Dự đoán xếp hạng bộ phim từ đánh giá\rBước 1: Sử dụng mô hình được huấn luyện trước (huấn luyện trên 800GB văn bản)\rBước 2: Fine-tune trên tập dữ liệu nhỏ của bạn (100 ví dụ)\rBước 3: Huấn luyện trong vài giờ/ngày\rBước 4: Triển khai với hiệu suất tốt hơn!\rLợi Ích: Nhanh và hoạt động với dữ liệu nhỏ Hai Hình Thức Transfer Learning 1. Transfer Learning Dựa Trên Tính Năng Khái Niệm: Sử dụng các embeddings được huấn luyện trước làm các tính năng cố định\nVí Dụ:\nTác Vụ Pre-training: Học word2vec embeddings từ Wikipedia\rBước 1: Huấn luyện Word2Vec trên Wikipedia\r\u0026#34;The cat sat on the mat\u0026#34;\rHọc: embedding[\u0026#34;cat\u0026#34;] = [0.2, -0.5, 0.8, ...]\rBước 2: Sử dụng các embeddings này cho một tác vụ khác\rTác Vụ: Phân tích cảm xúc trên các đánh giá sản phẩm\rĐánh giá: \u0026#34;Great product!\u0026#34;\rNhúng: \u0026#34;Great\u0026#34; → [0.1, 0.3, 0.9, ...]\r\u0026#34;product\u0026#34; → [0.5, -0.2, 0.1, ...]\rCấp embeddings cho bộ phân loại đơn giản Ưu Điểm:\nĐơn giản để triển khai Suy luận nhanh (embeddings được tính toán trước) Hoạt động với các mô hình hạ nguồn nhỏ Nhược Điểm:\nEmbeddings cố định không thích nghi với miền mới Bối cảnh không được nắm bắt tốt (\u0026ldquo;bank\u0026rdquo; có cùng embedding ở mọi nơi) 2. Fine-tuning Transfer Learning Khái Niệm: Sử dụng trọng số mô hình được huấn luyện trước và cập nhật chúng cho tác vụ mới\nVí Dụ:\nTác Vụ Pre-training: Dự đoán từ tiếp theo trên Wikipedia (Language Modeling)\rBước 1: Huấn luyện transformer trên 800GB dữ liệu Wikipedia\rMô hình học: các mô hình ngôn ngữ, kiến thức thế giới, ngữ pháp\rBước 2: Fine-tune trên tác vụ hạ nguồn\rĐánh giá bộ phim → Dự đoán Xếp hạng\rTùy chọn A: Cập nhật tất cả các lớp\r├─ Lớp cuối: Fine-tune (hầu hết là chuyên ụng cho tác vụ)\r├─ Lớp giữa: Fine-tune (một số chuyên ụng cho tác vụ)\r└─ Lớp đầu: Fine-tune (ngôn ngữ chung)\rTùy chọn B: Đóng băng một số lớp, huấn luyện những lớp mới\r├─ Lớp 1-10: FROZEN (giữ trọng số được huấn luyện trước)\r├─ Lớp 11-12: Fine-tune\r└─ Đầu phân loại mới: Huấn luyện từ đầu Ưu Điểm:\nTrọng số thích nghi với tác vụ mới Hiệu suất tốt hơn so với dựa trên tính năng Hội tụ nhanh Nhược Điểm:\nChi phí tính toán (cần cập nhật tất cả các tham số) Rủi ro quên thảm họa Dữ Liệu Pre-training vs Tác Vụ Hạ Nguồn Sự Khác Biệt Về Quy Mô Pre-training: 800 GB của văn bản\r├─ Wikipedia: 13 GB\r├─ Sách: 200 GB\r├─ Web crawl: 500+ GB\r└─ Tổng kiến thức được học: Khổng lồ\rDownstream: 100 - 10,000 ví dụ\r├─ Đánh giá bộ phim: 5,000 ví dụ\r├─ Văn bản y tế: 1,000 ví dụ\r└─ Phản hồi khách hàng: 500 ví dụ Hiểu Biết Chính: Pre-training có 80,000x nhiều dữ liệu hơn các tác vụ hạ nguồn điển hình! Điều này giải thích tại sao transfer learning hoạt động rất tốt.\nMục Tiêu Pre-training Mục Tiêu 1: Language Modeling Tác Vụ: Dự đoán từ tiếp theo dựa trên các từ trước đó\nĐầu Vào: \u0026#34;The quick brown\u0026#34;\rĐầu Ra: \u0026#34;fox\u0026#34;\rLoss: Cross-entropy giữa dự đoán và từ tiếp theo thực tế\rVí Dụ:\r\u0026#34;Learning from deeplearning.ai is like watching the _____\u0026#34;\rMô hình dự đoán: \u0026#34;sunset\u0026#34;\rMục Tiêu: \u0026#34;sunset\u0026#34;\rLoss = 0 (dự đoán hoàn hảo!) Tại Sao Nó Hoạt Động:\nMô hình học ngữ pháp, cú pháp, ngữ nghĩa Mô hình học các mô hình phổ biến Mô hình học kiến thức thế giới Mục Tiêu 2: Masked Language Modeling Tác Vụ: Dự đoán các từ được che (ẩn)\nĐầu Vào: \u0026#34;The quick [MASK] fox\u0026#34;\rĐầu Ra: \u0026#34;brown\u0026#34;\rĐây là những gì BERT sử dụng!\rPhức Tạp Hơn:\r\u0026#34;The [MASK] brown [MASK] fox [MASK]\u0026#34;\rDự đoán cả ba: \u0026#34;quick\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;here\u0026#34; Lợi Ích của Transfer Learning 1. Giảm Thời Gian Huấn Luyện Huấn Luyện Cổ Điển: Transfer Learning:\r┌─────────────────────┐ ┌──────────┐\r│ Pre-training: 3 mo │ │Pre-train │ (đã làm rồi!)\r│ (Từ đầu!) │ │ (tái sử dụng) │\r│ │ └──────────┘\r│ Fine-tuning: 1 mo │ ┌──────────┐\r│ (Trên dữ liệu tác vụ) │ │Fine-tune │ 1-7 ngày!\r│ │ │(trên tác vụ) │\r│ Tổng cộng: 4 THÁNG │ └──────────┘\r└─────────────────────┘ Tổng cộng: ~1 TUẦN Tăng Tốc Độ: 15-30x nhanh hơn!\n2. Cải Thiện Dự Đoán Tập dữ liệu nhỏ (100 ví dụ) với phương pháp cổ điển:\r├─ Mô hình overfits (ghi nhớ 100 ví dụ)\r├─ Độ chính xác 60% trên tập test\r└─ Về cơ bản vô dụng\rTập dữ liệu nhỏ (100 ví dụ) với transfer learning:\r├─ Bắt đầu từ mô hình được huấn luyện trước (biết ngôn ngữ!)\r├─ Fine-tune cẩn thận\r├─ Độ chính xác 85% trên tập test\r└─ Tốt hơn nhiều!\rCải Thiện: 25% tốt hơn! 3. Sử Dụng Các Tập Dữ Liệu Nhỏ Hơn Cổ Điển: \u0026#34;Bạn cần 10,000 ví dụ có gắn nhãn\u0026#34;\rTransfer: \u0026#34;100-1000 ví dụ có gắn nhãn là đủ!\u0026#34;\rTại Sao?\rMô hình được huấn luyện trước đã biết ngôn ngữ\rBạn chỉ cần dạy nó tác vụ cụ thể của bạn\rCần ít dữ liệu hơn Dữ Liệu Pre-training vs Fine-tuning Dữ Liệu Có Gắn Nhãn vs Không Gắn Nhãn Dữ Liệu Không Gắn Nhãn (dùng trong pre-training):\r\u0026#34;The quick brown fox jumps over...\u0026#34;\r(Không cần nhãn! Chỉ là văn bản thô)\rDữ Liệu Có Gắn Nhãn (dùng trong fine-tuning):\r\u0026#34;The quick brown fox jumps over...\u0026#34; → Positive (nhãn)\r(Yêu cầu chú thích thủ công)\rHiểu Biết: Dữ liệu không gắn nhãn \u0026gt;\u0026gt; Dữ liệu có gắn nhãn về số lượng\rPre-training có thể sử dụng hàng tỷ tokens! Chiến Lược Transfer Learning Chiến Lược 1: Fine-tuning Nhẹ Đóng băng hầu hết các lớp, huấn luyện chỉ lớp cuối cùng\r├─ Trường hợp sử dụng: Mô hình được huấn luyện trước lớn + tập dữ liệu nhỏ\r├─ Lớp đóng băng: 1-11\r├─ Lớp huấn luyện: 12 + phần đầu phân loại\r├─ Thời gian huấn luyện: 1-2 ngày\r└─ Rủi ro underfitting: Thấp Chiến Lược 2: Fine-tuning Đầy Đủ Cập nhật tất cả các lớp\r├─ Trường hợp sử dụng: Mô hình được huấn luyện trước trung bình + tập dữ liệu trung bình\r├─ Lớp đóng băng: Không có\r├─ Lớp huấn luyện: Tất cả 12 + phần đầu phân loại\r├─ Thời gian huấn luyện: 3-7 ngày\r└─ Rủi ro overfitting: Trung bình Chiến Lược 3: Progressive Unfreezing Mở khóa các lớp dần dần\r├─ Ngày 1: Mở khóa lớp cuối cùng, huấn luyện\r├─ Ngày 2: Mở khóa 2 lớp cuối cùng, huấn luyện\r├─ Ngày 3: Mở khóa 3 lớp cuối cùng, huấn luyện\r├─ ...\r├─ Thời gian huấn luyện: 7+ ngày\r└─ Hiệu suất tốt nhất: Thường xuyên! Khi Transfer Learning Thất Bại ❌ Không khớp miền: Pre-training trên tiếng Anh, fine-tune trên tiếng Trung ❌ Quên thảm họa: Cập nhật tất cả trọng số quá tích cực ❌ Fine-tuning quá nhiều: Sử dụng tỷ lệ học quá cao ❌ Trích xuất tính năng kém: Tác vụ pre-train quá khác biệt so với downstream\nDòng Thời Gian Lịch Sử 2013: Word2Vec\r└─ Transfer learning thành công đầu tiên\r└─ Embeddings từ đơn giản\r2015: ELMo\r└─ Bidirectional LSTM\r└─ Embeddings nhận thức bối cảnh\r2018: GPT\r└─ Mô hình ngôn ngữ dựa trên Transformer\r└─ Đơn chiều (trái sang phải)\r2018: BERT\r└─ Transformers lưỡng chiều!\r└─ Masked language modeling\r└─ Cải thiện hiệu suất khổng lồ\r2019: T5\r└─ Text-to-text transfer transformer\r└─ Multi-task learning\r└─ Tiên tiến nhất ở mọi nơi!\r2020+: GPT-2, GPT-3, RoBERTa, ELECTRA, XLNet...\r└─ Khám phá định luật mở rộng\r└─ Mô hình lớn hơn → Hiệu suất tốt hơn Các Lợi Ích Chính ✅ Transfer learning là tiêu chuẩn: Hầu hết NLP ngày nay sử dụng nó ✅ Pre-training rất quan trọng: 800GB pre-training \u0026raquo; 10KB fine-tuning ✅ Lợi thế tốc độ rất lớn: Ngày thay vì tháng ✅ Hiệu quả dữ liệu: Hoạt động với ít dữ liệu có gắn nhãn hơn nhiều ✅ Thích ứng miền: Có thể chuyển giao trên các ngôn ngữ, miền, tác vụ\nNhững Gì Chúng Ta Sẽ Học Tiếp BERT: Cách bối cảnh lưỡng chiều cải thiện sự hiểu biết MLM: Masked language modeling pre-training T5: Khung text-to-text cho tất cả các tác vụ NLP Fine-tuning: Các mẹo thực tế cho các tác vụ hạ nguồn Nền tảng này (transfer learning) là lý do tại sao một người bây giờ có thể xây dựng các hệ thống NLP tiên tiến trong vài tuần!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/4.1-event1/",
	"title": "Sự Kiện 1 - Vietnam Cloud Day 2025",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Ngày \u0026amp; Giờ: Thứ Năm, 18 tháng 9 năm 2025 | 9:00 – 17:00 VNT\nĐịa Điểm: Amazon Web Services Vietnam, Tầng 36, 2 Đường Hai Triều, Phường Sài Gòn, Thành phố Hồ Chí Minh\nTrạng Thái Đăng Ký: Đã Đóng\nTổng Quan Sự Kiện Vietnam Cloud Day 2025 là một sự kiện AWS toàn diện được thiết kế cho các nhà xây dựng và lãnh đạo doanh nghiệp, với các bài phát biểu chính từ các nhà lãnh đạo chính phủ, các giám đốc điều hành AWS và các lãnh đạo ngành. Sự kiện này giới thiệu các dịch vụ mới nhất của AWS và các sáng kiến chiến lược cho AI và hiện đại hóa đám mây thông qua hai track chính: track phát sóng trực tiếp và các phiên breakout trực tiếp.\nChương Trình Track Phát Sóng Trực Tiếp Giờ (VNT) Phiên Diễn Giả 7:35 - 9:00 Đăng Ký - 9:00 - 9:20 Khai Mạc Nhà Lãnh Đạo Chính Phủ 9:20 - 9:40 Bài Phát Biểu Chính Eric Yeo, Giám Đốc Quốc Gia, Việt Nam, Campuchia, Lào \u0026amp; Myanmar, AWS 9:40 - 10:00 Bài Phát Biểu Khách Hàng 1 Tiến Sĩ Jens Lottner, CEO, Techcombank 10:00 - 10:20 Bài Phát Biểu Khách Hàng 2 Bà Trang Phùng, CEO \u0026amp; Đồng Sáng Lập, U2U Network 10:20 - 10:50 Bài Phát Biểu AWS Jaime Valles, Phó Chủ Tịch, Giám Đốc Tổng Quát Khu Vực Châu Á Thái Bình Dương và Nhật Bản, AWS 11:00 – 11:40 Thảo Luận Bảng: Điều Hướng Cuộc Cách Mạng GenAI Điều Phối Viên: Jeff Johnson, Giám Đốc Quản Lý, ASEAN, AWS Chi Tiết Thảo Luận Bảng: Điều Hướng Cuộc Cách Mạng GenAI: Chiến Lược Lãnh Đạo Cấp Cao Thảo luận này đã đi sâu vào cách các nhà lãnh đạo cấp cao có thể điều hướng hiệu quả các tổ chức của họ thông qua những tiến bộ nhanh chóng trong AI tạo sinh. Các thành viên bảng điều hành đã chia sẻ những hiểu biết và hành trình cá nhân của họ về:\nXây dựng văn hóa đổi mới Căn chỉnh các sáng kiến AI với các mục tiêu kinh doanh Quản lý những thay đổi tổ chức kèm theo tích hợp AI Các Thành Viên Bảng:\nVũ Văn, Đồng Sáng Lập \u0026amp; CEO, ELSA Corp Nguyễn Hòa Bình, Chủ Tịch, Nexttech Group Dieter Botha, CEO, TymeX Các Track Breakout (Phiên Trực Tiếp) Track 1: Tập Trung vào AI \u0026amp; Phân Tích Giờ (VNT) Phiên Diễn Giả 13:15 - 13:30 Khai Mạc \u0026amp; Giới Thiệu Track Jun Kai Loke, Chuyên Gia AI/ML SA, AWS 13:30 - 14:00 Xây Dựng Nền Tảng Dữ Liệu Thống Nhất trên AWS cho Khối Lượng Công Việc AI và Phân Tích Kiên Nguyễn, Kiến Trúc Sư Giải Pháp, AWS 14:00 - 14:30 Xây Dựng Tương Lai: Áp Dụng Gen AI và Lộ Trình trên AWS Jun Kai Loke, Chuyên Gia AI/ML SA, AWS; Tamelly Lim, Chuyên Gia Lưu Trữ SA, AWS 14:30 - 15:00 Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) - Định Hình Tương Lai của Triển Khai Phần Mềm Bình Trần, Kiến Trúc Sư Giải Pháp Cao Cấp, AWS 15:00 - 15:30 Giải Lao Uống Trà - 15:30 - 16:00 Bảo Mật Các Ứng Dụng AI Tạo Sinh với AWS: Nguyên Tắc Cơ Bản và Thực Tiễn Tốt Nhất Taiki Dang, Kiến Trúc Sư Giải Pháp, AWS 16:00 - 16:30 Vượt Ra Ngoài Tự Động Hóa: Các Tác Nhân AI là Bộ Nhân Năng Suất Tối Ưu của Bạn Michael Armentano, Chuyên Gia GTM Toàn Cầu Chính, AWS Chi Tiết Phiên Xây Dựng Nền Tảng Dữ Liệu Thống Nhất trên AWS cho Khối Lượng Công Việc AI và Phân Tích\nPhiên này đã đi sâu vào các chiến lược và thực tiễn tốt nhất để xây dựng nền tảng dữ liệu thống nhất, có thể mở rộng trên AWS. Các tham gia viên đã học cách tận dụng các dịch vụ AWS để tạo cơ sở hạ tầng dữ liệu mạnh mẽ có thể xử lý các yêu cầu của các ứng dụng hướng dữ liệu hiện đại. Các chủ đề chính được đề cập:\nNhập dữ liệu, lưu trữ, xử lý và quản trị Quản lý và sử dụng dữ liệu hiệu quả cho phân tích nâng cao và các sáng kiến AI Xây Dựng Tương Lai: Áp Dụng Gen AI và Lộ Trình trên AWS\nAWS đã trình bày tầm nhìn toàn diện, các xu hướng mới nổi và lộ trình chiến lược cho việc áp dụng các công nghệ AI Tạo Sinh (GenAI). Cuộc thảo luận đã bao gồm các dịch vụ AWS chính và các sáng kiến được thiết kế để trao quyền cho các tổ chức trong việc tận dụng GenAI để thúc đẩy đổi mới và hiệu quả.\nVòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) - Định Hình Tương Lai của Triển Khai Phần Mềm\nVòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) là một cách tiếp cận biến đổi, tập trung vào AI, định hình lại tương lai của triển khai phần mềm bằng cách nhúng đầy đủ AI như một cộng tác viên trung tâm trong toàn bộ vòng đời phát triển phần mềm. Không giống như các phương pháp truyền thống nhúng AI như một trợ lý cho các quy trình do con người điều khiển hiện có, AI-DLC tích hợp thực thi do AI điều khiển với giám sát của con người và hợp tác nhóm động để:\nCải thiện đáng kể tốc độ phát triển phần mềm Nâng cao chất lượng mã Thúc đẩy đổi mới Bảo Mật Các Ứng Dụng AI Tạo Sinh với AWS: Nguyên Tắc Cơ Bản và Thực Tiễn Tốt Nhất\nPhiên này đã khám phá các thách thức bảo mật độc đáo ở mỗi lớp của ngăn xếp AI tạo sinh—cơ sở hạ tầng, mô hình và ứng dụng. Các tham gia viên đã học cách AWS tích hợp các biện pháp bảo mật tích hợp sẵn như:\nMã hóa Kiến trúc không tin tưởng Giám sát liên tục Kiểm soát truy cập chi tiết Các biện pháp này bảo vệ khối lượng công việc AI tạo sinh, đảm bảo tính bảo mật và toàn vẹn dữ liệu trong suốt vòng đời AI.\nVượt Ra Ngoài Tự Động Hóa: Các Tác Nhân AI là Bộ Nhân Năng Suất Tối Ưu của Bạn\nPhiên này đã trình bày một sự thay đổi mô hình trong đó các tác nhân AI không chỉ là công cụ, mà là những đối tác thông minh chủ động thúc đẩy kinh doanh phát triển. Các khái niệm chính bao gồm:\nCác tác nhân AI học hỏi, thích ứng và thực thi các tác vụ phức tạp một cách tự chủ Chuyển đổi hoạt động từ các quy trình thủ công sang hiệu quả chưa từng có Nhân năng suất theo cấp số nhân thông qua sức mạnh của AI Track 2: Tập Trung vào Di Chuyển Đám Mây \u0026amp; Hiện Đại Hóa Giờ (VNT) Phiên Diễn Giả 13:15 - 13:30 Khai Mạc \u0026amp; Giới Thiệu Track Hùng Nguyễn Gia, Trưởng Kiến Trúc Sư Giải Pháp, AWS 13:30 - 14:00 Hoàn Thành Di Chuyển và Hiện Đại Hóa Quy Mô Lớn với AWS Sơn Đỗ, Quản Lý Tài Khoản Kỹ Thuật, AWS; Nguyễn Văn Hải, Giám Đốc Kỹ Thuật Phần Mềm, Techcombank 14:00 - 14:30 Hiện Đại Hóa Ứng Dụng với Công Cụ Được Hỗ Trợ bởi AI Tạo Sinh Phúc Nguyễn, Kiến Trúc Sư Giải Pháp, AWS; Alex Trần, Giám Đốc AI, OCB 14:30 - 15:00 Thảo Luận Bảng: Hiện Đại Hóa Ứng Dụng - Tăng Tốc Độ Chuyển Đổi Kinh Doanh Điều Phối Viên: Hùng Nguyễn Gia, Trưởng Kiến Trúc Sư Giải Pháp, AWS 15:00 - 15:30 Giải Lao - 15:30 - 16:00 Chuyển Đổi VMware với Hiện Đại Hóa Đám Mây Do AI Điều Khiển Hùng Hoàng, Quản Lý Giải Pháp Khách Hàng, AWS 16:00 - 16:30 Bảo Mật AWS Ở Quy Mô: Từ Phát Triển đến Sản Xuất Taiki Dang, Kiến Trúc Sư Giải Pháp, AWS Chi Tiết Phiên Hoàn Thành Di Chuyển và Hiện Đại Hóa Quy Mô Lớn với AWS\nPhiên này tập trung vào những bài học quý báu từ hàng nghìn doanh nghiệp đã di chuyển và hiện đại hóa khối lượng công việc tại chỗ của họ với AWS. Các chủ đề bao gồm:\nCác mô hình tư duy được chứng minh và thực tiễn kỹ thuật tốt nhất Các con đường hiện đại hóa giúp các tổ chức hiện đại hóa trong khi di chuyển Các bộ tăng tốc di chuyển AWS và các công cụ di chuyển và hiện đại hóa mới nhất Nghiên cứu trường hợp cho thấy cách các tổ chức đã thiết lập nền tảng mạnh mẽ và lộ trình chiến lược tận dụng các khả năng đám mây AWS để đạt được các mục tiêu chuyển đổi kỹ thuật số Hiện Đại Hóa Ứng Dụng với Công Cụ Được Hỗ Trợ bởi AI Tạo Sinh\nPhiên này đã khám phá cách Amazon Q Developer biến đổi vòng đời phát triển phần mềm (SDLC) thông qua các khả năng tác nhân của nó trên:\nAWS Console IDE CLI Các nền tảng DevSecOps Các khả năng chính được trình diễn:\nCác tác nhân Q tăng tốc độ tạo mã và cải thiện chất lượng mã Tích hợp liền mạch với các quy trình công việc hiện có Tạo tự động tài liệu toàn diện và bài kiểm tra đơn vị Cải thiện khả năng bảo trì mã và độ tin cậy Hiểu các cơ sở mã phức tạp và đề xuất tối ưu hóa Tự động hóa các tác vụ thường xuyên trong vòng đời phát triển Thảo Luận Bảng: Hiện Đại Hóa Ứng Dụng - Tăng Tốc Độ Chuyển Đổi Kinh Doanh\nCác Thành Viên Bảng:\nNguyễn Minh Ngân, Chuyên Gia AI, OCB Nguyễn Mạnh Tuyền, Trưởng Ứng Dụng Dữ Liệu, LPBank Securities Vinh Nguyễn, Đồng Sáng Lập \u0026amp; CTO, Ninety Eight Chuyển Đổi VMware với Hiện Đại Hóa Đám Mây Do AI Điều Khiển\nPhiên này cho thấy cách các tổ chức Việt Nam đang tăng tốc độ áp dụng đám mây với các tài sản VMware. Các chủ đề chính:\nCách AWS Transform giúp di chuyển nhanh, an toàn và hiệu quả về chi phí Sách hướng dẫn từng bước và các mô hình nhận thức về thời gian chết Lộ trình để hiện đại hóa lên EKS, RDS và serverless sau khi hạ cánh Lý tưởng cho các nhà lãnh đạo CNTT, kiến trúc sư và các nhóm vận hành lên kế hoạch di chuyển VMware-to-AWS quy mô lớn Bảo Mật AWS Ở Quy Mô: Từ Phát Triển đến Sản Xuất\nPhiên này đã khám phá cách nâng cao tư thế bảo mật đám mây trên toàn bộ vòng đời phát triển và sản xuất. Các chủ đề được đề cập:\nCách tiếp cận bảo mật toàn diện của AWS: xác định, phòng ngừa, phát hiện, phản ứng và khắc phục Các nguyên tắc bảo mật theo thiết kế trong suốt quá trình phát triển Các khả năng phát hiện và phản ứng nâng cao Cách AI tạo sinh nâng cao phân tích bảo mật và tự động hóa hoạt động Xây dựng các kiến trúc có khả năng phục hồi phát triển theo các mối đe dọa mới nổi Tạo các môi trường đám mây an toàn hơn, có thể mở rộng Những Điểm Chính Rút Ra Hiểu biết toàn diện về chiến lược AI và hiện đại hóa đám mây của AWS Những hiểu biết thực tế về áp dụng AI ở quy mô doanh nghiệp và triển khai Thực tiễn tốt nhất cho nền tảng dữ liệu, bảo mật và hiện đại hóa ứng dụng Các nghiên cứu trường hợp thực tế và bài học từ các lãnh đạo ngành Kiến thức thực hành về các dịch vụ AWS cho GenAI, di chuyển và hiện đại hóa "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Tạo một Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": " Mở Amazon VPC console Trong thanh điều hướng, chọn Endpoints, click Create Endpoint: Bạn sẽ thấy 6 điểm cuối VPC hiện có hỗ trợ AWS Systems Manager (SSM). Các điểm cuối này được Mẫu CloudFormation triển khai tự động cho workshop này.\nTrong Create endpoint console: Đặt tên cho endpoint: s3-gwe Trong service category, chọn aws services Trong Services, gõ \u0026ldquo;s3\u0026rdquo; trong hộp tìm kiếm và chọn dịch vụ với loại gateway Đối với VPC, chọn VPC Cloud từ drop-down menu. Đối với Route tables, chọn bảng định tuyến mà đã liên kết với 2 subnets (lưu ý: đây không phải là bảng định tuyến chính cho VPC mà là bảng định tuyến thứ hai do CloudFormation tạo). Đối với Policy, để tùy chọn mặc định là Full access để cho phép toàn quyền truy cập vào dịch vụ. Bạn sẽ triển khai VPC endpoint policy trong phần sau để chứng minh việc hạn chế quyền truy cập vào S3 bucket dựa trên các policies. Không thêm tag vào VPC endpoint. Click Create endpoint, click x sau khi nhận được thông báo tạo thành công. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.1-blog1/",
	"title": "Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code",
	"tags": [],
	"description": "",
	"content": "Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code bởi Lauren Mullennex, Anagha Barve, Anchit Gupta, và Bhargava Varadharajan vào ngày 12 THÁNG 9 2025 trong Amazon SageMaker AI, Amazon SageMaker Unified Studio, Announcements, Intermediate (200), Technical How-to\nCác nhà phát triển và kỹ sư học máy (ML) giờ đây có thể kết nối trực tiếp tới Amazon SageMaker Unified Studio từ trình soạn thảo Visual Studio Code (VS Code) cục bộ của họ. Với khả năng này, bạn có thể giữ nguyên quy trình phát triển hiện có và cấu hình môi trường phát triển tích hợp (IDE) cá nhân hóa, đồng thời truy cập các dịch vụ phân tích AWS và trí tuệ nhân tạo \u0026amp; máy học (AI/ML) trong một môi trường phát triển dữ liệu và AI hợp nhất. Sự tích hợp này cung cấp truy cập liền mạch từ môi trường phát triển cục bộ của bạn đến cơ sở hạ tầng có thể mở rộng để chạy xử lý dữ liệu, phân tích SQL và các luồng công việc ML. Bằng cách kết nối IDE cục bộ của bạn với SageMaker Unified Studio, bạn có thể tối ưu hóa luồng phát triển dữ liệu và AI mà không làm gián đoạn các thực tiễn phát triển đã thiết lập.\nTrong bài viết này, chúng tôi minh họa cách kết nối VS Code cục bộ của bạn đến SageMaker Unified Studio để bạn có thể xây dựng luồng công việc dữ liệu và AI đầu-cuối trong khi làm việc trong môi trường phát triển ưa thích của bạn.\nTổng quan giải pháp Kiến trúc giải pháp bao gồm ba thành phần chính:\nMáy tính cục bộ – Máy phát triển của bạn chạy VS Code với AWS Toolkit cho Visual Studio Code và Microsoft Remote SSH được cài đặt. Bạn có thể kết nối thông qua extension Toolkit cho VS Code bằng cách duyệt các không gian (spaces) SageMaker Unified Studio có sẵn và chọn môi trường mục tiêu của chúng.\nSageMaker Unified Studio – Là phần của thế hệ kế tiếp của Amazon SageMaker, SageMaker Unified Studio là một môi trường phát triển dữ liệu và AI duy nhất, nơi bạn có thể tìm và truy cập dữ liệu của mình và thao tác nó bằng các công cụ AWS quen thuộc cho phân tích SQL, xử lý dữ liệu, phát triển mô hình và phát triển ứng dụng AI tạo sinh.\nAWS Systems Manager – Một dịch vụ truy cập từ xa và quản lý an toàn, có khả năng mở rộng, giúp kết nối liền mạch giữa VS Code cục bộ của bạn và các không gian SageMaker Unified Studio để đơn giản hóa luồng phát triển dữ liệu và AI.\nSơ đồ sau đây biểu diễn sự tương tác giữa IDE cục bộ của bạn và các không gian SageMaker Unified Studio.\nCác điều kiện tiên quyết Để thử kết nối IDE từ xa, bạn phải có các điều kiện sau:\nTruy cập vào domain SageMaker Unified Studio có kết nối Internet. Với các domain được cấu hình ở chế độ chỉ VPC, domain của bạn phải có tuyến ra Internet qua proxy hoặc NAT gateway. Nếu domain của bạn hoàn toàn cô lập khỏi Internet, tham khảo tài liệu để thiết lập kết nối từ xa. Nếu bạn chưa có domain SageMaker Unified Studio, bạn có thể tạo một domain bằng tùy chọn thiết lập nhanh (quick setup) hoặc thiết lập thủ công (manual setup).\nMột người dùng với thông tin đăng nhập SSO thông qua IAM Identity Center được yêu cầu. Để cấu hình truy cập người dùng SSO, hãy xem tài liệu.\nTruy cập hoặc có thể tạo một dự án SageMaker Unified Studio.\nCompute Space JupyterLab hoặc Code Editor với yêu cầu loại instance tối thiểu 8 GB bộ nhớ. Trong bài viết này, chúng tôi dùng instance ml.t3.large. Phiên bản ảnh phân phối SageMaker Distribution image phiên bản 2.8 trở lên được hỗ trợ.\nBạn có VS Code bản ổn định mới nhất với Microsoft Remote SSH (phiên bản 0.74.0 trở lên) và extension AWS Toolkit (phiên bản 3.74.0) được cài đặt trên máy cục bộ của bạn.\nTriển khai giải pháp Để cho phép kết nối từ xa và kết nối tới không gian từ VS Code, hoàn tất các bước sau. Để kết nối tới một space SageMaker Unified Studio từ xa, space đó phải được bật tính năng truy cập từ xa.\nĐiều hướng tới space JupyterLab hoặc Code Editor của bạn. Nếu nó đang chạy, dừng space và chọn Configure space để bật truy cập từ xa\nBật Remote access để kích hoạt tính năng và chọn Save and restart.\nĐiều hướng tới AWS Toolkit trong cài đặt VS Code cục bộ của bạn.\nTrên tab SageMaker Unified Studio, chọn Sign in để bắt đầu và cung cấp URL domain SageMaker Unified Studio, ví dụ https://\u0026lt;domain‑id\u0026gt;.sagemaker.\u0026lt;region\u0026gt;.on.aws.\nBạn sẽ được yêu cầu chuyển hướng sang trình duyệt web để cho phép truy cập các extension IDE AWS. Chọn Open để mở tab trình duyệt mới.\nChọn Allow access để kết nối tới dự án qua VS Code.\nBạn sẽ nhận được thông báo Request approved, cho thấy bạn đã có quyền truy cập domain từ xa.\nQuay lại VS Code cục bộ của bạn để truy cập dự án và tiếp tục xây dựng các công việc ETL, pipeline dữ liệu, đào tạo \u0026amp; triển khai mô hình ML hoặc xây ứng dụng AI tạo sinh. Để kết nối tới dự án cho xử lý dữ liệu và phát triển ML, thực hiện các bước:\nChọn Select a project để xem dữ liệu và tài nguyên tính toán. Tất cả các dự án trong domain được liệt kê, nhưng bạn chỉ được phép truy cập các dự án mà bạn là thành viên.\nBạn chỉ có thể xem một domain và một dự án tại một thời điểm. Để chuyển dự án hoặc đăng xuất khỏi domain, chọn biểu tượng dấu ba chấm.Bạn cũng có thể xem tài nguyên dữ liệu và tính toán mà bạn đã tạo trước đó.\nKết nối space JupyterLab hoặc Code Editor bằng cách chọn biểu tượng kết nối. Nếu tùy chọn này không hiển thị, có thể bạn đã tắt truy cập từ xa trong space. Nếu space đang ở trạng thái “Stopped”, di chuột lên space và chọn nút connect, điều này sẽ bật truy cập từ xa, khởi động space và kết nối nó. Nếu space đang ở trạng thái “Running”, space phải được khởi động lại với truy cập từ xa được bật bằng cách dừng space rồi kết nối lại từ toolkit.\nMột cửa sổ VS Code khác sẽ mở ra và được kết nối tới space SageMaker Unified Studio của bạn qua remote SSH.\nĐiều hướng tới Explorer để xem notebook, file và script của space. Từ AWS Toolkit, bạn cũng có thể xem nguồn dữ liệu của bạn.\nSử dụng thiết lập VS Code tùy chỉnh với tài nguyên SageMaker Unified Studio Khi bạn kết nối VS Code với SageMaker Unified Studio, bạn giữ nguyên tất cả phím tắt cá nhân và tùy chỉnh của bạn. Ví dụ, nếu bạn dùng đoạn code snippet để nhanh chóng chèn các mẫu mã phân tích và ML phổ biến, chúng vẫn hoạt động với cơ sở hạ tầng được quản lý của SageMaker Unified Studio.\nTrong hình minh họa, chúng tôi thể hiện cách sử dụng các snippet luồng phân tích: snippet “show-databases” truy vấn Athena để hiển thị các database có sẵn, “show-glue-tables” liệt kê bảng trong AWS Glue Data Catalog, và “query-ecommerce” lấy dữ liệu sử dụng Spark SQL để phân tích.\nBạn cũng có thể dùng các snippet để tự động hóa việc build và training mô hình ML trên SageMaker AI. Trong hình bên dưới, các snippet mã thể hiện xử lý dữ liệu, cấu hình và khởi chạy job đào tạo SageMaker AI. Cách tiếp cận này cho thấy người làm dữ liệu có thể giữ thiết lập phát triển quen thuộc của mình trong khi sử dụng tài nguyên dữ liệu và AI được quản lý trong SageMaker Unified Studio.\nVô hiệu hóa truy cập từ xa trong SageMaker Unified Studio Như một quản trị viên, nếu bạn muốn vô hiệu hóa tính năng này cho người dùng, bạn có thể thực thi nó bằng cách thêm chính sách sau vào vai trò IAM của dự án:\n{\n\u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;,\n\u0026ldquo;Statement\u0026rdquo;: [\n{\n\u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;DenyStartSessionForSpaces\u0026rdquo;,\n\u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Deny\u0026rdquo;,\n\u0026ldquo;Action\u0026rdquo;: [\n\u0026ldquo;sagemaker:StartSession\u0026rdquo;\n],\n\u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:sagemaker:*:*:space/*/*\u0026rdquo;\n}\n]\n}\nDọn dẹp Theo mặc định, SageMaker Unified Studio tắt các tài nguyên nhàn rỗi như các space JupyterLab và Code Editor sau 1 giờ. Nếu bạn đã tạo một domain SageMaker Unified Studio cho mục đích bài viết này, nhớ xóa domain đó.\nKết luận Kết nối trực tiếp từ IDE cục bộ của bạn đến Amazon SageMaker Unified Studio giảm ma sát khi chuyển giữa phát triển cục bộ và hạ tầng dữ liệu \u0026amp; AI có thể mở rộng. Bằng cách giữ cấu hình IDE cá nhân hóa, điều này giảm sự cần thiết phải thích nghi giữa các môi trường phát triển khác nhau. Dù bạn đang xử lý các tập dữ liệu lớn, đào tạo mô hình nền tảng (foundation models, FMs), hoặc xây dựng ứng dụng AI tạo sinh, bạn giờ có thể làm việc từ thiết lập cục bộ của mình trong khi truy cập các khả năng của SageMaker Unified Studio. Bắt đầu ngay hôm nay bằng cách kết nối IDE cục bộ của bạn đến SageMaker Unified Studio để hợp lý hóa luồng xử lý dữ liệu và tăng tốc phát triển mô hình ML.\nGiới thiệu về các tác giả Lauren Mullennex\rLauren là Kiến trúc sư Giải pháp Chuyên gia GenAI/ML Cấp cao tại AWS, có hơn một thập kỷ kinh nghiệm về ML, DevOps và hạ tầng. Cô là tác giả sách về thị giác máy tính và yêu thích du lịch, leo núi cùng hai chú chó.\rBhargava Varadharajan\rBhargava là Kỹ sư Phần mềm Cấp cao tại AWS, phát triển các sản phẩm như SageMaker Studio, Studio Lab và Unified Studio. Anh đã dành hơn 5 năm để biến các quy trình AI/ML phức tạp thành trải nghiệm liền mạch, đồng thời theo đuổi mục tiêu khám phá 63 công viên quốc gia Hoa Kỳ, leo núi, đá bóng, trượt tuyết, làm DIY và đọc sách.\rAnagha Barve\rAnagha là Quản lý Phát triển Phần mềm của nhóm Amazon SageMaker Unified Studio.\rAnchit Gupta\rAnchit là Quản lý Sản phẩm Cấp cao cho Amazon SageMaker Unified Studio. Cô giúp các nhóm xây dựng giải pháp ML dễ dàng hơn và yêu thích nấu ăn, chơi board/card games, đọc sách khi rảnh rỗi.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/",
	"title": "Tuần 1 - Kiến thức Nền tảng Cloud Computing",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-08 đến 2025-09-12\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 1 Tuần này tập trung củng cố những khái niệm cơ bản về Cloud Computing, hạ tầng AWS và các công cụ quản trị.\nNội dung chính Giới thiệu Cloud Computing và lợi ích. AWS Global Infrastructure (Region, AZ, Edge Location). Bộ công cụ quản lý AWS (Console, CLI, SDK). Chiến lược tối ưu chi phí. AWS Well-Architected Framework. Labs thực hành Lab 01: Thiết lập tài khoản AWS \u0026amp; IAM. Lab 07: AWS Budgets \u0026amp; Cost Management. Lab 09: AWS Support Plans. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/",
	"title": "Worklog - Hành Trình Học AWS",
	"tags": [],
	"description": "",
	"content": "Worklog Tổng quan Đây là nhật ký học tập ghi lại hành trình học AWS bắt đầu từ ngày 8/9/2025.\nCấu trúc Worklog được tổ chức theo tuần, mỗi tuần gồm 5 ngày làm việc (Thứ Hai đến Thứ Sáu).\nNội dung chính Nền tảng Điện toán Đám mây\nKiến thức cơ bản về AWS, hạ tầng toàn cầu, công cụ quản lý Tối ưu chi phí, các gói hỗ trợ Bộ khung Well-Architected Framework Mạng (Networking)\nVPC, subnet, security group, NACL Cân bằng tải (ALB, NLB, GWLB) VPC Peering, Transit Gateway VPN, Direct Connect Tính toán (Compute)\nEC2, AMI, EBS, Instance Store Auto Scaling, mô hình định giá Lightsail, EFS, FSx Lưu trữ (Storage)\nS3, các lớp lưu trữ, Glacier Snow Family, Storage Gateway Khôi phục thảm họa, AWS Backup Bảo mật \u0026amp; Danh tính (Security \u0026amp; Identity)\nIAM, Cognito, Organizations KMS, Security Hub Identity Center (SSO) Cơ sở dữ liệu (Database)\nRDS, Aurora, Redshift ElastiCache, DMS Thực hành tốt trong quản lý cơ sở dữ liệu Chủ đề Nâng cao (Advanced Topics)\nServerless (Lambda) Containers (ECS, EKS, ECR) Giám sát hệ thống (CloudWatch, X-Ray, CloudTrail) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "IAM permissions Gắn IAM permission policy sau vào tài khoản aws user của bạn để triển khai và dọn dẹp tài nguyên trong workshop này.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Khởi tạo tài nguyên bằng CloudFormation Trong lab này, chúng ta sẽ dùng N.Virginia region (us-east-1).\nĐể chuẩn bị cho môi trường làm workshop, chúng ta deploy CloudFormation template sau (click link): PrivateLinkWorkshop . Để nguyên các lựa chọn mặc định.\nLựa chọn 2 mục acknowledgement Chọn Create stack Quá trình triển khai CloudFormation cần khoảng 15 phút để hoàn thành.\n2 VPCs đã được tạo 3 EC2s đã được tạo "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Kiểm tra Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Tạo S3 bucket Đi đến S3 management console Trong Bucket console, chọn Create bucket Trong Create bucket console Đặt tên bucket: chọn 1 tên mà không bị trùng trong phạm vi toàn cầu (gợi ý: lab\u0026lt;số-lab\u0026gt;\u0026lt;tên-bạn\u0026gt;) Giữ nguyên giá trị của các fields khác (default) Kéo chuột xuống và chọn Create bucket Tạo thành công S3 bucket Kết nối với EC2 bằng session manager Trong workshop này, bạn sẽ dùng AWS Session Manager để kết nối đến các EC2 instances. Session Manager là 1 tính năng trong dịch vụ Systems Manager được quản lý hoàn toàn bởi AWS. System manager cho phép bạn quản lý Amazon EC2 instances và các máy ảo on-premises (VMs)thông qua 1 browser-based shell. Session Manager cung cấp khả năng quản lý phiên bản an toàn và có thể kiểm tra mà không cần mở cổng vào, duy trì máy chủ bastion host hoặc quản lý khóa SSH.\nFirst cloud journey Lab để hiểu sâu hơn về Session manager.\nTrong AWS Management Console, gõ Systems Manager trong ô tìm kiếm và nhấn Enter: Từ Systems Manager menu, tìm Node Management ở thanh bên trái và chọn Session Manager: Click Start Session, và chọn EC2 instance tên Test-Gateway-Endpoint. Phiên bản EC2 này đã chạy trong \u0026ldquo;VPC cloud\u0026rdquo; và sẽ được dùng để kiểm tra khả năng kết nối với Amazon S3 thông qua điểm cuối Cổng mà bạn vừa tạo (s3-gwe).\nSession Manager sẽ mở browser tab mới với shell prompt: sh-4.2 $\nBạn đã bắt đầu phiên kết nối đến EC2 trong VPC Cloud thành công. Trong bước tiếp theo, chúng ta sẽ tạo một S3 bucket và một tệp trong đó.\nCreate a file and upload to s3 bucket Đổi về ssm-user\u0026rsquo;s thư mục bằng lệnh \u0026ldquo;cd ~\u0026rdquo; Tạo 1 file để kiểm tra bằng lệnh \u0026ldquo;fallocate -l 1G testfile.xyz\u0026rdquo;, 1 file tên \u0026ldquo;testfile.xyz\u0026rdquo; có kích thước 1GB sẽ được tạo. Tải file mình vừa tạo lên S3 với lệnh \u0026ldquo;aws s3 cp testfile.xyz s3://your-bucket-name\u0026rdquo;. Thay your-bucket-name bằng tên S3 bạn đã tạo. Bạn đã tải thành công tệp lên bộ chứa S3 của mình. Bây giờ bạn có thể kết thúc session.\nKiểm tra object trong S3 bucket Đi đến S3 console. Click tên s3 bucket của bạn Trong Bucket console, bạn sẽ thấy tệp bạn đã tải lên S3 bucket của mình Tóm tắt Chúc mừng bạn đã hoàn thành truy cập S3 từ VPC. Trong phần này, bạn đã tạo gateway endpoint cho Amazon S3 và sử dụng AWS CLI để tải file lên. Quá trình tải lên hoạt động vì gateway endpoint cho phép giao tiếp với S3 mà không cần Internet gateway gắn vào \u0026ldquo;VPC Cloud\u0026rdquo;. Điều này thể hiện chức năng của gateway endpoint như một đường dẫn an toàn đến S3 mà không cần đi qua pub lic Internet.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/",
	"title": "Ngày 02 - Hạ tầng Toàn cầu của AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-09 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Hạ tầng AWS Trung tâm dữ liệu (Data Center) Mỗi trung tâm dữ liệu có thể chứa hàng chục nghìn máy chủ. AWS tự thiết kế và vận hành phần cứng riêng để tối ưu hiệu năng và độ tin cậy. Vùng khả dụng (Availability Zone - AZ) Một hoặc nhiều trung tâm dữ liệu tách biệt vật lý trong cùng một Region. Mỗi AZ được thiết kế cách ly lỗi. Kết nối với nhau bằng mạng riêng độ trễ thấp, băng thông cao. AWS khuyến nghị triển khai workload tối thiểu trên hai AZ. Region Mỗi Region chứa ít nhất ba AZ. Hiện có hơn 25 Region trên toàn thế giới. Các Region kết nối với nhau qua mạng backbone của AWS. Phần lớn dịch vụ mặc định ở phạm vi Region. Edge Location Mạng lưới edge toàn cầu giúp phân phối nội dung với độ trễ tối thiểu. Được sử dụng bởi các dịch vụ như: Amazon CloudFront (CDN) AWS WAF (Tường lửa ứng dụng web) Amazon Route 53 (Dịch vụ DNS) Hands-On Labs Lab 01 – Thiết lập Tài khoản AWS \u0026amp; IAM Tạo tài khoản AWS → 01-01 Cấu hình thiết bị MFA ảo → 01-02 Tạo nhóm Admin và người dùng Admin → 01-03 Cập nhật thông tin hỗ trợ xác thực tài khoản → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/",
	"title": "Ngày 07 - Định tuyến VPC &amp; Network Interface",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-16 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Định tuyến VPC \u0026amp; ENI Route Table Route table xác định cách điều hướng lưu lượng mạng. Mỗi VPC có một route table mặc định chỉ chứa tuyến local để các subnet giao tiếp nội bộ. Có thể tạo thêm route table tùy chỉnh, nhưng tuyến local không thể xóa. Elastic Network Interface (ENI) ENI là card mạng ảo có thể gắn sang các EC2 instance khác nhau. Khi chuyển ENI, địa chỉ IP riêng, EIP và MAC được giữ nguyên. Elastic IP (EIP) là địa chỉ IPv4 công cộng tĩnh có thể gắn vào ENI. Bị tính phí nếu EIP không gắn cho tài nguyên nào. Tình huống sử dụng ENI:\nTách mạng quản trị khỏi mạng dữ liệu. Xây dựng thiết bị mạng/bảo mật (appliance). Instance hai cổng mạng chạy workload ở các subnet khác nhau. Giải pháp high availability chi phí thấp. VPC Endpoint VPC Endpoint cho phép kết nối riêng tư tới dịch vụ AWS qua AWS PrivateLink mà không đi Internet công cộng. Hai loại endpoint: Interface Endpoint: Tạo một ENI với IP riêng. Gateway Endpoint: Sử dụng route table (chỉ dành cho S3 và DynamoDB). Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (tiếp tục) Tạo Internet Gateway (IGW) → 03-03.3 Tạo Route Table (Outbound qua IGW) → 03-03.4 Tạo Security Group → 03-03.5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/",
	"title": "Ngày 12 - Lưu trữ &amp; Sao lưu cho EC2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-23 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Lưu trữ \u0026amp; Bảo mật cho EC2 Sao lưu trong EC2 AWS Backup cung cấp giải pháp sao lưu tập trung cho nhiều dịch vụ, bao gồm EC2. EBS Snapshot sao lưu các volume EBS: Sao lưu theo thời điểm (point-in-time). Dạng incremental (chỉ lưu block thay đổi). Lưu trữ trong S3 (không truy cập trực tiếp). AMI Backup chụp toàn bộ cấu hình EC2 dưới dạng image. Best practices cho Snapshot:\nLên lịch snapshot định kỳ. Sao chép snapshot sang Region khác cho DR. Gắn thẻ (tag) để quản lý vòng đời. Sử dụng Amazon Data Lifecycle Manager (DLM). Key Pair Key Pair dùng để xác thực an toàn khi kết nối EC2: Public Key – lưu trên instance. Private Key – người dùng giữ để SSH (Linux) hoặc RDP (Windows). Thay thế mật khẩu, tăng cường bảo mật. Lưu ý: Nếu mất private key, AWS không thể khôi phục. Quản lý Key Pair:\nTạo key pair trên AWS hoặc import key sẵn có. Lưu trữ private key an toàn. Dùng key pair khác nhau cho từng môi trường. Luân phiên (rotate) định kỳ. Elastic Block Store (EBS) Amazon EBS cung cấp lưu trữ dạng block bền vững cho EC2. Các loại volume: General Purpose SSD (gp2/gp3) – cân bằng hiệu năng và chi phí. Provisioned IOPS SSD (io1/io2) – cho workload cần IOPS cao. Throughput Optimized HDD (st1) – dữ liệu lớn, truy cập tuần tự. Cold HDD (sc1) – dữ liệu ít truy cập, chi phí thấp. Tính năng chính:\nGắn/Tháo volume với instance. Dữ liệu vẫn giữ khi instance tắt. Tạo snapshot để sao lưu hoặc copy sang Region khác. Tự động nhân bản trong phạm vi AZ. So sánh volume EBS:\nLoại Tình huống Max IOPS Max Throughput gp3 Mục đích tổng quát 16.000 1.000 MB/s io2 Hiệu năng cao 64.000 1.000 MB/s st1 Big data 500 500 MB/s sc1 Lưu trữ lạnh 250 250 MB/s "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/",
	"title": "Ngày 17 - Tính năng nâng cao của S3",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-30 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Hosting website tĩnh trên Amazon S3 Hosting trực tiếp website tĩnh (HTML, CSS, JS, hình ảnh) từ S3.\nKhả năng chính Thiết lập đơn giản: Chỉ vài bước để bật chế độ static website cho bucket. Chi phí thấp: Trả phí lưu trữ và băng thông tiêu chuẩn, không cần máy chủ web riêng. Scale linh hoạt: Tự động xử lý spike traffic. Tích hợp CDN: Dễ dàng kết hợp Amazon CloudFront để tăng hiệu năng toàn cầu. Cấu hình website tĩnh:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS cho phép tài nguyên web (font, JavaScript, \u0026hellip;) trên một domain truy cập tài nguyên ở domain khác.\nCấu hình CORS trên S3 Định nghĩa policy: Chỉ rõ những origin nào được phép truy cập nội dung bucket. Kiểm soát method: Cho phép các HTTP method cụ thể (GET, PUT, POST, \u0026hellip;). Tăng cường bảo mật: Ngăn truy cập cross-origin trái phép. Ví dụ cấu hình CORS:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Hiệu năng \u0026amp; thiết kế khóa object Cách đặt tên object ảnh hưởng đáng kể tới hiệu năng S3:\nPrefix ngẫu nhiên: Phân tán key qua nhiều partition để tăng song song. Tránh prefix tuần tự: Không dùng tiền tố tăng dần (ví dụ timestamp) cho workload throughput cao. Truy cập song song: Thiết kế key hỗ trợ đọc/ghi đồng thời. Best practice đặt key:\n❌ Tệ: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg\r✅ Tốt: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier – Lưu trữ dài hạn Các lớp Glacier được tối ưu cho lưu trữ dài hạn chi phí thấp.\nTùy chọn truy xuất Expedited / Fast: Vài phút; chi phí cao. Standard: 3–5 giờ; cân bằng chi phí. Bulk: 5–12 giờ; rẻ nhất cho khôi phục khối lượng lớn. Glacier Deep Archive Lớp chi phí thấp nhất cho lưu trữ nhiều năm, thời gian truy xuất khoảng 12 giờ.\nHands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Phần 2) Cấu hình object public → 57-5 Kiểm tra website → 57-6 Chặn toàn bộ public access → 57-7.1 Cấu hình CloudFront → 57-7.2 Kiểm tra CloudFront → 57-7.3 Bật Versioning cho bucket → 57-8 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/",
	"title": "Ngày 22 - IAM Policies &amp; Roles",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-07 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học IAM Policies Policy IAM là tài liệu JSON mô tả quyền. Các loại: Identity-based policy (gắn vào principal). Resource-based policy (gắn vào resource). Quy tắc đánh giá: mọi explicit Deny sẽ ghi đè Allow trên tất cả policy. Ví dụ ràng buộc admin S3:\nCho phép toàn bộ s3:* trên một bucket cụ thể. Explicit Deny mọi hành động không phải S3. Cấu trúc policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Logic đánh giá policy:\nMặc định mọi request bị từ chối. Allow rõ ràng sẽ ghi đè deny mặc định. Deny rõ ràng ghi đè mọi Allow. Permissions boundary giới hạn quyền tối đa. IAM Roles Role cung cấp quyền tạm thời cho user, dịch vụ hoặc danh tính bên ngoài. Các tình huống phổ biến: Cho phép dịch vụ AWS hành động thay bạn (ví dụ EC2 ghi vào S3). Truy cập chéo tài khoản. Liên kết danh tính từ IdP ngoài (federation). Cấp credential cho ứng dụng trên EC2 mà không cần lưu access key. Lợi ích\nKhông có credential dài hạn, phiên ngắn, hỗ trợ nguyên tắc least privilege và quản lý truy cập quy mô lớn. Các loại role:\nService Role: Cho dịch vụ AWS (EC2, Lambda, \u0026hellip;). Cross-Account Role: Truy cập tài nguyên ở tài khoản khác. Identity Provider Role: Cho người dùng liên kết (federated). Instance Profile: Vỏ chứa role dành cho EC2 instance. Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Phần 2) Sử dụng Access Key → 48-2.2 Tạo IAM Role → 48-3.1 Gán IAM Role → 48-3.2 Dọn dẹp tài nguyên → 48-4 Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Phần 1) Tạo IAM User → 28-2.1 Tạo IAM Policy → 28-3 Tạo IAM Role → 28-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/",
	"title": "Ngày 27 - Amazon RDS &amp; Aurora",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-14 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học RDBMS vs NoSQL RDBMS RDBMS lưu dữ liệu trong các bảng có quan hệ (hàng/cột), đảm bảo ràng buộc toàn vẹn, dùng SQL và cung cấp đầy đủ đặc tính ACID. Engine phổ biến: Oracle, MySQL, SQL Server, PostgreSQL, IBM Db2. Tổng quan NoSQL NoSQL hướng tới dữ liệu phi cấu trúc/bán cấu trúc, ưu tiên khả năng mở rộng và hiệu năng cao. Các loại chính: Document (MongoDB, CouchDB) Key–Value (Redis, DynamoDB) Column-Family (Cassandra, HBase) Graph (Neo4j, Amazon Neptune) Đặc điểm: schema linh hoạt, mở rộng ngang, xử lý big data tốt, thiết kế theo CAP. So sánh nhanh RDBMS vs NoSQL OLTP vs OLAP OLTP: nhiều giao dịch nhỏ, đồng thời; dữ liệu chuẩn hóa; truy vấn ngắn, phụ thuộc index. OLAP: phân tích phức tạp trên dữ liệu lịch sử; schema sao/tuyết; đọc nhiều, ghi ít. Amazon RDS \u0026amp; Aurora Amazon Relational Database Service (RDS) Dịch vụ CSDL quan hệ managed giúp đơn giản hóa triển khai, vá lỗi, backup và HA.\nEngine hỗ trợ: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Amazon Aurora. Tính năng chính: backup/patch tự động, mở rộng dễ dàng, Multi-AZ cho HA, bảo mật với VPC/IAM/SSL. Kiểu triển khai: Single-AZ Multi-AZ (standby đồng bộ ở AZ khác) Read Replica để scale đọc Tính năng RDS:\nAutomated Backups: khôi phục tới từng thời điểm trong 35 ngày. Manual Snapshots: backup thủ công do người dùng kích hoạt. Multi-AZ: tự động failover để duy trì HA. Read Replica: tối đa 15 replica (tùy engine) cho workload đọc. Parameter Groups: quản lý cấu hình database. Option Groups: bật tính năng bổ sung (ví dụ Oracle Advanced Security). Amazon Aurora Hệ CSDL tương thích MySQL/PostgreSQL được thiết kế lại cho cloud.\nĐiểm nổi bật: Hiệu năng ~5× MySQL / ~3× PostgreSQL (benchmark điển hình). Storage tự động mở rộng tới 128 TB. Sao chép 6 bản trên 3 AZ, tự chữa lành storage. Aurora Serverless mở rộng theo nhu cầu. Global Database cho độ trễ thấp đa vùng. Aurora Features:\nAurora Replicas: tối đa 15 read replica với độ trễ \u0026lt; 10 ms. Aurora Serverless: tự động scale compute. Aurora Global Database: replicate xuyên vùng \u0026lt; 1 giây. Aurora Backtrack: tua ngược DB về thời điểm cụ thể. Aurora Parallel Query: tăng tốc truy vấn phân tích trên dữ liệu hiện thời. Aurora Machine Learning: tích hợp ML native. Aurora vs RDS:\nTính năng Aurora RDS Hiệu năng ~5× MySQL, ~3× PostgreSQL Chuẩn Lưu trữ Tự mở rộng tới 128 TB Tăng thủ công Replica Tối đa 15 Tối đa 5 (MySQL) Failover \u0026lt; 30 giây 1–2 phút Backtrack Có Không Labs thực hành Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Phần 2) Tạo EC2 Instance → 05-3 Tạo RDS Database Instance → 05-4 Triển khai ứng dụng → 05-5 Backup \u0026amp; Restore → 05-6 Dọn tài nguyên → 05-7 Lab 43 – AWS Database Migration Service (DMS) (Phần 1) EC2 Connect RDP Client → 43-01 EC2 Connect Fleet Manager → 43-02 Cấu hình nguồn SQL Server → 43-03 Oracle Connect Source DB → 43-04 Oracle Config Source DB → 43-05 Drop Constraint → 43-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/",
	"title": "Ngày 32 - Contract-First &amp; Mocking",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-21 (Thứ Ba)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Contract-First Development Quy trình 5 bước Viết OpenAPI spec để định nghĩa contract. Dùng spec làm Single Source of Truth cho cả frontend và backend. Frontend dựng UI với mock data dựa trên spec. Backend implement API bám sát schema (status code, payload). Chạy contract testing để chắc chắn backend tuân thủ spec. paths: /books/{id}: get: summary: Get book detail responses: \u0026#34;200\u0026#34;: $ref: \u0026#34;#/components/responses/BookDetail\u0026#34; Lợi ích Giảm mismatch API vì mọi người xem cùng một spec. Documentation, mock server, test script có thể sinh tự động. Dễ review và versioning trước khi triển khai thật. Insight Viết contract trước code giúp giảm ~80% lỗi integration khi frontend/backend phát triển song song.\nMock API với Prism Prism đọc OpenAPI để sinh response giả, cho phép frontend test UI sớm. Hỗ trợ nhiều scenario (200, 404, 500) bằng cách khai báo example trong spec. Giữ nhịp làm việc khi backend chưa xong hoặc đang refactor. Khi nên dùng Sprint đầu của vertical slice. Cần demo flow nhưng chưa có dữ liệu thật. Muốn viết test tự động cho UI dựa trên contract. Ghi chú vận hành Chạy Prism tại localhost:4010, cấu hình NEXT_PUBLIC_API_URL trỏ đến mock. Đảm bảo header CORS trong mock giống backend production. Luôn commit spec trước khi mock để mọi người dùng đúng version. Labs thực hành Tạo OpenAPI spec cho endpoint /books/{id}. Khởi chạy Prism mock server và test luồng UI. Viết checklist review contract (status code, schema, example data). "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.2-day37-2025-10-28/",
	"title": "Ngày 37 - Tìm Kiếm Giọng Nói &amp; Kiến Trúc Chatbot",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-28 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTìm Kiếm Giọng Nói (Cách Siri Hoạt Động) Hệ thống tìm kiếm giọng nói tuân theo một pipeline từ đầu vào giọng nói đến phản hồi có thể thực hiện được:\nCác Thành Phần Pipeline: 1. Chuyển Đổi Analog sang Digital Lời nói (phát âm) → Mô hình sóng âm thanh → Spectrogram (mô hình tần số) → Chuỗi các khung âm thanh sử dụng Fast Fourier Transform (FFT)\n2. Nhận Dạng Giọng Nói Tự Động (ASR) Phân tích đặc trưng: Trích xuất các đặc trưng âm thanh Hidden Markov Model (HMM): Nhận dạng mẫu cho chuyển giọng nói sang văn bản Thuật toán Viterbi: Tìm chuỗi trạng thái ẩn có khả năng nhất Từ điển ngữ âm: Ánh xạ âm thanh thành từ Mô hình ngôn ngữ: Đảm bảo tính chính xác ngữ pháp 3. Chú Thích NLP Tokenization Gắn nhãn POS Nhận dạng thực thể (NER) 4. Ánh Xạ Mẫu-Hành Động Ánh xạ các ý định được nhận dạng thành các hành động phù hợp\n5. Quản Lý Dịch Vụ API nội bộ \u0026amp; bên ngoài (email, SMS, bản đồ, thời tiết, cổ phiếu, v.v.) Thực hiện hành động được yêu cầu 6. Chuyển Văn Bản Thành Giọng Nói (TTS) Chuyển đổi phản hồi trở lại thành giọng nói\n7. Phản Hồi Người Dùng Hệ thống học từ các sửa chữa để cải thiện độ chính xác\nKiến Trúc Voicebot Pipeline xử lý voicebot bao gồm nhiều cấp độ ngôn ngữ:\nCác Lớp Xử Lý: Phân Tích Giọng Nói (Âm Vị Học) Nhận dạng và phiên âm giọng nói sử dụng Nhận Dạng Giọng Nói Tự Động (ASR)\nPhân Tích Hình Thái và Từ Vựng (Hình Thái Học) Phân tích cấu trúc và ý nghĩa của từ sử dụng các quy tắc hình thái và từ vựng\nPhân Tích Cú Pháp (Cú Pháp) Hiểu cấu trúc câu sử dụng từ vựng và quy tắc ngữ pháp\nSuy Luận Ngữ Cảnh (Ngữ Nghĩa) Hiểu ý nghĩa trong ngữ cảnh sử dụng ngữ cảnh diễn ngôn\nSuy Luận và Thực Thi Ứng Dụng (Lý Luận) Sử dụng kiến thức miền để quyết định hành động\nLập Kế Hoạch Phát Ngôn Lập kế hoạch những gì sẽ nói trong phản hồi\nHiện Thực Hóa Cú Pháp Tạo ra các câu chính xác ngữ pháp\nHiện Thực Hóa Hình Thái Áp dụng các dạng từ chính xác\nMô Hình Phát Âm Tạo ra phát âm phù hợp\nTổng Hợp Giọng Nói Chuyển đổi văn bản trở lại thành giọng nói\nQuy Trình Làm Việc Chatbot Quy Trình Từng Bước: 1. Người Dùng → Ứng Dụng Chat Người dùng gõ: \u0026ldquo;Tôi muốn kiểm tra số dư tài khoản.\u0026rdquo; Ứng Dụng Chat = giao diện nơi người dùng gõ (web, app, messenger)\n2. Ứng Dụng Chat → Chatbot Tin nhắn được gửi đến hệ thống chatbot\n3. Chatbot → NLP Engine Chatbot gửi tin nhắn đến NLP Engine để phân tích\nNLP Engine thực hiện hai tác vụ chính: (a) Phát Hiện Ý Định Xác định người dùng muốn làm gì\nVí dụ: kiểm_tra_số_dư (b) Trích Xuất Thực Thể Trích xuất dữ liệu quan trọng từ câu\nVí dụ: tài_khoản = thanh toán/tiết kiệm? 4. NLP Engine → Logic Nghiệp Vụ / Dịch Vụ Dữ Liệu Dựa trên ý định, chatbot gọi dịch vụ phù hợp:\nTruy vấn cơ sở dữ liệu Gọi API Thực thi quy tắc nghiệp vụ Xử lý logic backend Ví dụ: Gọi API để lấy số dư từ hệ thống ngân hàng\n5. Dịch Vụ Dữ Liệu → Chatbot Backend trả về kết quả:\n\u0026ldquo;Số dư tài khoản của bạn là 12.500.000₫\u0026rdquo;\n6. Chatbot → Ứng Dụng Chat Chatbot đóng gói thông tin thành phản hồi ngôn ngữ tự nhiên\n7. Hiển Thị Cho Người Dùng Người dùng nhìn thấy phản hồi\nChatbot = Lắng Nghe + Trò Chuyện Lắng Nghe (NLP - Hiểu) Nhận dạng ý định Trích xuất thực thể Hiểu ngữ cảnh Trò Chuyện (NLG - Tạo) Tạo ngôn ngữ tự nhiên Công thức hóa phản hồi Cá nhân hóa Đằng Sau Hậu Trường: Dữ liệu dựa trên kiến thức: Sự thật, quy tắc, FAQ Học máy: Học từ các tương tác Logic nghiệp vụ: Quy tắc cụ thể của ứng dụng Phân Biệt Quan Trọng: Từ Khóa vs Thực Thể Từ Khóa = các từ chỉ ra chủ đề hoặc đối tượng Thực Thể = các điểm dữ liệu cụ thể với loại và giá trị\nVí dụ: \u0026ldquo;Đặt chuyến bay đến Paris vào thứ Sáu\u0026rdquo;\nTừ khóa: đặt, chuyến bay Thực thể: điểm_đến = \u0026ldquo;Paris\u0026rdquo; (ĐỊA ĐIỂM) ngày = \u0026ldquo;thứ Sáu\u0026rdquo; (NGÀY THÁNG) Không phải tất cả từ khóa đều là thực thể, nhưng tất cả thực thể đều được trích xuất từ từ khóa!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.2-day42-2025-11-04/",
	"title": "Ngày 42 - Tổng Quan Kiến Trúc Transformer",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-04 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nKiến Trúc Transformer: Bức Tranh Toàn Cảnh Mô hình transformer được giới thiệu trong bài báo \u0026ldquo;Attention is All You Need\u0026rdquo; đã cách mạng hóa NLP. Hãy hiểu cấu trúc hoàn chỉnh của nó.\nKiến Trúc Cấp Cao CỬA VÀO CHUỖI\r↓\r[Tokenization \u0026amp; Embedding]\r↓\r[Thêm Positional Encoding]\r↓\r┌─────────────────────────────────┐\r│ ENCODER (N layers) │\r│ ├─ Multi-Head Attention │\r│ ├─ Layer Normalization │\r│ ├─ Feed-Forward Network │\r│ └─ Residual Connections │\r└─────────────────────────────────┘\r↓\r[Context Vectors từ Encoder]\r↓\r┌─────────────────────────────────┐\r│ DECODER (N layers) │\r│ ├─ Masked Multi-Head Attention │\r│ ├─ Encoder-Decoder Attention │\r│ ├─ Feed-Forward Network │\r│ └─ Layer Normalization │\r└─────────────────────────────────┘\r↓\r[Linear Layer + Softmax]\r↓\rĐẦU RA XÁC SUẤT Thành Phần 1: Word Embeddings Mỗi từ được chuyển đổi thành một vector dày đặc (thường là 512-1024 chiều).\nVí Dụ:\nTừ: \u0026#34;happy\u0026#34;\rEmbedding: [0.2, -0.5, 0.8, ..., 0.1] // 512 giá trị Thành Phần 2: Positional Encoding Vấn Đề: Transformers không có thứ tự tuần tự được tích hợp sẵn (không giống RNNs). Vì vậy chúng ta phải thêm thông tin vị trí một cách rõ ràng.\nGiải Pháp: Thêm các vector positional encoding vào embeddings.\nCông Thức:\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\rPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\rTrong đó:\r- pos = vị trí trong chuỗi (0, 1, 2, ...)\r- i = chỉ số chiều\r- d_model = chiều embedding (512, 1024, v.v.) Ý Tưởng:\nVị trí 0: \u0026ldquo;I\u0026rdquo; nhận PE₀ Vị trí 1: \u0026ldquo;am\u0026rdquo; nhận PE₁ Vị trí 2: \u0026ldquo;happy\u0026rdquo; nhận PE₂ Ví Dụ:\nEmbedding(\u0026#34;I\u0026#34;) = [0.2, -0.5, 0.8, ..., 0.1]\rPE(pos=0) = [0.0, 1.0, 0.0, 1.0, ..., 0.5]\rFinal = [0.2, 0.5, 0.8, 1.0, ..., 0.6] Thành Phần 3: Multi-Head Attention Thay vì một cơ chế attention, chúng ta có h \u0026ldquo;đầu\u0026rdquo; khác nhau chạy song song.\nKhái Niệm:\nĐầu vào: Query (Q), Key (K), Value (V) matrices\rĐầu 1: ScaledDotProductAttention(Q₁, K₁, V₁)\rĐầu 2: ScaledDotProductAttention(Q₂, K₂, V₂)\r...\rĐầu h: ScaledDotProductAttention(Qₕ, Kₕ, Vₕ)\rĐầu ra = Concatenate(Head₁, Head₂, ..., Headₕ) Tại Sao Nhiều Đầu?\nĐầu 1 có thể học mối quan hệ \u0026ldquo;chủ ngữ-động từ\u0026rdquo; Đầu 2 có thể học mối quan hệ \u0026ldquo;tính từ-danh từ\u0026rdquo; Đầu 3 có thể học mối quan hệ \u0026ldquo;đại từ-tham chiếu\u0026rdquo; Cùng nhau: Hiểu biết ngữ cảnh phong phú Cấu Hình Điển Hình:\nSố lượng đầu: 8-16 Chiều trên mỗi đầu: 64 (nếu tổng = 512, thì 512/8 = 64) Thành Phần 4: Residual Connections \u0026amp; Layer Normalization Residual Connections Đầu ra = Đầu vào + Attention(Đầu vào) Điều này giúp luồng gradient trong suốt huấn luyện và cho phép mạng lưới đi sâu hơn.\nLayer Normalization Normalized = (x - mean) / sqrt(variance + epsilon) Ổn định huấn luyện và tăng tốc độ hội tụ.\nThành Phần 5: Feed-Forward Network Sau attention, có một mạng feed-forward 2 lớp đơn giản:\nĐầu ra = ReLU(Linear₁(x)) → Linear₂ Các chiều điển hình:\nĐầu vào: [batch_size, seq_length, 512]\r↓ Linear₁ (512 → 2048)\r[batch_size, seq_length, 2048]\r↓ ReLU (non-linear)\r[batch_size, seq_length, 2048]\r↓ Linear₂ (2048 → 512)\r[batch_size, seq_length, 512] Điều này mở rộng rồi co lại, cho phép các phép biến đổi phi tuyến.\nEncoder: Chế Độ Xem Chi Tiết Lớp Encoder Duy Nhất:\nĐầu vào (x)\r↓\r[Multi-Head Self-Attention]\r↓\r[+ Residual Connection với đầu vào]\r↓\r[Layer Normalization]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual Connection]\r↓\r[Layer Normalization]\r↓\rĐầu ra Điểm Chính: Trong encoder, mỗi từ attend tới TẤT CẢ các từ (bao gồm cả chính nó) trong cùng một câu.\nEncoder cho: Đại diện ngữ cảnh của mỗi từ, xem xét tất cả các từ khác.\nDecoder: Chế Độ Xem Chi Tiết Decoder tương tự nhưng với masking:\nĐầu vào (shifted right by 1)\r↓\r[Masked Multi-Head Self-Attention] ← Chỉ có thể attend vào các vị trí trước\r↓\r[+ Residual + LayerNorm]\r↓\r[Encoder-Decoder Attention] ← Attend vào đầu ra encoder\r↓\r[+ Residual + LayerNorm]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual + LayerNorm]\r↓\rĐầu ra Ba Cơ Chế Attention trong Decoder:\nMasked Self-Attention:\nQueries, Keys, Values từ decoder Mỗi vị trí chỉ có thể attend tới các vị trí trước đó Ngăn chặn rò rỉ thông tin (decoder không thấy các từ tương lai) Encoder-Decoder Attention:\nQueries từ decoder Keys, Values từ encoder Decoder có thể attend tới bất kỳ vị trí encoder nào Feed-Forward:\nMạng 2 lớp giống như encoder Kết Hợp Tất Cả: Transformer Đầy Đủ Giai Đoạn Huấn Luyện Đầu vào: \u0026#34;Je suis heureux\u0026#34; (Tiếng Pháp)\rMục Tiêu: \u0026#34;I am happy\u0026#34; (Tiếng Anh)\rĐầu vào Encoder:\r- Tokenize: [Je, suis, heureux]\r- Embed mỗi token\r- Thêm positional encoding\r- Xử lý qua N lớp encoder\r→ Đầu ra: C (context vectors)\rĐầu vào Decoder:\r- Mục tiêu shifted right: [\u0026lt;START\u0026gt;, I, am]\r- Embed mỗi token\r- Thêm positional encoding\r- Xử lý qua N lớp decoder\r- Sử dụng masked self-attention\r- Sử dụng encoder-decoder attention trên C\r→ Đầu ra logits cho mỗi vị trí\rMất mát: So sánh dự đoán \u0026#34;am happy\u0026#34; với thực tế \u0026#34;am happy\u0026#34;\rBackprop: Cập nhật tất cả các trọng số Giai Đoạn Suy Luận Đầu vào Encoder: [Je, suis, heureux]\r→ Đầu ra: C (context vectors)\rDecoder:\rBước 1: Bắt đầu với [\u0026lt;START\u0026gt;]\rDự đoán từ tiếp theo: \u0026#34;I\u0026#34;\rBước 2: [\u0026lt;START\u0026gt;, I] Dự đoán từ tiếp theo: \u0026#34;am\u0026#34;\rBước 3: [\u0026lt;START\u0026gt;, I, am]\rDự đoán từ tiếp theo: \u0026#34;happy\u0026#34;\rBước 4: [\u0026lt;START\u0026gt;, I, am, happy]\rDự đoán từ tiếp theo: \u0026lt;END\u0026gt;\rĐầu ra: \u0026#34;I am happy\u0026#34; Tóm Tắt: Tại Sao Kiến Trúc Này Hoạt Động Tính Năng Lợi Ích Không RNN Hoàn toàn có thể song song hóa - huấn luyện trên GPU hiệu quả Self-Attention trong Encoder Mỗi từ nhận ngữ cảnh từ TẤT CẢ các từ khác Masked Attention trong Decoder Không thể nhìn thấy tương lai - huấn luyện với tạo hình autoregressive Positional Encoding Bảo tồn thứ tự từ mà không xử lý tuần tự RNN Multi-Head Attention Học nhiều loại mối quan hệ đồng thời Residual Connections Luồng gradient - cho phép huấn luyện mạng sâu Layer Normalization Ổn định - hội tụ nhanh hơn Các Đổi Mới Chính Song Song Hóa: O(1) độ sâu thay vì O(n) cho RNNs Phụ Thuộc Dài Hạn: Attention có thể kết nối trực tiếp bất kỳ hai vị trí nào Khả Năng Mở Rộng: Có thể tăng kích thước mô hình với cải thiện dự đoán Transfer Learning: Các transformer được huấn luyện trước (BERT, GPT) hoạt động trên các tác vụ Tiếp Theo: Chúng ta đi sâu vào chi tiết toán học của các cơ chế attention!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.2-day47-2025-11-11/",
	"title": "Ngày 47 - Kiến Trúc BERT",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-11 (Thứ Ba)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nBERT: Bidirectional Encoder Representations from Transformers BERT là bước ngoặt thay đổi NLP mãi mãi. Công bố bởi Google vào năm 2018, nó cho thấy rằng bối cảnh lưỡng chiều quan trọng hơn mọi người nghĩ.\nCái Gì Khác Biệt Về BERT? Sự Phát Triển Word2Vec (2013)\r├─ Embedding: \u0026#34;cat\u0026#34; → [0.2, -0.5, 0.8, ...]\r├─ Vấn Đề: Cùng embedding ở mọi nơi, không bối cảnh\r└─ Phương Pháp: Đơn giản, tĩnh\r↓\rELMo (2015)\r├─ BiLSTM: Xử lý trái VÀ phải\r├─ Vấn Đề: Vẫn tuần tự, chậm hơn\r└─ Phương Pháp: Lưỡng chiều nhưng không song song\r↓\rGPT (2018)\r├─ Transformer: Chú ý song song!\r├─ Vấn Đề: Chỉ nhìn TRÁI (trái sang phải)\r│ \u0026#34;The bank was robbed\u0026#34; → Không thể sử dụng \u0026#34;robbed\u0026#34; để hiểu \u0026#34;bank\u0026#34;\r└─ Phương Pháp: Đơn chiều, hạn chế\r↓\rBERT (2018) ⭐\r├─ Transformer: Song song VÀ lưỡng chiều!\r├─ Giải Pháp: Nhìn CẢ HAI bên của mỗi từ\r│ \u0026#34;The bank was robbed\u0026#34; → Sử dụng cả \u0026#34;The\u0026#34; và \u0026#34;was robbed\u0026#34; để hiểu \u0026#34;bank\u0026#34;\r└─ Phương Pháp: Bối cảnh lưỡng chiều đầy đủ Tổng Quan Kiến Trúc BERT Thông Số Mô Hình BERT-base:\r├─ Lớp Transformer: 12\r├─ Kích thước ẩn: 768\r├─ Đầu chú ý: 12 (768/12 = 64 mỗi đầu)\r├─ Kích thước feed-forward: 3072\r├─ Tổng tham số: 110 Triệu\r├─ Thời gian huấn luyện: 4 ngày trên 16 TPUs\r└─ Dữ liệu huấn luyện: 3.3 tỷ từ từ Wikipedia + BookCorpus\rBERT-large:\r├─ Lớp Transformer: 24\r├─ Kích thước ẩn: 1024\r├─ Tổng tham số: 340 Triệu\r└─ Chậm hơn nhiều nhưng hiệu suất cao hơn Kiến Trúc Encoder-Only BERT vs Transformer:\rTransformer (Đầy Đủ):\rInput → Encoder (6 lớp) → Decoder (6 lớp) → Output\r(Có thể nhìn tất cả) (Che, không thể nhìn tương lai)\rBERT (Encoder Only):\rInput → Encoder (12 lớp) → Output\r(Chú ý lưỡng chiều đầy đủ!)\rKhác Biệt Chính:\r├─ GPT = Encoder + Decoder nhưng decoder là đơn chiều\r├─ BERT = Encoder only, lưỡng chiều đầy đủ\r└─ T5 = Encoder + Decoder, cả hai lưỡng chiều Cơ Chế Self-Attention (Ôn Lại) Nhớ lại từ Ngày 43:\nĐối với mỗi từ, tạo ba vectơ:\r├─ Query (Q): \u0026#34;Tôi cần thông tin gì?\u0026#34;\r├─ Key (K): \u0026#34;Tôi cung cấp thông tin gì?\u0026#34;\r└─ Value (V): \u0026#34;Đại diện thực của tôi là gì?\u0026#34;\rĐiểm Chú Ý = softmax(Q·K^T / √d) · V\rVí Dụ: Xử lý \u0026#34;bank\u0026#34; trong \u0026#34;The bank was robbed\u0026#34;\rQuery cho \u0026#34;bank\u0026#34;: [0.1, 0.2, 0.3, ...]\rKeys cho tất cả các từ:\r├─ \u0026#34;The\u0026#34; key: [0.2, 0.1, 0.1, ...] → Điểm: 0.7\r├─ \u0026#34;bank\u0026#34; key: [0.1, 0.2, 0.3, ...] → Điểm: 0.95 (cao! self-attention)\r├─ \u0026#34;was\u0026#34; key: [0.5, 0.2, 0.1, ...] → Điểm: 0.8\r└─ \u0026#34;robbed\u0026#34; key: [0.3, 0.3, 0.1, ...] → Điểm: 0.85\rChú Ý: \u0026#34;robbed\u0026#34; giúp hiểu \u0026#34;bank\u0026#34;!\rĐây là lý do tại sao BERT là lưỡng chiều. BERT vs GPT: Sự Khác Biệt Chính Che Phủ Chú Ý GPT (Trái sang Phải, Kiểu Decoder) Xử lý: \u0026#34;The bank was robbed\u0026#34;\rVị Trí 1 (\u0026#34;The\u0026#34;):\r├─ Có thể chú ý đến: \u0026#34;The\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;bank\u0026#34; ✗ (che, tương lai)\r├─ Có thể chú ý đến: \u0026#34;was\u0026#34; ✗ (che, tương lai)\r└─ Có thể chú ý đến: \u0026#34;robbed\u0026#34; ✗ (che, tương lai)\rVị Trí 4 (\u0026#34;robbed\u0026#34;):\r├─ Có thể chú ý đến: \u0026#34;The\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;bank\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;was\u0026#34; ✓\r└─ Có thể chú ý đến: \u0026#34;robbed\u0026#34; ✓\rVấn Đề: \u0026#34;The\u0026#34; không thể sử dụng \u0026#34;robbed\u0026#34; để hiểu ý nghĩa của nó! BERT (Lưỡng Chiều, Kiểu Encoder) Xử lý: \u0026#34;The bank was robbed\u0026#34;\rVị Trí 1 (\u0026#34;The\u0026#34;):\r├─ Có thể chú ý đến: \u0026#34;The\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;bank\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;was\u0026#34; ✓\r└─ Có thể chú ý đến: \u0026#34;robbed\u0026#34; ✓\rVị Trí 4 (\u0026#34;robbed\u0026#34;):\r├─ Có thể chú ý đến: \u0026#34;The\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;bank\u0026#34; ✓\r├─ Có thể chú ý đến: \u0026#34;was\u0026#34; ✓\r└─ Có thể chú ý đến: \u0026#34;robbed\u0026#34; ✓\rLợi Ích: \u0026#34;The\u0026#34; CÓ THỂ sử dụng \u0026#34;robbed\u0026#34; để hiểu bối cảnh!\rĐại diện tốt hơn cho TẤT CẢ các từ! Tại Sao Lưỡng Chiều Tốt Hơn Ví Dụ Hiểu Biết Ngôn Ngữ Câu: \u0026#34;I went to the bank to deposit money\u0026#34;\rTừ: \u0026#34;bank\u0026#34; (tổ chức tài chính)\rXử Lý GPT (Trái sang Phải):\r├─ Có thể nhìn: \u0026#34;I went to the\u0026#34;\r├─ Không thể nhìn: \u0026#34;to deposit money\u0026#34; (tương lai!)\r├─ Hiểu biết bối cảnh: 30% (không đầy đủ)\rXử Lý BERT (Lưỡng Chiều):\r├─ Có thể nhìn: \u0026#34;I went to the\u0026#34; + \u0026#34;to deposit money\u0026#34;\r├─ Bối cảnh đầy đủ: 100%\r├─ Hiểu biết bối cảnh: 90% (tốt hơn nhiều!) Đơn Nghĩa Hóa Nghĩa Của Từ \u0026#34;I went to the bank to deposit money\u0026#34; → Tổ chức tài chính\r\u0026#34;The river bank was very scenic\u0026#34; → Đất dọc theo sông\r\u0026#34;The bank was robbed yesterday\u0026#34; → Công ty/tổ chức\rGPT: Chỉ nhìn bối cảnh bên trái\r├─ \u0026#34;I went to the\u0026#34; (không đầy đủ)\r├─ Rủi ro: Nghĩa sai!\rBERT: Nhìn bối cảnh đầy đủ\r├─ \u0026#34;I went to the _____ to deposit money\u0026#34;\r├─ Tự động đơn nghĩa hóa! ✓ Đại Diện Đầu Vào Của BERT Embeddings Token BERT sử dụng WordPiece tokenization:\rĐầu Vào: \u0026#34;The bank was robbed\u0026#34;\rTokenization:\r├─ \u0026#34;The\u0026#34; → [101] (Token CLS thêm vào đầu)\r├─ \u0026#34;bank\u0026#34; → [1998]\r├─ \u0026#34;was\u0026#34; → [2003]\r├─ \u0026#34;robbed\u0026#34; → [6861]\r├─ \u0026#34;[SEP]\u0026#34; → [102] (Token phân tách thêm vào cuối)\rToken thô: [101, 1998, 2003, 6861, 102] Ba Loại Embeddings BERT kết hợp ba loại embedding:\rToken Embedding: \u0026#34;bank\u0026#34; → [0.2, -0.5, 0.8, ...]\r├─ Đại diện từ cụ thể\r├─ Học được trong pre-training\rSegment Embedding: Câu A vs Câu B → [0.1, 0.2, ...]\r├─ Đánh dấu câu nào mỗi token thuộc về\r├─ Cho các tác vụ cặp câu (Next Sentence Prediction)\rPositional Embedding: Vị Trí 2 → [0.5, 0.1, ...]\r├─ Mã hóa vị trí trong chuỗi\r├─ 0, 1, 2, 3, 4...\rFinal Embedding = Token + Segment + Positional\r= [0.2, -0.5, 0.8, ...] + [0.1, 0.2, ...] + [0.5, 0.1, ...]\r= [0.8, -0.2, 1.3, ...] So Sánh: BERT vs GPT vs Transformer Gốc BERT GPT Transformer\r──── ─── ─────────────\rEncoder-Decoder Encoder only Encoder+Decoder Encoder+Decoder\rLoại Chú Ý Lưỡng chiều Đơn chiều Hỗn hợp\rTác Vụ Pre-train MLM + NSP Language Model Không (N/A)\rHướng Cả hai Trái sang phải Decoder che\rTốt Nhất Cho Hiểu Biết Tạo Văn Bản Dịch\rHiệu Suất Tốt Nhất (90%) Tốt (85%) Thay Đổi (thay đổi%)\rTốc Độ Trung Bình Nhanh Chậm Nhất\rTham Số 110M/340M 117M/345M ~100M Hiệu Suất Của BERT Trên Các Tác Vụ NLP Kết Quả Benchmark GLUE Trước BERT (Tiên Tiến Nhất 2017):\r├─ Cảm Xúc: 92.1% độ chính xác\r├─ Tương Tự Văn Bản: 80.5% Spearman\r├─ Tên Thực Thể: 91.6% F1\r└─ Trả Lời Câu Hỏi: 78.2% F1\rBERT-base:\r├─ Cảm Xúc: 94.9% độ chính xác (+2.8%)\r├─ Tương Tự Văn Bản: 85.8% Spearman (+5.3%)\r├─ Tên Thực Thể: 96.4% F1 (+4.8%)\r└─ Trả Lời Câu Hỏi: 84.2% F1 (+6.0%)\rCải Thiện: 4-6% trên toàn bộ!\rĐối với một hệ thống được triển khai, đó là rất lớn! Tại Sao BERT Trở Thành Rất Nổi Tiếng Fine-tuning Đơn Giản: Thêm một đầu phân loại, fine-tune tất cả các lớp Hiệu Suất Mạnh: Vượt qua tất cả các phương pháp trước đó bằng các biên độ lớn Trọng Số Được Huấn Luyện Trước: Có thể tải xuống trọng số, sử dụng ngay lập tức Đa Ngôn Ngữ: BERT được huấn luyện trên 104 ngôn ngữ (mBERT) Có Thể Chuyển Giao: Hoạt động cho hầu hết mọi tác vụ NLP Những Hạn Chế của BERT ⚠️ Lưỡng chiều = Không thể tạo văn bản trực tiếp\nVấn Đề: Bạn không thể làm \u0026#34;cho \u0026#39;The bank\u0026#39;, tạo phần còn lại\u0026#34;\rLý do: Mỗi token có thể nhìn thấy tương lai!\rSẽ sụp đổ tác vụ tạo văn bản.\rGiải Pháp: Fine-tune cho phân loại, không phải tạo văn bản ⚠️ Cửa sổ bối cảnh cố định: 512 token tối đa\nCác tài liệu dài không phù hợp!\rGiải Pháp: BERT phân cấp hoặc RoBERTa (4096 token) ⚠️ Tính toán tốn kém\n110M tham số = 440MB chỉ cho trọng số\rSuy luận chậm hơn các mô hình nhỏ hơn\rGiải Pháp: Chưng cất (DistilBERT, 40% tham số, 60% tốc độ) Các Biến Thể của BERT BERT\r├─ BERT-base: Gốc, 110M tham số\r├─ BERT-large: Lớn hơn, 340M tham số\r├─ mBERT: 104 ngôn ngữ, đa ngôn ngữ\r├─ DistilBERT: Chưng cất, 40M tham số, nhanh hơn\r└─ RoBERTa: Huấn luyện cải thiện, 125M tham số\rTất cả dựa trên cùng kiến trúc encoder-only Các Lợi Ích Chính ✅ Bối cảnh lưỡng chiều quan trọng: Cái nhìn sâu sắc về bước ngoặt BERT ✅ Kiến Trúc Encoder-Only: Hoàn hảo để hiểu ✅ Trọng số được huấn luyện trước là vàng: Không huấn luyện từ đầu! ✅ Self-attention với bối cảnh đầy đủ: Đại diện tốt hơn ✅ Fine-tuning Đơn Giản: Thêm đầu + huấn luyện 1-3 epoch\nĐiều Gì Sắp Tới Ngày 48: Cách BERT học được sự hiểu biết này (MLM + NSP) Ngày 49: T5 - Mở rộng quy mô đến encoder-decoder Ngày 50: Fine-tuning BERT cho các tác vụ thực tế Vẻ đẹp của BERT: Transformer lưỡng chiều cho thấy rằng nhìn thấy mọi thứ cùng một lúc là chìa khóa để hiểu ngôn ngữ.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Thư Viện Online - Nền Tảng Nội Dung Serverless Cho Nhóm Nhỏ 1. Tổng quan điều hành Dự án Thư Viện Online nhằm xây dựng một nền tảng serverless, chi phí thấp để lưu trữ và phân phối nội dung (PDF/ePub) cho một nhóm người dùng nhỏ (ban đầu ~100 người, nhóm người dùng gồm sinh viên/lab cần chia sẻ tài liệu nghiên cứu nội bộ có kiểm duyệt). Giải pháp này ưu tiên tính bảo mật, quy trình duyệt nội dung (Admin Approval), và chi phí vận hành minh bạch, tuyến tính khi mở rộng. Kiến trúc sử dụng AWS Serverless hoàn toàn (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB). Chi phí dự kiến cho MVP (không tính Free Tier) ≈ $9.80/tháng, đảm bảo khả năng mở rộng lên 5.000 đến 50.000 người dùng với chi phí dễ dự đoán.\n2. Vấn đề Vấn đề là gì? Tài liệu và sách bị phân tán; thiếu một hệ thống truyền tải nội dung an toàn và có kiểm soát truy cập; quy trình thêm hoặc kiểm duyệt nội dung tốn thời gian và nhiều vấn đề liên quan đến pháp lý.\nGiải pháp Xây dựng một pipeline serverless trên AWS: Người dùng tải lên qua Presigned PUT URL (tới S3 tạm); Admin phê duyệt → Lambda di chuyển file đến thư mục công khai (nhưng được bảo vệ); Người đọc truy cập qua Signed GET URL (từ CloudFront/CDN) để đảm bảo tốc độ và kiểm soát truy cập.\nLợi ích và Tỷ suất hoàn vốn Giá trị kinh doanh: Tập trung hóa nội dung; kiểm soát chất lượng qua quy trình duyệt; triển khai nhanh chóng với CI/CD. Lợi ích kỹ thuật: Chi phí vận hành thấp (≈ $9.80/tháng ở MVP, không tính Free Tier); kiến trúc Serverless có thể mở rộng quy mô lớn (scale) dễ dàng; bảo mật truy cập nội dung. 3. Kiến trúc giải pháp A. High-level B. Luồng xử lý yêu cầu Dịch vụ AWS Sử Dụng Dịch vụ Vai trò chính Hoạt động cụ thể Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, quản lý domain Cognito Authentication Đăng ký/Đăng nhập, cấp JWT, refresh token API Gateway Entry point API Nhận request, xác thực JWT, route đến Lambda Lambda Business Logic Xử lý upload, duyệt, tạo signed URL, ghi metadata S3 Object Storage Lưu file gốc, file đã duyệt, được download qua Cloudfront Signed URL CloudFront CDN Phân phối nhanh nội dung, chặn direct access qua OAC DynamoDB Database Lưu metadata (tên sách, uploader, trạng thái duyệt) Route 53 DNS Trỏ domain đến Amplify Hosting, API Gateway, CloudFront CloudWatch Monitoring Lưu log Lambda, cảnh báo lỗi hoặc chi phí bất thường Tìm kiếm (Search):\nTìm kiếm đơn giản theo trường (VD: tên sách, tác giả), sử dụng DynamoDB GSIs cho các thuộc tính này và query theo GSI. Luồng xử lý yêu cầu User Upload: Presigned PUT tới S3 thư mục uploads/. Admin Approval: Lambda copy file từ uploads/ sang public/books/ khi được duyệt. Reader Security: CloudFront sử dụng Origin Access Control (OAC) để chặn truy cập trực tiếp S3 và chỉ cho phép đọc qua Signed URL (ngắn hạn) do Lambda tạo ra. Kiến trúc tìm kiếm Tìm kiếm đơn giản: Thiết kế GSI cho title và author (ví dụ: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Thêm endpoint GET /search?title=...\u0026amp;author=... để query theo GSI thay vì Scan. Phân quyền Admin Sử dụng Cognito User Groups với một nhóm Admins trong User Pool. Khi Admin đăng nhập, JWT sẽ chứa cognito:groups: [\u0026quot;Admins\u0026quot;]. Các Lambda thuộc nghiệp vụ Admin (ví dụ approveBook, takedownBook) phải kiểm tra claim này; nếu thiếu group, trả 403 Forbidden. Có thể dùng JWT Authorizer (API Gateway HTTP API) để xác thực, phần phân quyền chi tiết xử lý trong Lambda dựa trên claim. 4. Triển khai Kỹ Thuật Triển khai Thiết kế \u0026amp; IaC (Infra-as-Code): Xây dựng các stack CDK (Cognito, DDB, S3, Amplify, Lambda, API). Flow Upload \u0026amp; Duyệt: Triển khai Presigned PUT, lưu metadata (trạng thái pending), và logic Admin duyệt (copy file). Flow Đọc Sách: Triển khai endpoint Signed GET, và giao diện đọc (FE stream qua CloudFront). Vận hành (Ops): Thiết lập logs CloudWatch (retention ngắn), cảnh báo ngân sách (Budget Alerts), hardening IAM. Search: MVP: thêm GSI cho title, author và endpoint GET /search query theo GSI. Yêu cầu Kỹ Thuật Sử dụng CDK để định nghĩa toàn bộ hạ tầng. API Gateway phải là HTTP API để tối ưu chi phí. Lambda (Python) xử lý logic nghiệp vụ và tương tác DynamoDB/S3. S3 Bucket Policy phải chặn truy cập công khai và chỉ cho phép CloudFront OAC. 5. Lộ trình và các mốc tiến độ Lộ trình Dự án Nền tảng \u0026amp; Xác thực (Tuần 1-2) Mục tiêu là thiết lập hạ tầng và cho phép người dùng đăng nhập.\nTác vụ Backend (CDK/DevOps): Viết stack CDK/IaC cho Cognito (User Pool, App Client). Viết stack CDK cho DynamoDB (bảng chính, chưa cần GSI). Viết stack CDK cho S3 (Bucket uploads, public, logs) và cấu hình OAC (Origin Access Control). Triển khai API Gateway (HTTP API) và một Lambda \u0026ldquo;hello world\u0026rdquo; để kiểm thử. Tác vụ Frontend (Amplify): Cấu hình Amplify Hosting và kết nối với repo GitHub (CI/CD). Tích hợp Amplify UI / Cognito SDK cho các trang: Đăng ký, Xác thực email, Đăng nhập, Quên mật khẩu. Kết quả (Milestone): Developer có thể git push và FE tự động deploy. Người dùng có thể đăng ký/đăng nhập và nhận được JWT token. Luồng Upload \u0026amp; Duyệt (Tuần 2-3) Mục tiêu là cho phép người dùng (đã đăng nhập) tải file lên và Admin duyệt file đó.\nTác vụ Backend (CDK/Lambda): Viết Lambda createUploadUrl: Xác thực JWT (phải đăng nhập). Tạo Presigned PUT URL trỏ đến thư mục uploads/ trên S3. Ghi metadata vào DynamoDB (status: PENDING). Viết Lambda approveBook: Xác thực JWT (phải là Admin). Copy file từ uploads/ sang public/books/. Cập nhật status trong DynamoDB (status: APPROVED). Tác vụ Frontend: Xây dựng Form Upload (kéo thả, chọn file). Gọi API createUploadUrl để lấy URL. Thực hiện upload file (HTTP PUT) trực tiếp lên S3 Presigned URL. Xây dựng Giao diện Admin: Lấy danh sách sách có status PENDING. Có nút \u0026ldquo;Duyệt\u0026rdquo; (gọi API approveBook). Luồng Đọc \u0026amp; Tìm kiếm (Tuần 3-4) Mục tiêu là cho phép người dùng đọc và tìm kiếm sách đã được duyệt.\nTác vụ Backend (CDK/Lambda): Viết Lambda getReadUrl: Xác thực JWT (phải đăng nhập). Kiểm tra xem sách có status APPROVED không. Tạo Signed GET URL (ngắn hạn) qua CloudFront trỏ đến file trong public/books/. Cập nhật CDK: Thêm GSI (Global Secondary Index) cho title và author vào bảng DynamoDB. Viết Lambda searchBooks: Query DynamoDB dựa trên GSI (không dùng Scan). Tác vụ Frontend: Xây dựng Trang chủ: Hiển thị danh sách sách (từ API, không có URL). Xây dựng Thanh tìm kiếm (gọi API searchBooks). Xây dựng Giao diện Đọc sách (Reader): Khi bấm \u0026ldquo;Đọc\u0026rdquo;, gọi API getReadUrl. Dùng URL nhận được để render file (ví dụ: dùng react-pdf). Vận hành \u0026amp; Bảo mật (Tuần 5-6) Mục tiêu là \u0026ldquo;hóa cứng\u0026rdquo; hệ thống, làm cho nó an toàn và dễ giám sát.\nTác vụ Backend (CDK/Lambda): Thiết lập S3 Event Notification (cho uploads/). Viết Lambda validateMimeType: Trigger khi có file mới, đọc \u0026ldquo;magic bytes\u0026rdquo; để xác thực đúng là PDF/ePub. Nếu sai, cập nhật status: REJECTED_INVALID_TYPE. Viết Lambda takedownBook (API cho Admin) và deleteUpload (xóa file PENDING sau 72h). Tác vụ DevOps (AWS Console/CDK): Thiết lập AWS Budget Alerts (cảnh báo khi chi phí vượt $X). Thiết lập CloudWatch Alarms (ví dụ: Lambda error rate \u0026gt; 5%). Rà soát lại IAM (đảm bảo \u0026ldquo;least-privilege\u0026rdquo;), CORS (chỉ cho phép domain của Amplify). 6. Budget Estimation You can find the budget estimation on the: AWS Pricing Calculator\nDưới đây là ước tính chi phí hàng tháng nghiêm ngặt (giả định không áp dụng AWS Free Tier) tại quy mô MVP (100 người dùng).\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10 000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10 000 HTTP API calls/tháng 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10 000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/tháng 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Chi phí hạ tầng Mô hình chi phí này cho thấy sự hiệu quả của kiến trúc serverless: chi phí tập trung chủ yếu vào giá trị mang lại cho người dùng (Cognito MAU) thay vì trả tiền cho \u0026ldquo;máy chủ chờ\u0026rdquo; (idle servers).\n7. Đánh giá rủi ro Ma trận rủi ro Rủi ro Tác động Chiến lược giảm thiểu Chi phí tăng khi user đột biến Cao Giới hạn MAU, cache metadata qua CloudFront Upload lạm dụng Trung bình Giới hạn ≤ 50MB/file, xóa auto sau 72h File loại giả mạo/độc hại Trung bình S3 Event → Lambda xác thực MIME (magic bytes) Giám sát quá tải Thấp CloudWatch alert, log 14 ngày Chiến lược giảm thiểu Chi phí: Đặt AWS Budget Alerts cho CloudFront và Cognito. Nhận thức rằng Signed URL có TTL ngắn nên không cache công khai dài hạn; thay vào đó, cache metadata/API response (danh sách sách, chi tiết) trên CloudFront 3–5 phút để giảm tải API. Chỉ tạo Signed URL khi người dùng thực sự bấm đọc (on‑demand), không tạo sẵn cho cả danh sách. Tải lên: Giới hạn kích thước file ≤ 50MB cho MVP. (Có thể nâng lên 200MB khi cần, dùng multipart upload ở FE để tránh timeout.) Áp dụng Rate Limit/Throttling trên API Gateway cho các endpoint tạo Presigned URL. Thiết lập S3 Lifecycle Policy để tự động xóa file chưa duyệt ở uploads/ sau 72h. Thêm Server‑side Validation: S3 Event Notifications → Lambda đọc magic bytes (vd. thư viện file-type) để xác thực đúng PDF/ePub; nếu sai, tự động xóa và ghi trạng thái REJECTED_INVALID_TYPE vào DynamoDB. Bản quyền (DMCA): Lưu Audit Log trong DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp để phục vụ truy vết. Xây dựng Takedown API (chỉ Admin): cập nhật status TAKEDOWN; tùy chọn di chuyển object từ public/books/ sang quarantine/books/ (không xóa hẳn) để lưu vết. Kế hoạch ứng phó Nếu chi phí tăng vượt ngân sách, có thể tạm thời giới hạn người dùng mới thông qua hệ thống mời để kiểm soát MAU Cognito và tối ưu hóa file.\n8. Kết quả mong đợi Cải tiến kỹ thuật: Đảm bảo tốc độ truyền tải nhanh và bảo mật nội dung (CDN + Signed URL). Tạo ra một kiến trúc Serverless tiêu chuẩn trên AWS, dễ dàng mở rộng lên đến 50.000 người dùng mà không cần thay đổi kiến trúc cốt lõi. Hệ thống CI/CD hoàn toàn tự động cho cả Frontend và Backend (CDK/Amplify). Giá trị lâu dài Thiết lập một nền tảng dữ liệu tập trung và có cấu trúc cho nội dung sách. Cung cấp một tài liệu tham khảo sống về việc triển khai Serverless E2E. Khả năng tích hợp các dịch vụ phân tích (như Amazon QuickSight) hoặc AI/ML trong tương lai. Hệ thống này chứng minh khả năng xây dựng nền tảng nội dung bảo mật, tiết kiệm chi phí và mở rộng dễ dàng bằng AWS Serverless — phù hợp triển khai thực tế cho nhóm nhỏ. Rẻ\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/4.2-event2/",
	"title": "Sự Kiện 2 - AWS GenAI Builder Club",
	"tags": [],
	"description": "",
	"content": "AWS GenAI Builder Club: Vòng Đời Phát Triển Do AI Điều Khiển - Tái Tưởng Tượng Kỹ Thuật Phần Mềm Ngày \u0026amp; Giờ: Thứ Sáu, 3 tháng 10 năm 2025 | 14:00 (2:00 PM)\nĐịa Điểm: AWS Event Hall, L26 Tòa Nhà Bitexco, Thành phố Hồ Chí Minh\nGiảng Viên: Toàn Huỳnh \u0026amp; Mỹ Nguyễn\nĐiều Phối Viên: Diễm Mỹ, Đại Trương, Định Nguyễn\nTổng Quan Sự Kiện Phiên làm việc AWS GenAI Builder Club này khám phá Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC), một cách tiếp cận biến đổi đối với kỹ thuật phần mềm tích hợp AI như một cộng tác viên trung tâm trong toàn bộ quá trình phát triển. Phiên làm việc này có các bản trình diễn thực hành về Amazon Q Developer và Kiro, giới thiệu các ứng dụng thực tế của AI trong phát triển phần mềm hiện đại.\nChương Trình Giờ Phiên Giảng Viên 14:00 - 14:15 Chào Mừng - 14:15 - 15:30 Tổng Quan Vòng Đời Phát Triển Do AI Điều Khiển \u0026amp; Bản Trình Diễn Amazon Q Developer Toàn Huỳnh 15:30 - 15:45 Giải Lao - 15:45 - 16:30 Bản Trình Diễn Kiro Mỹ Nguyễn Các Khái Niệm \u0026amp; Bài Học Chính 1. Tổng Quan Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC) Triết Lý Cốt Lõi Vòng Đời Phát Triển Do AI Điều Khiển đại diện cho một sự thay đổi cơ bản trong cách phần mềm được xây dựng. Thay vì coi AI là một suy nghĩ sau hoặc công cụ hoàn thành mã đơn giản, AI-DLC nhúng AI như một đối tác thông minh trong toàn bộ quá trình phát triển.\nCác Nguyên Tắc Chính:\nBạn Kiểm Soát - AI là trợ lý của bạn, không phải người quản lý của bạn. Bạn phải duy trì quyền quyết định về hướng dự án và chi tiết triển khai.\nAI Là Cộng Tác Viên, Không Phải Thay Thế - AI nên đặt những câu hỏi quan trọng về yêu cầu, kiến trúc và mục tiêu dự án của bạn. Sự hợp tác nên là hai chiều, với bạn hướng dẫn các đề xuất của AI.\nLập Kế Hoạch Trước Triển Khai - Luôn tạo một kế hoạch toàn diện trước khi đi vào mã. AI có thể giúp tạo kế hoạch này, nhưng bạn phải xem xét, xác thực và tinh chỉnh nó.\nQuy Trình Phát Triển Bước 1: Tạo Kế Hoạch Dự Án\nXác định rõ ràng yêu cầu và phạm vi dự án Yêu cầu AI tạo kế hoạch dựa trên thông số kỹ thuật của bạn Xem xét kế hoạch một cách phê phán và yêu cầu sửa đổi Đảm bảo kế hoạch chi tiết và rõ ràng Bước 2: Chia Nhỏ Thành User Stories\nChuyển đổi kế hoạch thành user stories với tiêu chí chấp nhận rõ ràng Chia phạm vi lớn thành các đơn vị nhỏ hơn, dễ quản lý Mỗi đơn vị trở thành một dự án nhỏ có thể được giao cho các thành viên nhóm Ước tính thời gian cho mỗi đơn vị (nhưng cẩn thận với việc ước tính quá cao) Bước 3: Xác Định Ngăn Xếp Công Nghệ\nChỉ định rõ ràng các công nghệ, framework và công cụ sẽ được sử dụng Thay vì bảo AI \u0026ldquo;đừng triển khai cái này\u0026rdquo;, hãy bảo nó \u0026ldquo;triển khai theo cách này\u0026rdquo; Hướng dẫn tích cực mang lại tỷ lệ thành công cao hơn các ràng buộc tiêu cực Bước 4: Yêu Cầu \u0026amp; Thiết Kế Chi Tiết\nViết yêu cầu với độ chính xác và rõ ràng Hợp tác với AI để tạo các thông số kỹ thuật chi tiết Xác định các mô hình dữ liệu, hợp đồng API và kiến trúc hệ thống Tạo tài liệu thiết kế trước khi triển khai bắt đầu Bước 5: Triển Khai \u0026amp; Xác Minh\nTriển khai các tính năng theo kế hoạch Sử dụng cách tiếp cận phát triển mob (nhóm làm việc cùng nhau trên mã) Xác minh tất cả mã đầu ra như một nhóm Tiến hành đánh giá mã và kiểm tra chất lượng Bước 6: Kiểm Thử \u0026amp; Triển Khai\nDi chuyển qua các môi trường: Development (Dev) → Testing (QA) → User Acceptance Testing (UAT) → Production (Prod) Đảm bảo các cổng chất lượng ở mỗi giai đoạn Xác thực chức năng trước khi phát hành sản xuất Các Yếu Tố Thành Công Quan Trọng Tạo Kế Hoạch Trước - Đừng mong đợi AI xử lý mọi thứ. Luôn bắt đầu với một kế hoạch rõ ràng. Xem Xét Thường Xuyên - Liên tục xem xét các đề xuất và đầu ra của AI. Tỷ lệ lỗi cao là có thể. Bạn Là Người Quản Lý - Giá trị của bạn nằm ở xác thực mã và quản lý dự án, không phải viết từng dòng mã. Đặt Câu Hỏi Làm Rõ - Đảm bảo AI hiểu ngữ cảnh dự án của bạn bằng cách đặt những câu hỏi quan trọng về yêu cầu, kiến trúc và mục tiêu. Sử Dụng Mẫu Prompt - Tạo các prompt có cấu trúc bao gồm ngữ cảnh người dùng, user stories và yêu cầu cụ thể để nhận được phản hồi AI rõ ràng hơn. Xuất Kế Hoạch Thành Tệp - Yêu cầu AI tạo kế hoạch dưới dạng tệp bạn có thể lưu, xem xét và sửa đổi. Điều này tạo ra một tài liệu sống cho tham chiếu trong tương lai. Lịch Sự Với AI - Duy trì giao tiếp tôn trọng với các công cụ AI. Mối quan hệ tốt có thể giúp ích trong các tương tác trong tương lai (và đó chỉ là thực hành tốt!). 2. Bản Trình Diễn Amazon Q Developer Amazon Q Developer Là Gì? Amazon Q Developer là một trợ lý được hỗ trợ bởi AI biến đổi vòng đời phát triển phần mềm (SDLC) thông qua các khả năng tác nhân trên nhiều nền tảng:\nAWS Console - Giúp với cấu hình cơ sở hạ tầng và dịch vụ IDE (Integrated Development Environment) - Cung cấp các đề xuất tạo mã và tối ưu hóa CLI (Command Line Interface) - Hỗ trợ tạo lệnh và tự động hóa Các Nền Tảng DevSecOps - Tích hợp các thực tiễn bảo mật vào quy trình phát triển Các Khả Năng Chính Tạo Mã \u0026amp; Chất Lượng\nTăng tốc độ tạo mã với các đề xuất được hỗ trợ bởi AI Cải thiện chất lượng mã thông qua các khuyến nghị thông minh Duy trì tích hợp liền mạch với các quy trình công việc hiện có Hiểu các cơ sở mã phức tạp và đề xuất tối ưu hóa Tài Liệu \u0026amp; Kiểm Thử\nTự động tạo tài liệu toàn diện Tạo bài kiểm tra đơn vị với nỗ lực thủ công tối thiểu Cải thiện đáng kể khả năng bảo trì mã và độ tin cậy Giảm boilerplate và các tác vụ mã lặp lại Hợp Tác Thông Minh\nHoạt động như một cộng tác viên thông minh tận dụng các mô hình ngôn ngữ lớn Kết hợp kiến thức dịch vụ AWS sâu sắc với chuyên môn mã hóa Giúp các nhà phát triển tăng tốc độ các chu kỳ phát triển Nâng cao chất lượng mã và tăng cường tư thế bảo mật Tự Động Hóa Trên Toàn Bộ Vòng Đời Phát Triển\nTự động hóa các tác vụ thường xuyên trên toàn bộ vòng đời phát triển Giảm công việc thủ công, lặp lại Cho phép các nhà phát triển tập trung vào các tác vụ sáng tạo có giá trị cao hơn Cải thiện năng suất và hiệu quả tổng thể Thực Tiễn Tốt Nhất Khi Sử Dụng Amazon Q Developer Cung Cấp Ngữ Cảnh Rõ Ràng - Cung cấp cho Q thông tin chi tiết về dự án, kiến trúc và yêu cầu của bạn Sử Dụng Prompt Cụ Thể - Thay vì các yêu cầu mơ hồ, cung cấp các prompt cụ thể, chi tiết với các ví dụ Xem Xét Các Đề Xuất - Luôn xem xét các đề xuất của Q trước khi triển khai chúng Lặp Lại \u0026amp; Tinh Chỉnh - Nếu đề xuất đầu tiên không hoàn hảo, tinh chỉnh prompt của bạn và thử lại Tận Dụng Kiến Thức AWS - Tận dụng sự hiểu biết sâu sắc của Q về các dịch vụ AWS và thực tiễn tốt nhất 3. Bản Trình Diễn Kiro Kiro Là Gì? Kiro là một IDE tác nhân (Integrated Development Environment) được phát triển bởi Amazon Web Services lấp khoảng cách giữa việc tạo prototype nhanh được hỗ trợ bởi AI và phát triển phần mềm sẵn sàng cho sản xuất. Nó hiện đang ở giai đoạn xem trước công khai.\nTriết Lý Cốt Lõi Kiro thể hiện nguyên tắc rằng AI nên nâng cao năng suất của nhà phát triển trong khi duy trì các tiêu chuẩn chuyên nghiệp, cấu trúc rõ ràng, kiểm thử toàn diện, tài liệu và khả năng bảo trì lâu dài.\nCác Tính Năng Chính Phát Triển Theo Đặc Tả\nKhi bạn gửi yêu cầu (ví dụ: \u0026ldquo;thêm hệ thống đánh giá sản phẩm\u0026rdquo;), Kiro chuyển đổi nó thành: User stories với tiêu chí chấp nhận rõ ràng Tài liệu thiết kế Danh sách tác vụ và kế hoạch triển khai Thông số kỹ thuật có cấu trúc trước khi tạo mã Agent Hooks \u0026amp; Tự Động Hóa\nTự động kích hoạt các tác vụ dựa trên các sự kiện: Lưu tệp kích hoạt cập nhật tài liệu Commit kích hoạt tạo bài kiểm tra Các hành động cụ thể kích hoạt tối ưu hóa hiệu năng Giảm công việc thủ công, lặp lại Steering \u0026amp; Ngữ Cảnh Dự Án\nTạo các tệp steering (markdown) để mô tả: Cấu trúc và tổ chức dự án Tiêu chuẩn mã hóa và quy ước Các mô hình kiến trúc mong muốn Hướng dẫn nhóm và thực tiễn tốt nhất Giúp Kiro hiểu sâu sắc ngữ cảnh dự án của bạn Phân Tích Đa Tệp \u0026amp; Hiểu Ý Định\nPhân tích nhiều tệp đồng thời Hiểu các mục tiêu chức năng trên toàn bộ cơ sở mã Thực hiện các thay đổi phù hợp với các mục tiêu dự án tổng thể Vượt ra ngoài hoàn thành mã đơn giản Tích Hợp VS Code\nĐược xây dựng dựa trên nền tảng mã nguồn mở của VS Code Nhập cài đặt, chủ đề và tiện ích mở rộng từ VS Code Giao diện quen thuộc cho người dùng VS Code hiện có Chuyển đổi liền mạch cho các nhà phát triển Lựa Chọn Mô Hình AI Linh Hoạt\nHiện sử dụng Claude Sonnet 4 làm mặc định Chế độ \u0026ldquo;Auto\u0026rdquo; kết hợp nhiều mô hình dựa trên ngữ cảnh Cân bằng giữa chất lượng và chi phí Linh hoạt để chọn các mô hình khác nhau cho các tác vụ khác nhau Ưu Điểm Của Việc Sử Dụng Kiro Tăng Tính Minh Bạch \u0026amp; Kiểm Soát\nBắt đầu với các thông số kỹ thuật trước khi tạo mã Xem xét và xác thực các thông số kỹ thuật trước khi triển khai Giảm mã \u0026ldquo;ảo tưởng\u0026rdquo; hoặc triển khai không phù hợp Duy trì khả năng truy xuất rõ ràng từ yêu cầu đến mã Giảm Boilerplate \u0026amp; Các Tác Vụ Lặp Lại\nAgent hooks tự động hóa tạo tài liệu Tạo bài kiểm tra đơn vị tự động Cập nhật thông tin tự động Giải phóng các nhà phát triển cho công việc có giá trị cao hơn Bảo Mật \u0026amp; Quyền Riêng Tư\nHầu hết các hoạt động mã xảy ra cục bộ Dữ liệu chỉ được gửi bên ngoài với sự cho phép rõ ràng Duy trì kiểm soát thông tin nhạy cảm Khả Năng Mở Rộng \u0026amp; Linh Hoạt\nTích hợp các công cụ bên ngoài thông qua MCP (Model Context Protocol) Hỗ trợ nhiều mô hình AI Không bị ràng buộc vào một môi trường AI duy nhất Thích ứng với các quy trình làm việc nhóm khác nhau Hạn Chế \u0026amp; Cân Nhắc Trạng Thái Xem Trước - Vẫn ở giai đoạn xem trước công khai; tính ổn định và tính năng có thể thay đổi Các Dự Án Phức Tạp - Có thể gặp khó khăn trong việc hiểu ngữ cảnh sâu sắc trong các dự án rất phức tạp Cần Giám Sát - Người dùng vẫn cần giám sát và xác thực các quyết định của AI Giá Cả Trong Tương Lai - Các tầng giá dự kiến: Miễn phí: ~50 tác vụ/tháng Pro: ~1.000 tác vụ/tháng Pro+: ~3.000 tác vụ/tháng Khi Nào Nên Sử Dụng Kiro Bạn muốn một quy trình làm việc AI + lập trình duy trì tính chuyên nghiệp và cấu trúc rõ ràng Xây dựng prototype nhanh nhưng lo ngại về tính bền vững sản xuất Khám phá cách AI có thể trở thành một đồng nghiệp lập trình thực sự, không chỉ là công cụ gợi ý mã Bạn cần phát triển theo đặc tả với tài liệu và kiểm thử tự động Các Lỗi Thường Gặp Khi Sử Dụng AI Trong Phát Triển 1. Mong Đợi AI Xử Lý Mọi Thứ Vấn Đề: Nhiều nhà phát triển mong đợi AI hoàn thành toàn bộ dự án một cách tự chủ.\nGiải Pháp: Luôn tạo kế hoạch trước và xem xét thường xuyên. AI là công cụ để nâng cao năng suất, không phải thay thế phán đoán của nhà phát triển.\n2. Tỷ Lệ Lỗi Cao Vấn Đề: AI có thể mắc lỗi, đặc biệt là trong các tình huống phức tạp.\nGiải Pháp: Triển khai các chu kỳ xem xét thường xuyên. Xác thực tất cả mã do AI tạo trước khi triển khai.\n3. Thiếu Yêu Cầu Rõ Ràng Vấn Đề: Yêu cầu mơ hồ hoặc không rõ ràng dẫn đến đầu ra AI mơ hồ.\nGiải Pháp: Viết yêu cầu với độ chính xác. Hợp tác với AI để tạo các thông số kỹ thuật chi tiết trước khi triển khai.\n4. Ràng Buộc Tiêu Cực Thay Vì Hướng Dẫn Tích Cực Vấn Đề: Bảo AI \u0026ldquo;đừng làm cái này\u0026rdquo; ít hiệu quả hơn \u0026ldquo;làm cái này\u0026rdquo;.\nGiải Pháp: Sử dụng các hướng dẫn tích cực, cụ thể. Tỷ lệ thành công cao hơn đến từ hướng dẫn tích cực rõ ràng.\n5. Ngữ Cảnh Dự Án Không Đủ Vấn Đề: AI không hiểu các yêu cầu và ràng buộc độc đáo của dự án của bạn.\nGiải Pháp: Tạo các tệp steering, cung cấp ngữ cảnh chi tiết và đặt những câu hỏi quan trọng cho AI về dự án của bạn.\n6. Coi AI Là Người Quản Lý Vấn Đề: Để AI quyết định tất cả về hướng dự án và kiến trúc.\nGiải Pháp: Nhớ: Bạn là người quản lý. Giá trị của bạn nằm ở xác thực mã và giám sát dự án, không phải viết từng dòng mã.\nNhững Điểm Chính Rút Ra AI Là Trợ Lý Của Bạn - Duy trì kiểm soát các quyết định dự án và hướng triển khai\nLập Kế Hoạch Trước, Mã Sau - Luôn tạo kế hoạch toàn diện trước khi triển khai\nHợp Tác Hơn Tự Động Hóa - AI nên đặt câu hỏi và hợp tác, không chỉ thực thi lệnh\nYêu Cầu Rõ Ràng Quan Trọng - Độ chính xác trong yêu cầu dẫn đến đầu ra AI tốt hơn\nXem Xét Thường Xuyên Là Cần Thiết - Đừng mong đợi AI hoàn hảo; xem xét và xác thực liên tục\nBạn Là Người Quản Lý Mã - Giá trị của bạn nằm ở xác thực và giám sát, không phải viết từng dòng\nSử Dụng Prompt Có Cấu Trúc - Các mẫu với ngữ cảnh, user stories và yêu cầu mang lại kết quả tốt hơn\nXuất Kế Hoạch Thành Tệp - Tạo các tài liệu sống bạn có thể tham chiếu và sửa đổi\nHướng Dẫn Tích Cực Hiệu Quả Hơn - Bảo AI phải làm gì, không phải tránh làm gì\nKinh Nghiệm Quan Trọng - Sử dụng các công cụ này thực hành để hiểu khả năng và hạn chế của chúng\nCông Cụ \u0026amp; Tài Nguyên Được Đề Xuất Amazon Q Developer - Trợ lý phát triển được hỗ trợ bởi AI tích hợp với các dịch vụ AWS Kiro IDE - Môi trường phát triển theo đặc tả với hợp tác AI AWS CodeWhisperer - Công cụ tạo mã và tối ưu hóa MCP (Model Context Protocol) - Khung công tác để tích hợp các công cụ và dịch vụ bên ngoài Kết Luận Vòng Đời Phát Triển Do AI Điều Khiển đại diện cho một mô hình mới trong kỹ thuật phần mềm nơi AI và con người hợp tác như những người bình đẳng. Thành công đòi hỏi lập kế hoạch rõ ràng, xem xét thường xuyên, yêu cầu chính xác và duy trì kiểm soát của nhà phát triển đối với hướng dự án. Các công cụ như Amazon Q Developer và Kiro đang cho phép quy trình làm việc mới này, nhưng chúng hoạt động tốt nhất khi các nhà phát triển hiểu khả năng và hạn chế của chúng, và duy trì vai trò của họ là người quản lý dự án và xác thực mã.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Tạo một S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "Trong phần này, bạn sẽ tạo và kiểm tra Interface Endpoint S3 bằng cách sử dụng môi trường truyền thống mô phỏng.\nQuay lại Amazon VPC menu. Trong thanh điều hướng bên trái, chọn Endpoints, sau đó click Create Endpoint.\nTrong Create endpoint console:\nĐặt tên interface endpoint Trong Service category, chọn aws services Trong Search box, gõ S3 và nhấn Enter. Chọn endpoint có tên com.amazonaws.us-east-1.s3. Đảm bảo rằng cột Type có giá trị Interface. Đối với VPC, chọn VPC Cloud từ drop-down. Đảm bảo rằng bạn chọn \u0026ldquo;VPC Cloud\u0026rdquo; và không phải \u0026ldquo;VPC On-prem\u0026rdquo;\nMở rộng Additional settings và đảm bảo rằng Enable DNS name không được chọn (sẽ sử dụng điều này trong phần tiếp theo của workshop) Chọn 2 subnets trong AZs sau: us-east-1a and us-east-1b Đối với Security group, chọn SGforS3Endpoint: Giữ default policy - full access và click Create endpoint Chúc mừng bạn đã tạo thành công S3 interface endpoint. Ở bước tiếp theo, chúng ta sẽ kiểm tra interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.2-blog2/",
	"title": "Thông báo Amazon EC2 M4 và M4 Pro Mac instances",
	"tags": [],
	"description": "",
	"content": "Thông báo Amazon EC2 M4 và M4 Pro Mac instances bởi Sébastien Stormacq vào ngày 12 THÁNG 9 2025 trong Amazon EC2 Mac Instances, Launch, News Permalink Comments\nPermalink Comments Share\nVoiced by Polly\nLà người đã sử dụng macOS từ năm 2001 và các Amazon EC2 Mac instances từ khi chúng ra mắt 4 năm trước, tôi đã giúp nhiều khách hàng mở rộng các pipeline tích hợp \u0026amp; phân phối liên tục (CI/CD) trên AWS. Hôm nay, tôi rất hào hứng chia sẻ rằng các instance Amazon EC2 M4 và M4 Pro Mac hiện đã khả dụng chính thức.\nCác nhóm phát triển xây ứng dụng cho các nền tảng Apple cần tài nguyên tính toán mạnh để xử lý các quy trình build phức tạp và chạy nhiều giả lập iOS cùng lúc. Khi các dự án phát triển ngày càng lớn và tinh vi, các nhóm cần hiệu năng và dung lượng bộ nhớ cao hơn để duy trì chu kỳ phát triển nhanh.\nApple M4 Mac mini làm lõi Các instance EC2 M4 Mac (được gọi là mac-m4.metal trong API) được xây dựng dựa trên máy Apple M4 Mac mini và sử dụng hệ thống AWS Nitro System. Chúng có chip Apple silicon M4 với 10 lõi CPU (bốn lõi hiệu năng và sáu lõi hiệu quả), GPU 10 lõi, Neural Engine 16 lõi, và bộ nhớ hợp nhất 24 GB, mang lại hiệu năng cải thiện cho các workload build ứng dụng iOS và macOS. Khi xây dựng và kiểm thử ứng dụng, các instance M4 Mac cho hiệu năng build ứng dụng tốt hơn tới 20% so với các instance EC2 M2 Mac.\nInstance EC2 M4 Pro Mac ( mac-m4pro.metal trong API ) được trang bị chip Apple silicon M4 Pro với 14 lõi CPU, 20 lõi GPU, Neural Engine 16 lõi và bộ nhớ hợp nhất 48 GB. Những instance này cung cấp hiệu năng build ứng dụng tốt hơn tới 15% so với các instance EC2 M2 Pro Mac. Thêm dung lượng bộ nhớ và công suất tính toán cho phép chạy nhiều bài kiểm thử song song bằng nhiều giả lập thiết bị.\nMỗi instance M4 và M4 Pro Mac giờ đây đi kèm với 2 TB lưu trữ nội bộ (local storage), cung cấp lưu trữ độ trễ thấp để cải thiện caching và hiệu năng build \u0026amp; test.\nCả hai loại instance đều hỗ trợ macOS Sonoma phiên bản 15.6 và mới hơn như các AMI (Amazon Machine Images (AMIs).). Hệ thống AWS Nitro cung cấp băng thông mạng Amazon Virtual Private Cloud (Amazon VPC) lên đến 10 Gbps và băng thông lưu trữ Amazon Elastic Block Store (Amazon EBS) 8 Gbps qua kết nối Thunderbolt tốc độ cao.\nCác instance EC2 Mac tích hợp liền mạch với các dịch vụ AWS, nghĩa là bạn có thể:\nXây dựng pipeline CI/CD tự động sử dụng AWS CodeBuild và AWS CodePipeline\nLưu trữ và quản lý nhiều phiên bản bí mật build của bạn, như chứng chỉ phát triển Apple và khóa, trên AWS Secrets Manager\nQuản lý hạ tầng phát triển của bạn bằng AWS CloudFormation\nGiám sát hiệu năng instance với Amazon CloudWatch\nCách bắt đầu Bạn có thể khởi chạy một instance EC2 M4 hoặc M4 Pro Mac qua AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs.\nVí dụ trong demo này, tôi sẽ khởi động một instance M4 Pro từ console. Tôi đầu tiên cấp phát một dedicated host để chạy các instance của mình. Trên AWS Management Console tôi vào EC2, rồi Dedicated Hosts, và chọn Allocate Dedicated Host.\nRồi, tôi nhập tag Name và chọn Family instance (mac‑m4pro) và loại instance (mac‑m4pro.metal). Tôi chọn một Availability Zone và bỏ chọn Host maintenance.\nEC2 Mac M$ – Dedicated hosts\nHoặc tôi có thể dùng CLI:\naws ec2 allocate-hosts \\\n--availability-zone-id \u0026ldquo;usw2-az4\u0026rdquo; \\\n--auto-placement \u0026ldquo;off\u0026rdquo; \\\n--host-recovery \u0026ldquo;off\u0026rdquo; \\\n--host-maintenance \u0026ldquo;off\u0026rdquo; \\\n--quantity 1 \\\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo;\nSau khi host dedicated được cấp cho tài khoản của tôi, tôi chọn host vừa cấp, rồi chọn menu Actions và chọn Launch instance(s) onto host.\nLưu ý console cung cấp cho bạn, bên cạnh các thông tin khác, các phiên bản macOS hỗ trợ mới nhất cho loại host này. Trong trường hợp này, là macOS 15.6.\nTrên trang Launch an instance, tôi nhập Name. Tôi chọn một AMI macOS Sequoia. Tôi đảm bảo Architecture là Arm 64-bit và loại instance là mac-m4pro.metal.\nPhần còn lại các tham số không đặc thù cho EC2 Mac: cấu hình mạng và lưu trữ. Khi khởi động một instance dùng cho phát triển, hãy chắc chọn volume tối thiểu 200 GB trở lên. Volume mặc định 100 GB không đủ để tải xuống và cài Xcode.\nKhi đã sẵn sàng, tôi nhấn nút Launch instance màu cam ở cuối trang. Instance sẽ nhanh chóng xuất hiện ở trạng thái Running trong console. Tuy nhiên, có thể mất tới 15 phút để bạn có thể kết nối qua SSH.\nHoặc tôi có thể dùng lệnh này:\naws ec2 run-instances \\\n--image-id \u0026ldquo;ami-000420887c24e4ac8\u0026rdquo; \\ # ID AMI tùy vùng !\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo; \\\n--key-name \u0026ldquo;my-ssh-key-name\u0026rdquo; \\\n--network-interfaces \u0026lsquo;{\u0026ldquo;AssociatePublicIpAddress\u0026rdquo;:true,\u0026ldquo;DeviceIndex\u0026rdquo;:0,\u0026ldquo;Groups\u0026rdquo;:[\u0026ldquo;sg-0c2f1a3e01b84f3a3\u0026rdquo;]}\u0026rsquo; \\ # Security Group ID phụ thuộc config của bạn \\\n--tag-specifications \u0026lsquo;{\u0026ldquo;ResourceType\u0026rdquo;:\u0026ldquo;instance\u0026rdquo;,\u0026ldquo;Tags\u0026rdquo;:[{\u0026ldquo;Key\u0026rdquo;:\u0026ldquo;Name\u0026rdquo;,\u0026ldquo;Value\u0026rdquo;:\u0026ldquo;My Dev Server\u0026rdquo;}]}\u0026rsquo; \\\n--placement \u0026lsquo;{\u0026ldquo;HostId\u0026rdquo;:\u0026ldquo;h-0e984064522b4b60b\u0026rdquo;,\u0026ldquo;Tenancy\u0026rdquo;:\u0026ldquo;host\u0026rdquo;}\u0026rsquo; \\ # Host ID tùy config của bạn --private-dns-name-options \u0026lsquo;{\u0026ldquo;HostnameType\u0026rdquo;:\u0026ldquo;ip-name\u0026rdquo;,\u0026ldquo;EnableResourceNameDnsARecord\u0026rdquo;:true,\u0026ldquo;EnableResourceNameDnsAAAARecord\u0026rdquo;:false}\u0026rsquo; \\\n--count \u0026ldquo;1\u0026rdquo;\nCài Xcode từ Terminal Sau khi instance có thể truy cập, tôi có thể kết nối bằng SSH và cài công cụ phát triển. Tôi dùng xcodeinstall để tải và cài Xcode 16.4.\nTừ laptop của tôi, tôi mở session với credentials Apple developer:\n# on my laptop, with permissions to access AWS Secret Manager\n» xcodeinstall authenticate -s eu-central-1\nRetrieving Apple Developer Portal credentials\u0026hellip;\nAuthenticating\u0026hellip;\n🔐 Two factors authentication is enabled, enter your 2FA code: 067785\n✅ Authenticated with MFA.\nTôi kết nối với EC2 Mac instance cái mà tôi vừa mới launched. Sau đó, tôi tải và cài đặc Xcode:\n» ssh ec2-user@44.234.115.119\nWarning: Permanently added \u0026lsquo;44.234.115.119\u0026rsquo; (ED25519) to the list of known hosts.\nLast login: Sat Aug 23 13:49:55 2025 from 81.49.207.77\n┌───┬──┐ \\_\\_| \\_\\_|\\_ ) │ ╷╭╯╷ │ \\_| ( / │ └╮ │ \\_\\_\\_|\\\\\\_\\_\\_|\\_\\_\\_| │ ╰─┼╯ │ Amazon EC2 └───┴──┘ macOS Sequoia 15.6\rec2-user@ip-172-31-54-74 ~ % brew tap sebsto/macos\n==\u0026gt; Tapping sebsto/macos\nCloning into \u0026lsquo;/opt/homebrew/Library/Taps/sebsto/homebrew-macos\u0026rsquo;\u0026hellip;\nremote: Enumerating objects: 227, done.\nremote: Counting objects: 100% (71/71), done.\nremote: Compressing objects: 100% (57/57), done.\nremote: Total 227 (delta 22), reused 63 (delta 14), pack-reused 156 (from 1)\nReceiving objects: 100% (227/227), 37.93 KiB | 7.59 MiB/s, done.\nResolving deltas: 100% (72/72), done.\nTapped 1 formula (13 files, 61KB).\nec2-user@ip-172-31-54-74 ~ % brew install xcodeinstall\n==\u0026gt; Fetching downloads for: xcodeinstall\n==\u0026gt; Fetching sebsto/macos/xcodeinstall\n==\u0026gt; Downloading https://github.com/sebsto/xcodeinstall/releases/download/v0.12.0/xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\nAlready downloaded: /Users/ec2-user/Library/Caches/Homebrew/downloads/9f68a7a50ccfdc479c33074716fd654b8528be0ec2430c87bc2b2fa0c36abb2d\u0026ndash;xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\n==\u0026gt; Installing xcodeinstall from sebsto/macos\n==\u0026gt; Pouring xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\n🍺 /opt/homebrew/Cellar/xcodeinstall/0.12.0: 8 files, 55.2MB\n==\u0026gt; Running `brew cleanup xcodeinstall`\u0026hellip;\nDisable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\nHide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n==\u0026gt; No outdated dependents to upgrade!\nec2-user@ip-172-31-54-74 ~ % xcodeinstall download -s eu-central-1 -f -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nDownloading Xcode 16.4\n100% [============================================================] 2895 MB / 180.59 MBs\n[ OK ]\n✅ Xcode 16.4.xip downloaded\nec2-user@ip-172-31-54-74 ~ % xcodeinstall install -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nInstalling\u0026hellip;\n[1/6] Expanding Xcode xip (this might take a while)\n[2/6] Moving Xcode to /Applications\n[3/6] Installing additional packages\u0026hellip; XcodeSystemResources.pkg\n[4/6] Installing additional packages\u0026hellip; CoreTypes.pkg\n[5/6] Installing additional packages\u0026hellip; MobileDevice.pkg\n[6/6] Installing additional packages\u0026hellip; MobileDeviceDevelopment.pkg\n[ OK ]\n✅ file:///Users/ec2-user/.xcodeinstall/download/Xcode%2016.4.xip installed\nec2-user@ip-172-31-54-74 ~ % sudo xcodebuild -license accept\nec2-user@ip-172-31-54-74 ~ %\nNhững điều cần biết Chọn volume EBS tối thiểu 200 GB cho mục đích phát triển. Volume mặc định 100 GB không đủ để cài Xcode. Tôi thường chọn 500 GB. Khi tăng kích thước EBS sau khi instance đã khởi chạy, nhớ to resize the APFS filesystem.\nNgoài ra, bạn có thể chọn cài công cụ phát triển và framework của bạn lên ổ SSD nội bộ 2 TB độ trễ thấp có sẵn trong Mac mini. Lưu ý rằng nội dung volume này gắn với vòng đời instance, không với dedicated host. Nghĩa là mọi thứ sẽ bị xóa khỏi ổ SSD nội bộ khi bạn dừng và khởi động lại instance.\nCác instance mac-m4.metal và mac-m4pro.metal hỗ trợ macOS Sequoia 15.6 và các phiên bản mới hơn.\nBạn có thể di chuyển các instance EC2 Mac hiện tại khi instance di chuyển đang chạy macOS 15 (Sequoia). Tạo một AMI tùy chỉnh từ instance hiện tại và khởi động một instance M4 hoặc M4 Pro từ AMI đó.\nCuối cùng, tôi gợi ý bạn xem các hướng dẫn tôi viết để giúp bạn bắt đầu với EC2 Mac:\nKhởi động một instance EC2 Mac\nKết nối tới instance EC2 Mac (tôi chỉ bạn ba cách khác nhau để kết nối)\nXây ứng dụng nhanh hơn với pipeline CI/CD trên EC2 Mac\nGiá cả và khả dụng Các instance EC2 M4 và M4 Pro Mac hiện có tại US East (N. Virginia) và US West (Oregon), dự kiến mở rộng sang các vùng khác trong tương lai.\nCác instance EC2 Mac có thể mua dưới dạng Dedicated Hosts theo mô hình giá On-Demand và Savings Plans. Việc tính phí cho EC2 Mac là theo giây với mức tối thiểu 24 giờ cấp phát để tuân theo Thỏa thuận Bản quyền phần mềm macOS của Apple. Sau khoảng thời gian tối thiểu 24 giờ, host có thể được giải phóng bất cứ lúc nào mà không cần cam kết tiếp.\nLà người làm việc chặt với các nhà phát triển Apple, tôi tò mò xem bạn sẽ dùng các instance mới này như thế nào để tăng tốc chu kỳ phát triển của bạn. Sự kết hợp giữa hiệu năng gia tăng, dung lượng bộ nhớ cải thiện và tích hợp với dịch vụ AWS mở ra nhiều khả năng mới cho các đội xây ứng dụng cho iOS, macOS, iPadOS, tvOS, watchOS, và visionOS. Ngoài phát triển ứng dụng, Neural Engine của Apple silicon khiến các instance này là ứng viên hiệu quả chi phí để chạy workload inference machine learning (ML). Tôi sẽ thảo luận chi tiết chủ đề này tại AWS re:Invent 2025, nơi tôi sẽ chia sẻ benchmark và best practices để tối ưu workload ML trên EC2 Mac.\nĐể tìm hiểu thêm về các instance EC2 M4 và M4 Pro Mac, bạn có thể truy cập trang Amazon EC2 Mac Instances hoặc tham khảo EC2 Mac documentation. Bạn có thể bắt đầu sử dụng các instance này ngay hôm nay để hiện đại hóa workflow phát triển Apple trên AWS.\n— seb\nGiới thiệu tác giả Sébastien Stormacq\rSeb đã viết code từ khi chạm Commodore 64 vào những năm tám mươi. Anh truyền cảm hứng cho cộng đồng xây dựng khai phá giá trị của AWS Cloud bằng sự kết hợp giữa đam mê, nhiệt huyết, advocacy khách hàng, tò mò và sáng tạo. Các mối quan tâm của anh là kiến trúc phần mềm, công cụ dành cho nhà phát triển và điện toán di động. Nếu muốn bán thứ gì đó cho Seb, hãy chắc chắn rằng nó có API. Theo dõi @sebsto trên Bluesky, X, Mastodon và các nền tảng khác.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/",
	"title": "Tuần 2 - Dịch vụ Mạng trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-15 đến 2025-09-19\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 2 Tuần này đào sâu các dịch vụ mạng của AWS, từ VPC cơ bản đến giải pháp kết nối nâng cao và cân bằng tải.\nNội dung chính Amazon VPC và Subnet. Security Group và Network ACL. Internet Gateway, NAT Gateway. VPC Peering và AWS Transit Gateway. Elastic Load Balancing (ALB, NLB, GWLB). Labs thực hành Lab 03: Amazon VPC \u0026amp; Networking Basics. Lab 10: Hybrid DNS (Route 53 Resolver). Lab 19: VPC Peering. Lab 20: AWS Transit Gateway. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/3.3-blog3/",
	"title": "Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD bởi Suyash Nadkarni và Dylan Souvage vào ngày 12 THÁNG 9 2025 trong Amazon EC2, Best Practices, Expert (400), Technical\nKhi các tổ chức di chuyển nhiều khối lượng công việc quan trọng sang đám mây, tối ưu hóa về giá — hiệu suất (price-performance) trở thành một cân nhắc chủ chốt. Các instance Amazon Elastic Compute Cloud(Amazon EC2) sử dụng bộ xử lý AMD EPYC đem lại mật độ lõi cao, băng thông bộ nhớ lớn và các tính năng bảo mật được hỗ trợ phần cứng, khiến chúng trở thành một lựa chọn mạnh mẽ cho nhiều loại khối lượng công việc tính toán, bộ nhớ hoặc I/O. Trong bài viết này, chúng tôi giải thích cách chọn loại instance Amazon EC2 dựa trên AMD phù hợp và mô tả các kỹ thuật điều chỉnh có thể giúp người dùng cải thiện hiệu quả khối lượng công việc. Cho dù bạn đang chạy mô phỏng, phân tích quy mô lớn hoặc các khối lượng inference, bài viết này cung cấp hướng dẫn thực tiễn để tối ưu hóa instance Amazon EC2 dùng AMD.\nAmazon EC2 cung cấp instances AMD dựa trên nhiều thế hệ AMD EPYC. Bài viết tập trung vào chiến lược tối ưu cho thế hệ 3 và 4, vốn tăng cường khả năng cho workload tính toán và bộ nhớ chuyên sâu.\nThế hệ 3 (M6a, R6a, C6a, Hpc6a): Cân bằng tính toán, bộ nhớ và lưu trữ — phù hợp với phân tích dữ liệu, máy chủ web và tính toán hiệu năng cao.\nThế hệ thứ 4 (M7a, R7a, C7a, Hpc7a): Cung cấp hiệu suất tốt hơn tới 50% so với các thế hệ AMD trước đó. Các instance này giới thiệu hỗ trợ AVX‑512, bộ nhớ DDR5 và Simultaneous Multithreading (SMT) bị tắt; SMT là công nghệ cho phép một lõi vật lý chạy nhiều luồng cùng lúc; với SMT bị tắt, mỗi vCPU (virtual CPU) ánh xạ trực tiếp đến một lõi vật lý, điều này có thể cải thiện tính cô lập và nhất quán trong khối lượng công việc.\nChọn loại instance Amazon EC2 dùng AMD EPYC phù hợp Việc chọn loại instance Amazon EC2 dùng AMD EPYC phù hợp bắt đầu bằng việc hiểu cách ứng dụng của bạn sử dụng tài nguyên tính toán (compute), bộ nhớ, lưu trữ và mạng. Mỗi instance family được tối ưu hóa cho những đặc tính khối lượng công việc nhất định.\nKhối lượng công việc tính toán\nNhững khối lượng này liên quan tới các phép tính quy mô lớn, mô phỏng, hoặc mã hóa, và thường cần thông lượng CPU cao và hỗ trợ tập lệnh nâng cao.\nKhuyến nghị: C7a, Hpc7a, C6a, Hpc6a\nTình huống dùng: điện toán khoa học, mô hình tài chính, chuyển mã media, mã hóa, inference ML\nBig Data \u0026amp; Analytics\nỨng dụng xử lý và phân tích tập dữ liệu lớn hưởng lợi từ băng thông bộ nhớ cao và tỷ lệ tính toán‑bộ nhớ cân bằng.\nKhuyến nghị: R7a, M7a, R6a, M6a\nTình huống dùng: xử lý luồng, phân tích thời gian thực, công cụ business intelligence, caching phân tán\nKhối lượng công việc cơ sở dữ liệu\nCông việc database thường cần hiệu suất bộ nhớ ổn định và throughput I/O cao cho các hoạt động đọc/ghi.\nKhuyến nghị: R7a, M7a, R6a, M6a\nTình huống dùng: database quan hệ (MySQL, PostgreSQL), NoSQL (MongoDB, Cassandra), database trong bộ nhớ (Redis)\nWeb và máy chủ ứng dụng\nNhững ứng dụng này xử lý các tải yêu cầu biến đổi và hưởng lợi từ sự cân bằng giữa tính toán, bộ nhớ và hiệu năng mạng.\nKhuyến nghị: C7a, M7a, C6a, M6a\nTình huống dùng: máy chủ web, hệ quản lý nội dung, nền tảng e‑commerce, các điểm cuối API\nAI/ML trên CPU\nCác tác vụ ML không cần GPU — như inference hoặc tiền xử lý — có thể chạy hiệu quả trên các instance dựa CPU.\nKhuyến nghị: M7a, R7a, C7a\nTình huống dùng: inference mô hình, xử lý ngôn ngữ tự nhiên, thị giác máy tính, hệ gợi ý\nHigh Performance Computing\nNhững khối lượng công việc này cần nhiều lõi, băng thông bộ nhớ cao và mạng độ trễ thấp cho các tính toán liên kết chặt chẽ.\nKhuyến nghị: Hpc7a, Hpc6a, R7a, M7a\nTình huống dùng: động lực chất lỏng, genomics, phân tích địa chấn, mô phỏng kỹ thuật\nViệc phù hợp hóa loại instance với nhu cầu khối lượng công việc giúp cung cấp hiệu suất dự đoán được và hiệu quả chi phí. Các dịch vụ như Amazon EC2 Auto Scaling và AWS Compute Optimizer có thể hỗ trợ trong việc lựa chọn instance và ra quyết định scale liên tục.\nTối ưu hóa các instance Amazon EC2 dùng AMD EPYC Các instance EC2 dùng bộ xử lý AMD EPYC thế hệ thứ 4 vận hành với kiến trúc “chiplet modular”, như minh họa trong hình dưới đây. Mỗi bộ xử lý bao gồm nhiều Core Complex Dies (CCD), và mỗi CCD chứa một hoặc nhiều tổ hợp lõi (core complexes, gọi là CCX). Một CCX gom tối đa tám lõi vật lý, mỗi lõi có 1 MB bộ nhớ đệm L2 riêng và tám lõi đó cùng chia sẻ 32 MB bộ nhớ đệm L3. Các CCD này được kết nối với một die I/O trung tâm, chịu trách nhiệm quản lý bộ nhớ và liên kết giữa các chip.\n(Biểu đồ 1: Sơ đồ của die CPU ‘Zen 4’ với 8 lõi mỗi die)\nKiến trúc module của các bộ xử lý AMD EPYC thế hệ thứ 4 cho phép các instance như m7a.24xlarge và m7a.48xlarge hỗ trợ số lượng lõi cao — lên đến 96 lõi vật lý mỗi socket. Ví dụ:\nm7a.24xlarge cung cấp 96 lõi vật lý từ một socket đơn\nm7a.48xlarge trải rộng hai socket, cung cấp 192 lõi vật lý\nHiểu cách các kích cỡ instance của EC2 ánh xạ tới bố cục bộ xử lý vật lý có thể giúp bạn tối ưu hóa hiệu suất và tính nhất quán bộ nhớ đệm (cache). Những khối lượng công việc liên quan tới truy cập bộ nhớ chia sẻ hoặc đồng bộ hóa luồng — như HPC hoặc database trong bộ nhớ — có thể hưởng lợi khi chọn kích cỡ instance giảm thiểu giao tiếp giữa socket và tận dụng hiệu quả bộ nhớ đệm L3.\n(Biểu đồ 2: Bố cục CPU ‘EPYC Chiplet’)\nCác instance EC2 dùng AMD EPYC thế hệ thứ 4 hoạt động với SMT bị tắt. Trong cấu hình này, mỗi vCPU ánh xạ trực tiếp tới một lõi vật lý, loại bỏ chia sẻ tài nguyên như đơn vị thực thi và bộ nhớ đệm giữa các luồng “chị/em”. Thiết kế này có thể giảm nhiễu nội lõi và giúp cung cấp hiệu suất ổn định hơn cho các khối lượng công việc nhất định. Người dùng có thể cô lập luồng ở cấp lõi và quan sát độ biến thiên thấp hơn và thông lượng ổn định hơn cho các khối lượng như HPC, inference ML và database giao dịch.\nCPU optimizations Các công cụ như htop giúp xác định mẫu sử dụng CPU, trung bình tải hệ thống, và tiêu thụ tài nguyên theo tiến trình. Việc sử dụng CPU nên được đánh giá trong ngữ cảnh khối lượng công việc và yêu cầu hiệu năng. Nếu mức sử dụng liên tục đạt 100%, điều đó có thể chỉ ra rằng khối lượng công việc bị CPU-bound và chưa được cân bằng tối ưu. Trước khi thay đổi kích thước instance, bật Auto Scaling, hoặc chuyển đổi giữa các gia đình instance, cần thực hiện đánh giá các cơ hội tuning có thể cải thiện hiệu suất mà không thay đổi hạ tầng. Trung bình tải (load averages) vượt thường xuyên hơn số lượng vCPU cũng có thể là dấu hiệu bão hòa tính toán và có thể yêu cầu tối ưu tiếp.\nSử dụng bộ nhớ đệm L3 L3 cache là lớp nhớ nhanh chia sẻ được sử dụng bởi một nhóm lõi CPU. Trên EC2 dựa AMD, các lõi được tổ chức thành các slice bộ nhớ đệm L3, mỗi slice được chia sẻ bởi một tập hợp lõi trên cùng một socket. Các luồng được lập lịch trong cùng slice có thể truy cập dữ liệu chia sẻ hiệu quả hơn, giảm độ trễ bộ nhớ. Trên các instance AMD thế hệ 4 như m7a.2xlarge hoặc r7a.2xlarge, tất cả vCPU thường ánh xạ tới các lõi nằm trong cùng một slice L3, đảm bảo tính nhất quán địa phương bộ nhớ đệm. Đối với các kích cỡ lớn hơn (ví dụ m7a.8xlarge trở lên), thread pinning — gán các luồng tới lõi vật lý cụ thể — có thể giúp duy trì tính địa phương này. Thread pinning có thể giảm biến động hiệu suất trong các khối lượng với mẫu truy cập bộ nhớ chia sẻ thường xuyên.\nBạn có thể pin luồng với lệnh:\ntaskset -c 0-3 ./your_application\nVí dụ này pin ứng dụng của bạn vào các lõi CPU 0 đến 3. Để xác định lõi nào chia sẻ cùng vùng bộ nhớ đệm L3, sử dụng các công cụ như lscpu hoặc lstopo để kiểm tra topology CPU hệ thống. Gom các luồng liên quan vào các lõi cùng chia sẻ L3 cache có thể cải thiện tính nhất quán hiệu suất cho các khối lượng có truy cập bộ nhớ chia sẻ.\nTối ưu container Docker Trong môi trường container chạy trên các instance EC2 dựa AMD, điều chỉnh các thiết lập liên quan CPU có thể cải thiện tính nhất quán và hiệu quả khối lượng công việc — đặc biệt cho các ứng dụng tính toán nặng hoặc nhạy độ trễ. Mặc dù cấu hình mặc định hoạt động cho nhiều kịch bản tổng quát, một số workload có thể lợi từ việc kiểm soát rõ ràng cách phân bổ tài nguyên CPU. Theo mặc định, runtime container như Docker cho phép hệ điều hành lập lịch container trên bất kỳ lõi CPU nào sẵn có. Việc này có thể dẫn đến biến thiên hiệu suất khi container di chuyển giữa các lõi không chia sẻ cache. Để giảm biến thiên và cải thiện hiệu quả cache, container có thể được pin vào các lõi cụ thể bằng flag --cpuset-cpus.\ndocker run --cpuset-cpus=\u0026ldquo;1,3\u0026rdquo; my-container\nThiết lập này giới hạn container chỉ dùng các lõi đã chỉ định. Trong ví dụ này, lõi 1 và 3 được dùng để minh hoạ. Lựa chọn lõi thực tế nên dựa trên topology CPU để đảm bảo lập lịch hiệu quả bộ nhớ đệm. Pin container vào các lõi chia sẻ bộ nhớ đệm L3 có thể giảm overhead lập lịch và cải thiện tính nhất quán cho các workload có mẫu truy cập bộ nhớ chia sẻ.\nThiết lập governor tần số CPU Một số hệ điều hành điều chỉnh tần số CPU động để tiết kiệm điện. Thông thường điều này được kiểm soát bởi thiết lập gọi là CPU frequency governor. Mặc dù hành vi này hiệu quả cho các workload tổng quát, nó có thể gây độ trễ hoặc biến thiên hiệu suất trong môi trường nhạy với tính toán. Với các workload cần hiệu suất CPU ổn định cao — như xử lý dữ liệu thông lượng lớn, mô phỏng, hoặc ứng dụng thời gian thực — chúng tôi khuyến nghị đặt governor của CPU về performance mode. Điều này đảm bảo CPU chạy ở tần số tối đa khi chịu tải, tránh thời gian tăng tốc từ trạng thái năng lượng thấp.\nBạn có thể áp dụng thiết lập này trên các instance bare metal hoặc Amazon EC2 Dedicated Hosts bằng lệnh:\nsudo cpupower frequency-set -g performance\nTrước khi áp dụng, hãy cân nhắc benchmark hiệu suất workload với các governor khác (như ondemand hoặc schedutil) để đảm bảo rằng chế độ performance mang lại lợi ích đo được mà không đánh đổi điện năng không cần thiết.\nDùng flag compiler kiến trúc cụ thể Khi biên dịch các ứng dụng C hoặc C++ nhạy hiệu suất, các flag kiến trúc cụ thể như -march=znverX có thể mở khóa tối ưu hóa dành riêng cho AMD EPYC, bao gồm cải thiện vectorization và hiệu suất số thực. Dù điều này có lợi cho workload tính toán nặng, nó có thể giảm tính di động giữa các kiến trúc. Để cân bằng giữa hiệu suất và tính linh hoạt, cân nhắc dùng phát hiện tính năng thời chạy (runtime feature detection) và dispatching — cách nhiều thư viện tối ưu dùng để thích ứng hành vi dựa trên CPU nền tảng.\nTrước khi dùng các flag này, xác minh rằng phiên bản compiler của bạn hỗ trợ chúng và đảm bảo kiến trúc instance EC2 đích phù hợp với flag chỉ định. Ví dụ, một binary biên dịch với -march=znver4 có thể lỗi chạy với lỗi “illegal instruction” (SIGILL) nếu chạy trên các instance thế hệ trước như M5a. Bảng dưới đây mô tả các flag phù hợp và phiên bản compiler tối thiểu hỗ trợ cho mỗi thế hệ AMD EPYC:\nThế hệ AMD EPYC Flag -march Phiên bản GCC tối thiểu Phiên bản LLVM/Clang tối thiểu Thế hệ 4 (ví dụ M7a) znver4 GCC 12 Clang 15 Thế hệ 3 (ví dụ M6a) znver3 GCC 11 Clang 13 Thế hệ 2 (ví dụ M5a) znver2 GCC 9 Clang 11 Các flag sau được hỗ trợ cho GCC 11+ hoặc LLVM Clang 13+:\nCho EPYC thế hệ 4 (M7a, R7a, C7a, Hpc7a): -march=znver4\nCho EPYC thế hệ 3 (M6a, R6a, C6a): -march=znver3\nCho EPYC thế hệ 2 (M5a, R5a, C5a): -march=znver2\nKhi nào bật AVX‑512 và VNNI Các instance EC2 dùng AMD EPYC thế hệ 4 hỗ trợ các tập lệnh SIMD tiên tiến như AVX2, AVX‑512 và VNNI. Những tập lệnh này có thể cải thiện thông lượng cho các workload vector nặng như inference ML, xử lý hình ảnh hoặc mô phỏng khoa học. Tuy nhiên, những flag này là đặc trưng theo thế hệ — việc cố chạy các binary được biên dịch với AVX‑512 trên instance không hỗ trợ (ví dụ thế hệ 2 như M5a) có thể gây lỗi thời gian chạy như “illegal instruction” (SIGILL).\nKhi biên dịch mã C hoặc C++:\ngcc -mavx2 -mavx512f -O2 your_program.c -o your_program\nĐể hiểu rõ hơn tối ưu hóa nào được áp dụng, dùng:\n-ftree-vectorizer-verbose=2 -fopt-info-vec-missed\nCách này giúp xác định các vòng lặp hưởng lợi từ vectorization và những vòng lặp không. Chỉ bật các tối ưu hóa này nếu workload của bạn hưởng lợi và bạn đã xác thực tính tương thích với thế hệ instance đang dùng. Tránh áp dụng flag AVX một cách bừa bãi, vì có thể làm giảm tính di động và tăng độ phức tạp binary.\nThư viện tối ưu CPU của AMD (AMD Optimizing CPU Libraries – AOCL) Thư viện AMD Optimizing CPU Libraries (AOCL) cung cấp các thư viện toán học được tinh chỉnh hiệu năng dành riêng cho các bộ xử lý AMD EPYC. Những thư viện này bao gồm các triển khai tối ưu của các hàm hay dùng trong khoa học, kỹ thuật và workload ML. Bạn có thể liên kết ứng dụng của bạn với AOCL để sử dụng các tối ưu phần cứng mà không cần viết lại mã. AOCL gồm các thư viện cho toán vector, scalar, tạo số ngẫu nhiên, FFT, BLAS và LAPACK, trong số những cái khác.\nCấu hình AOCL Gán biến môi trường AOCL_ROOT trỏ tới thư mục cài đặt:\nexport AOCL_ROOT=/path/to/aocl\nBiên dịch ứng dụng với đường dẫn include và library tương ứng:\ngcc -I$AOCL_ROOT/include -L$AOCL_ROOT/lib -lamdlibm -lm your_program.c -o your_program\nTối ưu toán vector và scalar: bạn có thể bật các tuning vector hóa hay scalar cụ thể cho workload:\n# Tối ưu toán vector\ngcc -lamdlibm -fveclib=AMDLIBM -lm your_program.c -o your_program\n# Toán scalar nhanh hơn\ngcc -lamdlibm -fsclrlib=AMDLIBM -lamdlibmfast -lm your_program.c -o your_program\nProfiling runtime AOCL\nAOCL hỗ trợ profiling runtime, giúp các nhà phát triển xác định các phép toán nào chiếm thời gian thực thi lớn. Để bật profiling:\nexport AOCL_PROFILE=1\n./your_program\nSau khi chạy, một file báo cáo tên aocl_profile_report.txt được sinh ra. Nó cung cấp phân tích theo hàm gồm số lần gọi, thời gian thực thi và việc sử dụng luồng. Các nhà phát triển có thể dùng nó để tập trung tối ưu hóa vào các phần có ảnh hưởng cao nhất.\nKết luận Bài viết này khảo sát cách chọn các loại instance Amazon EC2 dựa AMD phù hợp với đặc điểm khối lượng công việc, và cách áp dụng các kỹ thuật điều chỉnh tập trung vào việc sử dụng CPU, định vị luồng, hiệu quả cache và tối ưu thư viện toán học. Những phương pháp này đặc biệt quan trọng với khối lượng công việc bị giới hạn bởi CPU hoặc nhạy độ trễ, nơi hiệu suất ổn định là thiết yếu.\nSẵn sàng bắt đầu? Đăng nhập AWS Management Console và khởi động các instance Amazon EC2 dùng AMD EPYC để bắt tay vào tối ưu hóa workload của bạn ngay hôm nay.\nTAGS: AMD\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Kiểm tra Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Lấy regional DNS name (tên DNS khu vực) của S3 interface endpoint Trong Amazon VPC menu, chọn Endpoints.\nClick tên của endpoint chúng ta mới tạo ở mục 4.2: s3-interface-endpoint. Click details và lưu lại regional DNS name của endpoint (cái đầu tiên) vào text-editor của bạn để dùng ở các bước sau.\nKết nối đến EC2 instance ở trong \u0026ldquo;VPC On-prem\u0026rdquo; (giả lập môi trường truyền thống) Đi đến Session manager bằng cách gõ \u0026ldquo;session manager\u0026rdquo; vào ô tìm kiếm\nClick Start Session, chọn EC2 instance có tên Test-Interface-Endpoint. EC2 instance này đang chạy trên \u0026ldquo;VPC On-prem\u0026rdquo; và sẽ được sử dụng để kiểm tra kết nối đến Amazon S3 thông qua Interface endpoint. Session Manager sẽ mở 1 browser tab mới với shell prompt: sh-4.2 $\nĐi đến ssm-user\u0026rsquo;s home directory với lệnh \u0026ldquo;cd ~\u0026rdquo;\nTạo 1 file tên testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file vào S3 bucket mình tạo ở section 4.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; Câu lệnh này yêu cầu thông số \u0026ndash;endpoint-url, bởi vì bạn cần sử dụng DNS name chỉ định cho endpoint để truy cập vào S3 thông qua Interface endpoint. Không lấy \u0026rsquo; * \u0026rsquo; khi copy/paste tên DNS khu vực. Cung cấp tên S3 bucket của bạn Bây giờ tệp đã được thêm vào bộ chứa S3 của bạn. Hãy kiểm tra bộ chứa S3 của bạn trong bước tiếp theo.\nKiểm tra Object trong S3 bucket Đi đến S3 console Click Buckets Click tên bucket của bạn và bạn sẽ thấy testfile2.xyz đã được thêm vào s3 bucket của bạn "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/",
	"title": "Ngày 03 - Công cụ Quản lý AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-10 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bộ công cụ quản trị AWS AWS Management Console Đăng nhập bằng Root User hoặc IAM User (cần ID tài khoản 12 chữ số). Tìm kiếm và mở dashboard của từng dịch vụ. Support Center cho phép tạo ticket hỗ trợ trực tiếp. AWS Command Line Interface (CLI) Công cụ dòng lệnh mã nguồn mở tương tác với dịch vụ AWS. Cung cấp tính năng tương đương Console. Đặc điểm chính:\nHỗ trợ đa nền tảng (Windows, macOS, Linux). Dễ script và tự động hóa. Truy cập trực tiếp API dịch vụ AWS. Quản lý nhiều tài khoản thông qua profiles. AWS SDK (Software Development Kit) Đơn giản hóa việc tích hợp dịch vụ AWS vào ứng dụng. Tự động xử lý xác thực, retry và tuần tự hóa dữ liệu. Ngôn ngữ hỗ trợ:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go và các ngôn ngữ khác "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/",
	"title": "Ngày 08 - Bảo mật VPC &amp; Flow Logs",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-17 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Bảo mật VPC Security Group (SG) Tường lửa ảo stateful kiểm soát lưu lượng vào/ra tài nguyên AWS. Quy tắc dựa trên giao thức, cổng, nguồn hoặc security group khác. Chỉ hỗ trợ rule cho phép (allow). Áp dụng ở cấp độ Elastic Network Interface (ENI). Đặc điểm của Security Group:\nStateful: lưu lượng trả về được cho phép tự động. Chỉ có rule cho phép. Đánh giá toàn bộ rule trước khi quyết định. Áp dụng ở mức instance/ENI. Network Access Control List (NACL) Tường lửa ảo stateless hoạt động ở cấp subnet. Quy tắc điều khiển lưu lượng vào/ra theo giao thức, cổng và nguồn. NACL mặc định cho phép tất cả lưu lượng. Đặc điểm của NACL:\nStateless: phải cho phép rõ ràng lưu lượng chiều về. Hỗ trợ cả rule allow và deny. Các rule được xử lý theo thứ tự số. Áp dụng ở mức subnet. VPC Flow Logs Ghi nhận metadata về lưu lượng IP đi/đến các network interface trong VPC. Log có thể gửi tới Amazon CloudWatch Logs hoặc S3. Flow Logs không ghi phần payload của gói tin. Trường hợp sử dụng:\nKhắc phục sự cố kết nối. Theo dõi mẫu lưu lượng. Phân tích bảo mật. Đáp ứng yêu cầu tuân thủ. Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (tiếp) Khởi chạy EC2 trong các subnet → 04-1 Kiểm tra kết nối giữa các instance → 04-2 Tạo NAT Gateway (Private ↔ Internet) → 04-3 EC2 Instance Connect Endpoint (không cần bastion) → 04-5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/",
	"title": "Ngày 13 - Instance Store &amp; User Data",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-24 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tính năng nâng cao của EC2 Instance Store Instance Store cung cấp lưu trữ block tạm thời gắn trực tiếp vào host EC2. Đặc điểm\nI/O và thông lượng rất cao. Dữ liệu mất khi instance dừng hoặc terminate. Không thể tách rời hoặc tạo snapshot. Tình huống sử dụng\nCache hoặc xử lý dữ liệu tạm. Ứng dụng đã có cơ chế nhân bản/replication riêng. So sánh Instance Store và EBS:\nTiêu chí Instance Store EBS Tính bền vững Tạm thời Bền vững Hiệu năng Rất cao Cao Snapshot Không Có Tháo rời Không Có Chi phí Đã bao gồm Tính riêng User Data Script User Data chạy tự động khi instance khởi tạo (mỗi lần provision AMI). Linux dùng bash script, Windows dùng PowerShell. Ví dụ User Data:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata cung cấp thông tin về instance đang chạy như IP private/public, hostname, security group. Thường dùng trong User Data để cấu hình động. Truy cập Metadata:\n# Lấy instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Lấy public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Lấy credential IAM role curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Tạo Budget từ template → 07-01 Hướng dẫn tạo Cost Budget → 07-02 Tạo Usage Budget → 07-03 Tạo Budget cho Reserved Instance → 07-04 Tạo Budget cho Savings Plans → 07-05 Dọn dẹp Budget → 07-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/",
	"title": "Ngày 18 - AWS Snow Family &amp; Hybrid Storage",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-01 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Snow Family Bộ thiết bị/dịch vụ chuyên dụng để di chuyển khối lượng dữ liệu lớn vào/ra AWS khi băng thông hạn chế hoặc dữ liệu quá khủng.\nAWS Snowcone: Thiết bị nhỏ gọn, chịu va đập (~8 TB). Phù hợp môi trường edge, vùng xa. AWS Snowball: Snowball Edge Storage Optimized: Tối đa ~80 TB lưu trữ khả dụng. Snowball Edge Compute Optimized: Thêm khả năng compute mạnh với ~42 TB lưu trữ. AWS Snowmobile: Trung tâm dữ liệu container hóa, phục vụ chuyển dữ liệu quy mô exabyte (tới 100 PB). So sánh Snow Family:\nThiết bị Lưu trữ Compute Use case Snowcone 8 TB 2 vCPU Edge, IoT Snowball Storage 80 TB 40 vCPU Di chuyển dữ liệu Snowball Compute 42 TB 52 vCPU Edge computing Snowmobile 100 PB N/A Di dời datacenter Khi nào dùng Snow Family:\nBăng thông hạn chế hoặc chi phí cao. Khối lượng dữ liệu rất lớn (TB tới PB). Địa điểm xa xôi, khó kết nối. Nhu cầu xử lý edge computing. Yêu cầu tuân thủ lưu trữ dữ liệu tại chỗ. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/",
	"title": "Ngày 23 - Amazon Cognito &amp; Organizations",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-08 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Cognito Dịch vụ managed cho xác thực/ủy quyền và quản lý người dùng cho web \u0026amp; mobile. Thành phần: User Pools: Thư mục người dùng phục vụ đăng ký/đăng nhập. Identity Pools: Danh tính liên kết (federated) cung cấp credential tạm thời để truy cập dịch vụ AWS. Tính năng của Cognito User Pools:\nĐăng ký và đăng nhập. Hỗ trợ IdP xã hội (Google, Facebook, Amazon). Hỗ trợ IdP SAML. Multi-factor authentication (MFA). Xác thực email và số điện thoại. Tùy biến luồng đăng nhập. Lambda trigger để mở rộng logic. Tính năng của Cognito Identity Pools:\nCredential AWS tạm thời. Phân biệt truy cập authenticated và unauthenticated. Kiểm soát truy cập dựa trên role. Tích hợp với User Pool. Hỗ trợ IdP bên ngoài. AWS Organizations Quản lý tập trung nhiều tài khoản AWS trong một tổ chức. Lợi ích\nQuản lý tài khoản tập trung. Consolidated Billing. Cấu trúc phân cấp bằng Organizational Unit (OU). Thiết lập guardrail bằng Service Control Policy (SCP). Organizational Unit (OU) Gom tài khoản theo phòng ban, dự án hoặc môi trường; có thể lồng OUs để áp policy theo tầng. Ví dụ cấu trúc OU:\nRoot\r├── Production OU\r│ ├── Web App Account\r│ └── Database Account\r├── Development OU\r│ ├── Dev Account\r│ └── Test Account\r└── Security OU\r└── Audit Account Consolidated Billing Một hóa đơn cho mọi tài khoản; hưởng lợi từ volume pricing; không phát sinh phí thêm. Lợi ích:\nGiảm giá theo khối lượng dùng chung giữa các tài khoản. Dễ theo dõi và lập báo cáo. Đơn giản hóa phương thức thanh toán. Chia sẻ Reserved Instance. Hands-On Labs Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Phần 2) Switch Role → 28-5.1 Truy cập EC2 Console – Tokyo → 28-5.2.1 Truy cập EC2 Console – N. Virginia → 28-5.2.2 Tạo EC2 (không đáp ứng tag) → 28-5.2.3 Chỉnh sửa tag tài nguyên EC2 → 28-5.2.4 Kiểm tra chính sách → 28-5.2.5 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Phần 1) Tạo EC2 Instance kèm Tag → 27-2.1.1 Quản lý tag cho tài nguyên AWS → 27-2.1.2 Lọc tài nguyên theo tag → 27-2.1.3 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/",
	"title": "Ngày 28 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-15 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Redshift Dịch vụ data warehouse fully-managed tối ưu cho workload phân tích quy mô lớn (OLAP).\nLưu trữ dạng cột, nén dữ liệu, thực thi song song MPP; mở rộng từ hàng trăm GB tới hàng PB. Tích hợp sâu với S3, Kinesis, DynamoDB, các công cụ BI; hỗ trợ nhiều lớp bảo mật. Concurrency Scaling tự động bổ sung compute khi lưu lượng tăng đột biến. Kiến trúc: cluster gồm leader node + compute nodes; mỗi compute node chia thành nhiều slice. Tùy chọn triển khai:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (truy vấn trực tiếp dữ liệu trên S3) Use case: BI doanh nghiệp, phân tích data lake, dashboard, phân tích xu hướng và dự báo.\nTính năng nổi bật:\nLưu trữ dạng cột: phù hợp truy vấn phân tích. Massively Parallel Processing: phân tán truy vấn trên nhiều node. Result Caching: tăng tốc truy vấn lặp lại. Automatic Compression: giảm dung lượng lưu trữ. Workload Management (WLM): ưu tiên truy vấn quan trọng. Concurrency Scaling: xử lý workload bùng nổ. Redshift vs Data Warehouse truyền thống:\nTiêu chí Redshift DW truyền thống Thiết lập Vài phút Vài tuần/tháng Mở rộng Elastic Cố định Chi phí Pay-as-you-go Đầu tư lớn ban đầu Vận hành Managed Tự quản lý Redshift Spectrum:\nTruy vấn trực tiếp dữ liệu trên S3 mà không cần load. Tách biệt compute và storage. Hỗ trợ nhiều định dạng (Parquet, ORC, JSON\u0026hellip;). Tiết kiệm chi phí cho dữ liệu truy cập không thường xuyên. Labs thực hành Lab 43 – AWS Database Migration Service (DMS) (Phần 2) Cấu hình đích MSSQL → Aurora MySQL → 43-07 Tạo project MSSQL → Aurora MySQL → 43-08 Convert schema MSSQL → Aurora MySQL → 43-09 Convert schema Oracle → MySQL (1) → 43-10 Tạo migration task \u0026amp; endpoints → 43-11 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/",
	"title": "Ngày 33 - Next.js App Router",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-22 (Thứ Tư)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Next.js 16 App Router Tận dụng Server Components để fetch data trực tiếp từ server và tránh bundle dư thừa. Route /app/books/[id]/page.tsx lo việc fetch dữ liệu, trả về UI đã render sẵn. generateMetadata giúp bổ sung meta SEO dựa trên dữ liệu sách. // app/books/[id]/page.tsx import { getBook } from \u0026#34;@/lib/api\u0026#34;; export default async function BookDetail({ params }) { const book = await getBook(params.id); if (!book) return notFound(); return \u0026lt;BookPage book={book} /\u0026gt;; } Xử lý lỗi \u0026amp; not-found Chỉ cần not-found.tsx cho trường hợp không tìm được sách → coi là flow dự kiến. Không dùng error.tsx để tránh xử lý trùng; các lỗi còn lại sẽ được log ở backend. Giữ UX đồng nhất: hiển thị CTA quay về danh sách và hotline hỗ trợ. Environment \u0026amp; Config Sử dụng biến môi trường rõ ràng: NEXT_PUBLIC_API_URL cho frontend, API_URL cho route handler. Luôn cập nhật .env.example khi thêm biến mới. Gom logic build URL vào helper lib/api.ts để tránh lặp code. Insight Server Components giảm đáng kể latency khi render trang chi tiết. App Router giúp cấu trúc thư mục rõ ràng, dễ mở rộng thêm slice mới. Khi dùng mock API, chỉ cần đổi base URL để fetch từ Prism. Labs thực hành Tạo not-found.tsx tùy biến với CTA điều hướng. Viết helper getBook(id) tái sử dụng cho Server Components và tests. Chạy npm run lint \u0026amp;\u0026amp; npm run build để chắc cấu hình Next.js sạch. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.3-day38-2025-10-29/",
	"title": "Ngày 38 - Mô Hình Seq2seq &amp; LSTM Chi Tiết",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-29 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nMô Hình Seq2seq Mô hình Sequence-to-Sequence (Seq2seq) giới thiệu kiến trúc encoder-decoder hiệu quả cho các tác vụ như dịch máy và tóm tắt văn bản.\nĐặc Điểm Chính: Ánh xạ chuỗi có độ dài thay đổi thành bộ nhớ có độ dài cố định Đầu vào và đầu ra có thể có độ dài khác nhau Sử dụng LSTM và GRU để tránh vanishing và exploding gradients Encoder nhận token từ làm đầu vào → các vector trạng thái ẩn → decoder tạo ra chuỗi đầu ra Kiến Trúc LSTM: Tìm Hiểu Sâu LSTM là gì? LSTM (Long Short-Term Memory) giống như phiên bản mini của não người khi xử lý trí nhớ.\nCấu Trúc LSTM = 3 Cổng + 1 Trạng Thái Tế Bào 1. Cổng Quên – Quyết Định Quên Gì Quyết định thông tin nào cần loại bỏ khỏi trạng thái cũ.\nCông thức:\nf_t = σ(W_f · [h_{t-1}, x_t] + b_f) Ví dụ não người:\nTin nhắn vô ích từ người đã ghosting bạn → quên Công thức bạn dùng hàng ngày → giữ 2. Cổng Đầu Vào – Quyết Định Nhớ Gì Quyết định thông tin mới nào cần thêm vào bộ nhớ.\nCông thức:\ni_t = σ(W_i · [h_{t-1}, x_t] + b_i)\rĈ_t = tanh(W_C · [h_{t-1}, x_t] + b_C) Ví dụ não người:\nThông tin có giá trị → lưu vào trí nhớ dài hạn Nhiễu không liên quan → loại bỏ ngay lập tức 3. Cập Nhật Trạng Thái Tế Bào – Trí Nhớ Dài Hạn Cập nhật trí nhớ dài hạn bằng cách kết hợp cổng quên và cổng đầu vào.\nCông thức:\nC_t = f_t ⊙ C_{t-1} + i_t ⊙ Ĉ_t Trong đó:\nf_t ⊙ C_{t-1} = những gì cần giữ từ trí nhớ cũ i_t ⊙ Ĉ_t = những gì cần thêm từ đầu vào mới 4. Cổng Đầu Ra – Quyết Định Xuất Ra Gì Quyết định trí nhớ nào sử dụng cho đầu ra hiện tại.\nCông thức:\no_t = σ(W_o · [h_{t-1}, x_t] + b_o)\rh_t = o_t ⊙ tanh(C_t) Ví dụ não người:\nKhi thi NLP → nhớ lại công thức LSTM Khi nói chuyện với ai đó → nhớ ngữ cảnh cuộc trò chuyện Khi làm DevOps → nhớ thông số AWS LSTM vs Não Người Não Người LSTM Trí nhớ dài hạn Cell State Lọc thông tin không cần thiết Forget Gate Chấp nhận thông tin mới có giá trị Input Gate Truy xuất trí nhớ phù hợp để phản hồi Output Gate Học từ kinh nghiệm tuần tự RNN backbone Không quên nhanh Long-term dependencies Cổng là gì? Cổng = bộ lọc nhận thức\nMỗi cổng = một cơ chế quyết định \u0026ldquo;giữ hay bỏ\u0026rdquo;\nVí dụ: Khi Bạn Học NLP Cổng Quên: \u0026ldquo;Tôi còn cần nhớ phương pháp lỗi thời này không?\u0026rdquo; → Bỏ nếu không Cổng Đầu Vào: \u0026ldquo;Khái niệm mới này có giá trị không?\u0026rdquo; → Lưu nếu có Cổng Đầu Ra: \u0026ldquo;Tôi cần kiến thức gì ngay bây giờ?\u0026rdquo; → Truy xuất phần liên quan Giới Hạn Hidden State Hidden state không có giới hạn token, nhưng có giới hạn khả năng nhớ hiệu quả.\nGóc Độ Toán Học: Hidden state = vector có kích thước cố định (ví dụ: 128, 256, 512 chiều) Có thể xử lý 10 token hoặc 10.000 token → không bị crash Vấn đề: không thể nhớ mọi thứ Tại Sao? Ngay cả với cell state, gradient suy yếu qua nhiều bước thời gian Các phụ thuộc dài hạn bị mất Các token xa điểm bắt đầu có ảnh hưởng yếu đến đầu ra cuối cùng Giải pháp: Đây là lý do chúng ta cần cơ chế Attention!\nThrottling trong NLP Hai Nghĩa của Throttling: 1. Throttling Cấp Hệ Thống (API) Giới hạn tốc độ request hoặc xử lý token để:\nBảo vệ tài nguyên GPU Phân phối tài nguyên công bằng Tránh quá tải server Kiểm soát chi phí Ví dụ:\nOpenAI GPT: 10 requests/giây, 90k tokens/phút Anthropic Claude: 20 requests/giây HuggingFace: timeout nếu generation mất quá lâu 2. Throttling Cấp Mô Hình (Kiến Trúc) LSTM, Transformer và Attention đều có cơ chế giới hạn xử lý thông tin tại bất kỳ thời điểm nào:\n(A) LSTM Throttling → Cổng Quên Khi chuỗi quá dài:\nCổng quên tự động \u0026ldquo;throttle\u0026rdquo; thông tin cũ Chỉ cho phép một phần ý nghĩa đi qua Giống throttling mạng: \u0026ldquo;quá tải → giảm băng thông → bỏ packet\u0026rdquo; (B) Transformer Throttling → Giới Hạn Context Window\nBERT: 512 tokens GPT-3: 2048-4096 tokens GPT-4: 128k-1M tokens Claude 3.5 Sonnet: 200k-1M tokens Khi đầu vào vượt giới hạn:\nMô hình cắt dữ liệu Hoặc từ chối xử lý Hoặc hạ chất lượng attention (C) Attention Throttling → Sparse Attention Trong các mô hình ngữ cảnh dài (Longformer, BigBird, Mistral):\nKhông thể tính toán full n² attention Chỉ chú ý đến các vùng quan trọng (local attention) Hoặc global tokens Hoặc sliding window (D) Token Generation Throttling Một số decoder sẽ:\nLàm chậm tốc độ sinh token Giới hạn sampling Áp dụng kiểm soát nhiệt độ Cắt beam search Khi đầu vào nhiễu hoặc không chắc chắn, điều này hoạt động như phanh: \u0026ldquo;Không chắc → làm chậm generation → tăng chất lượng\u0026rdquo;\nTóm Tắt LSTM không chỉ là một mô hình — nó là sự bắt chước tính toán cách trí nhớ con người hoạt động. Hiểu các cổng giúp bạn hiểu tại sao một số thông tin tồn tại trong khi thông tin khác biến mất.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.3-day43-2025-11-05/",
	"title": "Ngày 43 - Sâu Hơn Về Scaled Dot-Product Attention",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-05 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nScaled Dot-Product Attention: Cơ Chế Lõi Đây là tim và linh hồn của transformers. Hiểu biết sâu về điều này là rất quan trọng.\nCông Thức Attention (Q × K^T)\rAttention(Q, K, V) = softmax(─────────) × V\r√(d_k) Trong đó:\nQ = ma trận Query (chúng ta đang tìm kiếm cái gì?) K = ma trận Key (chúng ta có thể attend tới cái gì?) V = ma trận Value (chúng ta nhận được thông tin gì?) d_k = chiều của keys (thường là 64) Tính Toán Từng Bước Hãy tính toán attention cho một ví dụ đơn giản:\nCâu Đầu Vào: \u0026ldquo;I am happy\u0026rdquo;\nGiai Đoạn Thiết Lập Bước 1: Tạo Word Embeddings\nI: [0.1, 0.2, 0.3]\ram: [0.4, 0.5, 0.6]\rhappy: [0.7, 0.8, 0.9] Bước 2: Chuyển Đổi Thành Q, K, V Trong thực tế, chúng ta học các phép chiếu tuyến tính:\nQ = Embedding × W_q\rK = Embedding × W_k\rV = Embedding × W_v Để đơn giản, hãy nói rằng:\nQ = [0.1, 0.2, 0.3] K = [0.1, 0.2, 0.3] V = [0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6] [0.4, 0.5, 0.6] [0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] [0.7, 0.8, 0.9] [0.7, 0.8, 0.9] (Trong thực tế, Q, K, V sẽ là các phép chiếu khác nhau, nhưng điều này cho thấy khái niệm)\nGiai Đoạn Tính Toán Bước 3: Tính Q × K^T (dot products)\nQ × K^T = [0.1, 0.2, 0.3] [0.1, 0.4, 0.7]\r[0.4, 0.5, 0.6] × [0.2, 0.5, 0.8]\r[0.7, 0.8, 0.9] [0.3, 0.6, 0.9]\rQ₁·K₁ = 0.1×0.1 + 0.2×0.2 + 0.3×0.3 = 0.01 + 0.04 + 0.09 = 0.14\rQ₁·K₂ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32\rQ₁·K₃ = 0.1×0.7 + 0.2×0.8 + 0.3×0.9 = 0.07 + 0.16 + 0.27 = 0.50\rMa trận kết quả:\r[0.14, 0.32, 0.50]\r[0.32, 0.77, 1.22]\r[0.50, 1.22, 1.94] Diễn Giải:\nQ₁ (query cho \u0026ldquo;I\u0026rdquo;) có điểm tương tự: [0.14, 0.32, 0.50] 0.14 với \u0026ldquo;I\u0026rdquo; chính nó 0.32 với \u0026ldquo;am\u0026rdquo; 0.50 với \u0026ldquo;happy\u0026rdquo; Bước 4: Tỷ Lệ Theo √d_k\nd_k = 3 (chiều embedding), vì vậy √d_k = √3 ≈ 1.73\nMa trận được tỷ lệ = Q×K^T / √3:\r[0.14/1.73, 0.32/1.73, 0.50/1.73] [0.08, 0.18, 0.29]\r[0.32/1.73, 0.77/1.73, 1.22/1.73] = [0.18, 0.44, 0.70]\r[0.50/1.73, 1.22/1.73, 1.94/1.73] [0.29, 0.70, 1.12] Tại Sao Tỷ Lệ?\nKhi d_k lớn (ví dụ: 64), dot products trở nên rất lớn Các số lớn khiến softmax có gradient rất nhỏ (bão hòa) Tỷ lệ theo √d_k giữ các số trong phạm vi hợp lý để huấn luyện Bước 5: Áp Dụng Softmax\nSoftmax chuyển đổi điểm thành xác suất (tổng bằng 1):\nsoftmax(x) = exp(x) / sum(exp(x))\rCho hàng đầu tiên [0.08, 0.18, 0.29]:\rexp(0.08) ≈ 1.083\rexp(0.18) ≈ 1.197\rexp(0.29) ≈ 1.336\rTổng ≈ 3.616\rXác suất:\r[1.083/3.616, 1.197/3.616, 1.336/3.616] ≈ [0.30, 0.33, 0.37] Cả ba hàng:\nTrọng số Softmax (ma trận attention):\r[0.30, 0.33, 0.37]\r[0.26, 0.37, 0.37]\r[0.25, 0.36, 0.39] Diễn Giải:\nTừ \u0026ldquo;I\u0026rdquo; chi 30% sự chú ý cho chính nó, 33% cho \u0026ldquo;am\u0026rdquo;, 37% cho \u0026ldquo;happy\u0026rdquo; Từ \u0026ldquo;am\u0026rdquo; chi 26% sự chú ý cho \u0026ldquo;I\u0026rdquo;, 37% cho chính nó, 37% cho \u0026ldquo;happy\u0026rdquo; Từ \u0026ldquo;happy\u0026rdquo; chi 25% cho \u0026ldquo;I\u0026rdquo;, 36% cho \u0026ldquo;am\u0026rdquo;, 39% cho chính nó Bước 6: Nhân Với Ma Trận Value (V)\nContext = Softmax_weights × V\rContext(cho \u0026#34;I\u0026#34;): 0.30×[0.1,0.2,0.3] + 0.33×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.03,0.06,0.09] + [0.13,0.17,0.20] + [0.26,0.30,0.33]\r= [0.42, 0.53, 0.62]\rContext(cho \u0026#34;am\u0026#34;): 0.26×[0.1,0.2,0.3] + 0.37×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.026,0.052,0.078] + [0.148,0.185,0.222] + [0.259,0.296,0.333]\r= [0.433, 0.533, 0.633]\rContext(cho \u0026#34;happy\u0026#34;): 0.25×[0.1,0.2,0.3] + 0.36×[0.4,0.5,0.6] + 0.39×[0.7,0.8,0.9]\r= [0.025,0.05,0.075] + [0.144,0.18,0.216] + [0.273,0.312,0.351]\r= [0.442, 0.542, 0.642] Ma Trận Context Đầu Ra:\n[0.42, 0.53, 0.62]\r[0.433, 0.533, 0.633]\r[0.442, 0.542, 0.642] Mỗi từ bây giờ có một vector ngữ cảnh kết hợp thông tin từ tất cả các từ được tính trọng số bằng điểm attention!\nTại Sao Scaled Dot-Product Attention? Khía Cạnh Lý Do Dot Product Thước đo tương tự hiệu quả (chỉ là phép nhân ma trận) Scaling Ngăn chặn bão hòa softmax (giữ gradient khỏe mạnh) Softmax Chuyển đổi tương tự thành trọng số chuẩn hóa [0,1] Nhân Với V Lấy thông tin thực tế (kết hợp có trọng số) Multi-Head Attention: Nhiều Quan Điểm Thay vì một đầu attention, chúng ta sử dụng h = 8 (hoặc nhiều hơn) đầu attention:\nMultiHeadAttention(Q, K, V) = Concat(Head₁, ..., Head₈) × W_o\rTrong đó:\rHead_i = Attention(Q × W_q^i, K × W_k^i, V × W_v^i) Các đầu khác nhau học các mối quan hệ khác nhau:\nĐầu 1: Mối quan hệ chủ ngữ-động từ Đầu 2: Mối quan hệ tính từ-danh từ Đầu 3: Mối quan hệ đại từ-tham chiếu Đầu 4-8: Các mô hình ngữ nghĩa khác Ví Dụ:\nCâu: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;\rĐầu 1 (chủ ngữ-động từ):\r- \u0026#34;fox\u0026#34; → \u0026#34;jumps\u0026#34;: 0.9\r- \u0026#34;dog\u0026#34; → has_property: 0.7\rĐầu 2 (tính từ-danh từ):\r- \u0026#34;quick\u0026#34; → \u0026#34;fox\u0026#34;: 0.85\r- \u0026#34;brown\u0026#34; → \u0026#34;fox\u0026#34;: 0.8\r- \u0026#34;lazy\u0026#34; → \u0026#34;dog\u0026#34;: 0.9\rĐầu 3 (không gian):\r- \u0026#34;over\u0026#34; → kết nối \u0026#34;fox\u0026#34; và \u0026#34;dog\u0026#34;: 0.8 Tất cả các quan điểm khác nhau này kết hợp lại tạo ra sự hiểu biết ngữ cảnh phong phú.\nHiệu Quả Tính Toán Tại sao scaled dot-product attention lại hiệu quả đến vậy?\nPhép Toán Ma Trận: Chỉ là phép nhân và softmax (GPU-optimized) Có Thể Song Song Hóa: Có thể xử lý toàn bộ chuỗi cùng lúc Tiết Kiệm Bộ Nhớ: Bộ nhớ O(n²) cho chuỗi độ dài n (chấp nhận được) Huấn Luyện Nhanh: GPU hiện đại có thể thực hiện hàng tỷ dot products/giây So Sánh:\nRNN: O(n) bước tuần tự → chậm Attention: O(1) độ sâu, O(n²) bộ nhớ → nhanh! Những Hiểu Biết Chính ✅ Attention được học: W_q, W_k, W_v là các tham số có thể huấn luyện ✅ Không phụ thuộc vị trí: Không có phụ thuộc tuần tự - có thể attend trên bất kỳ khoảng cách nào ✅ Có thể Diễn Giải: Có thể hình dung các từ nào attend tới từ nào ✅ Hiệu Quả: Chỉ sử dụng phép toán ma trận (GPU-friendly)\nBước Tiếp Theo Bây giờ chúng ta hiểu scaled dot-product attention, chúng ta sẽ khám phá:\nSelf-attention (query=key=value) Masked attention (decoder chỉ nhìn thấy quá khứ) Encoder-decoder attention (kết nối xuyên ngôn ngữ) Chi tiết Multi-head attention (học nhiều mô hình) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.3-day48-2025-11-12/",
	"title": "Ngày 48 - Mục Tiêu Pre-training của BERT",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-12 (Thứ Tư)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nCách BERT Thực Sự Học BERT không phải là một dự đoán may mắn. Các nhà khoa học Google đã thiết kế hai tác vụ pre-training cụ thể khiến nó học được sự hiểu biết lưỡng chiều. Những tác vụ này rất đơn giản một cách xuất sắc.\nTác Vụ Pre-training 1: Masked Language Modeling (MLM) Ý Tưởng Cốt Lõi Ẩn các từ ngẫu nhiên và dự đoán chúng.\nGốc: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;\rChe: \u0026#34;The quick [MASK] fox jumps over the lazy dog\u0026#34;\rTác Vụ: Dự đoán [MASK] là gì!\rCâu Trả Lời: \u0026#34;brown\u0026#34;\rTại Sao Nó Hoạt Động:\r├─ Mô hình phải hiểu bối cảnh từ CẢ HAI bên\r├─ Để dự đoán \u0026#34;brown\u0026#34;, cần biết:\r│ ├─ Bối cảnh trái: \u0026#34;The quick\u0026#34;\r│ └─ Bối cảnh phải: \u0026#34;fox jumps over the lazy dog\u0026#34;\r└─ Buộc học lưỡng chiều! Chiến Lược Che Phủ Không phải tất cả các token được che phủ theo cách giống nhau:\nKhi chúng ta chọn một token để che:\r80% thời gian: Thay thế bằng token [MASK]\r├─ \u0026#34;The quick [MASK] fox\u0026#34;\r├─ Mô hình phải học từ\r└─ Trường hợp bình thường\r10% thời gian: Thay thế bằng từ ngẫu nhiên\r├─ \u0026#34;The quick apple fox\u0026#34; (thay vì \u0026#34;brown\u0026#34;)\r├─ Mô hình học để sửa đầu vào rườm rà\r└─ Huấn luyện mạnh mẽ\r10% thời gian: Giữ từ gốc\r├─ \u0026#34;The quick brown fox\u0026#34;\r├─ Mô hình học đại diện dù sao\r└─ Giúp ổn định fine-tuning Toán Học Đối với mỗi vị trí token i trong 15% từ vựng bị che:\rP(mask token tại vị trí i) = 0.15\rLoss_MLM = -log(P(từ_chính_xác | bối_cảnh))\rĐối với vị trí bị che với từ \u0026#34;brown\u0026#34;:\r├─ Mô hình đầu ra: [0.02, 0.05, 0.88, 0.03, ...] (xác suất cho tất cả các từ)\r│ ↑ vị trí của \u0026#34;brown\u0026#34;\r├─ Từ chính xác: \u0026#34;brown\u0026#34; (vị trí 3)\r├─ Loss = -log(0.88) = 0.128\r└─ Điều này đẩy xác suất cao hơn!\rTổng Hợp: Tổng bình phương loss trên tất cả 15% token bị che Ví Dụ Đi Bộ Câu: \u0026#34;The bank was robbed by masked men\u0026#34;\rToken: [The, bank, was, robbed, by, masked, men]\rChe 15%: [The, bank, [MASK], robbed, by, masked, men]\rBước 1: Tokenize và nhúng tất cả các token\r├─ Input IDs: [101, 1996, 2924, 103, 2001, 10122, 2039, 2095, 2273, 102]\r├─ Token embeddings: 7 x 768 vectơ chiều\r└─ Thêm embeddings vị trí: vị trí 0, 1, 2, 3, ...\rBước 2: Xử lý qua 12 lớp transformer\r├─ Lớp 1: Self-attention trên tất cả các vị trí\r├─ Lớp 2: Self-attention với các đại diện cập nhật\r├─ ...\r└─ Lớp 12: Đại diện ngữ cảnh cuối cùng\rBước 3: Dự đoán token bị che\r├─ Đầu ra cho vị trí 2 (was): [0.02, 0.05, 0.88, 0.03, ...]\r├─ argmax = 0.88 → Word ID 3 → \u0026#34;was\u0026#34;\r├─ Chính Xác! ✓\r└─ Loss: nhỏ\rBước 4: Backprop gradient qua tất cả 12 lớp\r└─ Cập nhật tất cả các tham số để làm cho dự đoán này có khả năng xảy ra Tác Vụ Pre-training 2: Next Sentence Prediction (NSP) Ý Tưởng Cốt Lõi Cho hai câu, dự đoán nếu chúng liên tiếp.\nCâu A: \u0026#34;The bank was robbed yesterday.\u0026#34;\rCâu B: \u0026#34;The police are investigating.\u0026#34;\rCâu Hỏi: Đây có phải là câu liên tiếp không?\rCâu Trả Lời: Có (IsNext) ✓\rCâu A: \u0026#34;The cat sat on the mat.\u0026#34;\rCâu B: \u0026#34;I enjoy pizza for lunch.\u0026#34;\rCâu Hỏi: Đây có phải là câu liên tiếp không?\rCâu Trả Lời: Không (NotNext) ✗\rTại Sao Nó Giúp:\r├─ Mô hình học hiểu mối quan hệ giữa các câu\r├─ Quan trọng cho các tác vụ như trả lời câu hỏi\r├─ Giúp với sự hiểu biết ngữ nghĩa\r└─ Nhưng: Nghiên cứu gần đây cho thấy điều này thực sự ít quan trọng hơn MLM! Định Dạng Đầu Vào Các token đặc biệt của BERT cho cặp câu:\r[CLS] Câu A [SEP] Câu B [SEP]\r↑ ↑ ↑\rToken cho Phân Tách Kết Thúc\rphân loại giữa chuỗi\r(dùng cho NSP) câu\rSegment IDs:\r[0 0 0 0 0 0 1 1 1 1 1 1]\rCLS The bank was robbed SEP police are investigating\rToken A = 0 (phân đoạn đầu tiên)\rToken B = 1 (phân đoạn thứ hai)\rĐại diện token CLS dùng cho phân loại NSP! Nhiệm Vụ NSP là một vấn đề phân loại nhị phân:\rĐầu Vào: [CLS] ... [SEP] ... [SEP]\r↓\rMô hình đầu ra: [0.1, 0.9]\r↑ ↑\rNotNext IsNext\rNếu nhãn là IsNext: Loss = -log(0.9) = 0.105\rNếu nhãn là NotNext: Loss = -log(0.1) = 2.303\rMô hình học cách phân biệt các câu liên tiếp! Hàm Loss Tổng Hợp BERT huấn luyện cả hai mục tiêu đồng thời:\nLoss_total = Loss_MLM + Loss_NSP\rVí Dụ Lặp Lại:\r├─ Batch chứa 32 cặp câu\r├─ Đối với mỗi cặp:\r│ ├─ 15% của token bị che → MLM loss\r│ ├─ Thứ tự câu được gắn nhãn → NSP loss\r│ └─ Cả hai loss được tính toán\r├─ Trung bình các loss: 2.15 (MLM) + 1.45 (NSP)\r├─ Tổng loss: 3.60\r└─ Backprop cập nhật tất cả các tham số\rTại Sao Cả Hai?\r├─ MLM buộc hiểu biết lưỡng chiều\r├─ NSP buộc ngữ nghĩa ở cấp độ câu\r└─ Cùng Nhau: Kiến thức ngôn ngữ phong phú!\rLưu Ý: Các biến thể hiện đại (RoBERTa) loại bỏ NSP\r(Phát hiện rằng MLM một mình là đủ!) Embeddings Đầu Vào của BERT (Xem Xét Kỹ Lưỡng) Ba Embeddings Được Kết Hợp Embedding của BERT = Token + Segment + Position\rVí Dụ: \u0026#34;The quick brown fox\u0026#34;\rVị Trí: 0 1 2 3\rToken Embedding:\r├─ \u0026#34;The\u0026#34; (ID=1996): [0.2, -0.5, 0.1, ...]\r├─ \u0026#34;quick\u0026#34; (ID=2522): [0.1, 0.3, -0.2, ...]\r├─ \u0026#34;brown\u0026#34; (ID=2829): [-0.1, 0.2, 0.4, ...]\r└─ \u0026#34;fox\u0026#34; (ID=4397): [0.3, 0.1, -0.3, ...]\rSegment Embedding (cùng câu):\r├─ Vị Trí 0: [0.1, 0.2, 0.3, ...] (Phân Đoạn A)\r├─ Vị Trí 1: [0.1, 0.2, 0.3, ...] (Phân Đoạn A)\r├─ Vị Trí 2: [0.1, 0.2, 0.3, ...] (Phân Đoạn A)\r└─ Vị Trí 3: [0.1, 0.2, 0.3, ...] (Phân Đoạn A)\rPositional Embedding:\r├─ Vị Trí 0: [0.0, 0.5, -0.1, ...]\r├─ Vị Trí 1: [1.0, 0.3, 0.2, ...]\r├─ Vị Trí 2: [0.5, -0.1, 0.5, ...]\r└─ Vị Trí 3: [-0.3, 0.6, 0.1, ...]\rFinal Embedding (tổng cả ba):\r├─ Vị Trí 0: [0.3, 0.7, 0.3, ...]\r├─ Vị Trí 1: [1.1, 0.8, 0.0, ...]\r├─ Vị Trí 2: [0.5, 0.3, 0.9, ...]\r└─ Vị Trí 3: [0.0, 0.7, -0.2, ...] Chi Tiết Mã Hóa Vị Trí BERT sử dụng mã hóa vị trí hình sin:\rPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\rPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\rTrong Đó:\r├─ pos = vị trí trong chuỗi (0, 1, 2, ...)\r├─ i = chỉ số chiều (0, 1, 2, ...)\r├─ d_model = 768 (kích thước ẩn)\rĐối với vị trí 0:\r├─ PE(0, 0) = sin(0) = 0.0\r├─ PE(0, 1) = cos(0) = 1.0\rĐối với vị trí 1:\r├─ PE(1, 0) = sin(1 / 10000^0) = sin(1) = 0.841\r├─ PE(1, 1) = cos(1 / 10000^0) = cos(1) = 0.540\rMẫu: Vị trí khác → Mã hóa khác!\rKhông thể thay đổi hai vị trí và giữ mã hóa giống nhau! Dữ Liệu Pre-training và Quy Mô Corpus Corpus Pre-training BERT:\rWikipedia: 13 GB\r├─ 2,500M từ\r├─ Chất lượng cao, các chủ đề đa dạng\r└─ Nhưng tương đối nhỏ!\rBookCorpus: Không được phát hành công khai nhưng ~12 GB\r├─ 800M từ\r├─ Sách được viết bởi mọi người → Ngữ pháp tốt\r└─ Tài liệu dài (tốt để học các phụ thuộc tầm xa)\rTổng: ~25 GB (3.3 tỷ từ pieces)\r├─ Đây là nước xốt bí mật!\r├─ Các mô hình hiện đại sử dụng 100-1000x nhiều dữ liệu hơn\r└─ Dữ liệu Nhiều Hơn = Hiệu Suất Tốt Hơn (Định Luật Mở Rộng) Chi Tiết Huấn Luyện Pre-training BERT:\rThời Gian: 4 ngày (BERT-base)\rPhần Cứng: 16 thiết bị TPUv2\rKích Thước Lô: 256 chuỗi (512 token mỗi cái) = 131,072 token mỗi lô\rTỷ Lệ Học: 1e-4 với khởi động\rTối Ưu Hóa: Adam\rBước: ~1,000,000 bước\rChi Phí Tính Toán:\r├─ 16 TPUs × 4 ngày = 64 TPU-ngày\r├─ ~$100,000+ trong tính toán\r└─ Đó là tại sao chúng ta sử dụng các mô hình được huấn luyện trước ngây!\rSau Khi Huấn Luyện:\r├─ Mô hình có thể được sử dụng cho fine-tuning\r├─ Chỉ cần thêm đầu phân loại!\r└─ Không cần huấn luyện từ đầu bao giờ! Tại Sao Những Tác Vụ Này Hoạt Động Rất Tốt Lợi Ích MLM ✅ Học lưỡng chiều: Phải sử dụng cả hai bên ✅ Tác vụ tự nhiên: Tương tự như đọc con người ✅ Linh Hoạt: Có thể che bất kỳ tỷ lệ nào ✅ Đủ khó: Dự đoán không tầm thường\nLợi Ích NSP ✅ Hiểu biết Diễn Ngôn: Học các mối quan hệ câu ✅ Tác Vụ Cặp: Giúp với QA và NLI ✅ Định Dạng Phân Loại: Dễ dàng trích xuất tính năng cho phân loại\nKết Hợp ✅ Học Đa Tác Vụ: Tín Hiệu Giám Sát Phong Phú ✅ Bổ Sung: MLM + NSP dạy những thứ khác nhau ✅ Hiệu Quả: Một lần chạy huấn luyện, hai tín hiệu học\nSo Sánh: Các Mục Tiêu Pre-training Khác Nhau Word2Vec:\r├─ Tác Vụ: Dự đoán các từ lân cận\r├─ Kết Quả: Static embeddings\r└─ Vấn Đề: Cùng embedding ở mọi nơi\rELMo:\r├─ Tác Vụ: Lập Mô Hình Ngôn Ngữ Lưỡng Chiều\r├─ Kết Quả: Embeddings nhạy cảm bối cảnh\r└─ Vấn Đề: Vẫn xử lý tuần tự\rGPT:\r├─ Tác Vụ: Dự đoán từ tiếp theo (trái sang phải)\r├─ Kết Quả: Tốt cho việc tạo\r└─ Vấn Đề: Không thể nhìn thấy bối cảnh tương lai\rBERT (MLM + NSP):\r├─ Tác Vụ: Dự đoán Che + thứ tự câu\r├─ Kết Quả: Hiểu biết lưỡng chiều sâu\r└─ Lợi Ích: Cái tốt nhất của cả hai!\rT5:\r├─ Tác Vụ: Text-to-text với nhiều mục tiêu\r├─ Kết Quả: Khung thống nhất cho tất cả các tác vụ\r└─ Lợi Ích: Đơn Giản Hơn So Với Hai Loss Riêng Biệt Các Lợi Ích Chính ✅ MLM là ngôi sao: Quan trọng nhất cho hiệu suất ✅ Tỷ lệ che 15%: Được xác định theo kinh nghiệm là tối ưu ✅ Chiến lược che: 80-10-10 làm cho mô hình mạnh mẽ ✅ NSP giúp nhưng là thứ yếu: Chủ yếu cho các tác vụ được ghép ✅ Pre-training tốn kém: Nhưng có thể tái sử dụng! ✅ Ý tưởng đơn giản, kết quả mạnh mẽ: Thiên tài của BERT\nĐiều Gì Sắp Tới Ngày 49: Cách T5 mở rộng quy mô ý tưởng này (và loại bỏ MLM+NSP) Ngày 50: Fine-tuning các mô hình được huấn luyện trước này Các mục tiêu pre-training là nền tảng của NLP hiện đại. Mọi thứ hạ lưu phụ thuộc vào hai tác vụ đơn giản này!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Phần này liệt kê và giới thiệu các blog bạn đã dịch:\nBlog 1 - Tăng tốc luồng dữ liệu và AI của bạn bằng cách kết nối đến Amazon SageMaker Unified Studio từ Visual Studio Code Blog này hướng dẫn cách kết nối Visual Studio Code cục bộ với Amazon SageMaker Unified Studio để tối ưu hóa quy trình phát triển dữ liệu và AI. Bạn sẽ học cách thiết lập môi trường phát triển tích hợp (IDE) cá nhân hóa, truy cập các dịch vụ AWS Analytics và AI/ML trong một môi trường thống nhất, đồng thời duy trì quy trình phát triển hiện có của mình. Bài viết cung cấp hướng dẫn chi tiết về cấu hình kết nối từ xa, các điều kiện tiên quyết cần thiết, và cách triển khai giải pháp để xây dựng luồng công việc dữ liệu và AI đầu-cuối.\nBlog 2 - Thông báo Amazon EC2 M4 và M4 Pro Mac instances Blog này giới thiệu về Amazon EC2 M4 và M4 Pro Mac instances mới được ra mắt, được xây dựng trên nền tảng Apple M4 Mac mini và hệ thống AWS Nitro. Bạn sẽ tìm hiểu về cấu hình phần cứng của các instance này (chip Apple silicon M4/M4 Pro, CPU đa lõi, GPU, Neural Engine), hiệu năng cải thiện so với thế hệ trước (tốt hơn 15-20% cho build ứng dụng), và các tính năng mới như 2TB lưu trữ nội bộ. Bài viết cũng hướng dẫn cách khởi chạy instance thông qua AWS Management Console hoặc CLI, và cách tích hợp với các dịch vụ AWS khác để xây dựng pipeline CI/CD tự động.\nBlog 3 - Hướng dẫn tinh chỉnh cho Amazon EC2 instances dùng AMD Blog này cung cấp hướng dẫn chi tiết về cách tối ưu hóa hiệu suất của Amazon EC2 instances sử dụng bộ xử lý AMD EPYC. Bạn sẽ học cách chọn loại instance phù hợp dựa trên đặc điểm khối lượng công việc (compute-intensive, big data \u0026amp; analytics, database, web servers, AI/ML), hiểu về các thế hệ AMD EPYC (thế hệ 3 và 4) và tính năng của chúng. Bài viết cũng trình bày các kỹ thuật điều chỉnh thực tiễn để cải thiện hiệu quả, bao gồm tối ưu hóa cấu hình CPU, bộ nhớ, và các tham số hệ thống để đạt được tỷ lệ giá-hiệu suất tốt nhất.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.3-s3-vpc/",
	"title": "Truy cập S3 từ VPC",
	"tags": [],
	"description": "",
	"content": "Sử dụng Gateway endpoint Trong phần này, bạn sẽ tạo một Gateway endpoint để truy cập Amazon S3 từ một EC2 instance. Gateway endpoint sẽ cho phép tải một object lên S3 bucket mà không cần sử dụng Internet Công cộng. Để tạo endpoint, bạn phải chỉ định VPC mà bạn muốn tạo endpoint và dịch vụ (trong trường hợp này là S3) mà bạn muốn thiết lập kết nối.\nNội dung Tạo gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/",
	"title": "Tuần 3 - Dịch vụ Compute trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-22 đến 2025-09-26\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 3 Tuần này tập trung vào các dịch vụ Compute của AWS, đặc biệt là Amazon EC2 và những dịch vụ bổ trợ.\nNội dung chính Amazon EC2 và các loại instance. AMI và chiến lược sao lưu. EBS và Instance Store. EC2 Auto Scaling. Lựa chọn mô hình giá cho EC2. Amazon Lightsail, EFS, FSx. Labs thực hành Lab 01: AWS Account \u0026amp; IAM Setup. Lab 07: AWS Budgets \u0026amp; Cost Management. Lab 09: AWS Support Plans. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/4-eventparticipated/",
	"title": "Các Sự Kiện Tham Gia",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập, tôi đã có cơ hội tham gia vào nhiều sự kiện có tác động lớn, giúp làm phong phú thêm hành trình chuyên nghiệp của tôi với những kiến thức quý báu và những trải nghiệm đáng nhớ.\nSự Kiện 1 Tên Sự Kiện: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nNgày \u0026amp; Giờ: 09:00 – 17:00 VNT, Thứ Năm, 18 tháng 9 năm 2025\nĐịa Điểm: Tầng 36, 2 Đường Hai Triều, Phường Sài Gòn, Thành phố Hồ Chí Minh\nVai Trò: Người tham dự\nMô Tả: Vietnam Cloud Day 2025 là một sự kiện AWS toàn diện với các bài phát biểu chính từ các nhà lãnh đạo chính phủ, các giám đốc điều hành AWS và các lãnh đạo ngành. Sự kiện bao gồm hai track chính: track phát sóng trực tiếp với các bài phát biểu chính và thảo luận bảng về cuộc cách mạng GenAI và lãnh đạo cấp cao, cùng các track riêng biệt bao gồm các chủ đề như nền tảng dữ liệu thống nhất cho AI/phân tích, lộ trình áp dụng GenAI, vòng đời phát triển do AI điều khiển, bảo mật các ứng dụng GenAI và các tác nhân AI để tăng năng suất. Sự kiện này giới thiệu các dịch vụ mới nhất của AWS và các sáng kiến chiến lược cho AI và hiện đại hóa đám mây.\nKết Quả: Hiểu rõ hơn về các chiến lược áp dụng AI ở quy mô doanh nghiệp, tìm hiểu về các dịch vụ AWS cho nền tảng dữ liệu và triển khai GenAI, và nắm vững các thực tiễn tốt nhất để bảo mật các ứng dụng AI và hiện đại hóa các hệ thống cũ.\nSự Kiện 2 Tên Sự Kiện: AWS GenAI Builder Club - AI-Driven Development Life Cycle: Reimagining Software Engineering\nNgày \u0026amp; Giờ: 14:00 (2:00 PM), Thứ Sáu, 3 tháng 10 năm 2025\nĐịa Điểm: AWS Event Hall, L26 Tòa Nhà Bitexco, Thành phố Hồ Chí Minh\nVai Trò: Người tham dự\nMô Tả: Phiên làm việc AWS GenAI Builder Club này tập trung vào Vòng Đời Phát Triển Do AI Điều Khiển (AI-DLC), khám phá cách AI tạo sinh biến đổi phát triển phần mềm từ kiến trúc đến triển khai và bảo trì. Phiên làm việc này có các bản trình diễn về Amazon Q Developer và Kiro, cho thấy cách AI có thể tự động hóa các tác vụ nặng nề không phân biệt và cho phép các nhà phát triển tập trung vào công việc sáng tạo có giá trị cao hơn. Chương trình bao gồm tổng quan về các khái niệm AI-DLC, khả năng của Amazon Q Developer và các bản trình diễn Kiro thực hành.\nKết Quả: Học được các ứng dụng thực tế của AI trong vòng đời phát triển phần mềm, có được kinh nghiệm thực hành với các công cụ Amazon Q Developer và Kiro, và hiểu cách tích hợp AI như một cộng tác viên trung tâm trong các quy trình phát triển để tăng năng suất và chất lượng mã.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Mô phỏng On-premises DNS ",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoint có một địa chỉ IP cố định trong từng AZ nơi chúng được triển khai, trong suốt thời gian tồn tại của endpoint (cho đến khi endpoint bị xóa). Các địa chỉ IP này được gắn vào Elastic network interface (ENI). AWS khuyến nghị sử dụng DNS để resolve địa chỉ IP cho endpoint để các ứng dụng downstream sử dụng địa chỉ IP mới nhất khi ENIs được thêm vào AZ mới hoặc bị xóa theo thời gian.\nTrong phần này, bạn sẽ tạo một quy tắc chuyển tiếp (forwarding rule) để gửi các yêu cầu resolve DNS từ môi trường truyền thống (mô phỏng) đến Private Hosted Zone trên Route 53. Phần này tận dụng cơ sở hạ tầng do CloudFormation triển khai trong phần Chuẩn bị môi trường.\nTạo DNS Alias Records cho Interface endpoint Click link để đi đến Route 53 management console (Hosted Zones section). Mẫu CloudFormation mà bạn triển khai trong phần Chuẩn bị môi trường đã tạo Private Hosted Zone này. Nhấp vào tên của Private Hosted Zone, s3.us-east-1.amazonaws.com: Tạo 1 record mới trong Private Hosted Zone: Giữ nguyên Record name và record type Alias Button: click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Add another record, và add 1 cái record thứ 2 sử dụng những thông số sau: Record name: *. Record type: giữ giá trị default (type A) Alias Button: Click để enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Chọn endpoint: Paste tên (Regional VPC Endpoint DNS) bạn đã lưu lại ở phần 4.3 Click Create records Record mới xuất hiện trên giao diện Route 53.\nTạo một Resolver Forwarding Rule Route 53 Resolver Forwarding Rules cho phép bạn chuyển tiếp các DNS queries từ VPC của bạn đến các nguồn khác để resolve name. Bên ngoài môi trường workshop, bạn có thể sử dụng tính năng này để chuyển tiếp các DNS queries từ VPC của bạn đến các máy chủ DNS chạy trên on-premises. Trong phần này, bạn sẽ mô phỏng một on-premises conditional forwarder bằng cách tạo một forwarding rule để chuyển tiếp các DNS queries for Amazon S3 đến một Private Hosted Zone chạy trong \u0026ldquo;VPC Cloud\u0026rdquo; để resolve PrivateLink interface endpoint regional DNS name.\nTừ giao diện Route 53, chọn Inbound endpoints trên thanh bên trái\nTrong giao diện Inbound endpoint, Chọn ID của Inbound endpoint.\nSao chép 2 địa chỉ IP trong danh sách vào trình chỉnh sửa. Từ giao diện Route 53, chọn Resolver \u0026gt; Rules và chọn Create rule Trong giao diện Create rule Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: điền cả hai IP bạn đã lưu trữ trên trình soạn thảo (inbound endpoint addresses) và sau đó chọn Submit Bạn đã tạo thành công resolver forwarding rule.\nKiểm tra on-premises DNS mô phỏng. Kết nối đến Test-Interface-Endpoint EC2 instance với Session Manager Kiểm tra DNS resolution. Lệnh dig sẽ trả về địa chỉ IP được gán cho VPC endpoint interface đang chạy trên VPC (địa chỉ IP của bạn sẽ khác): dig +short s3.us-east-1.amazonaws.com Các địa chỉ IP được trả về là các địa chỉ IP VPC enpoint, KHÔNG phải là các địa chỉ IP Resolver mà bạn đã dán từ trình chỉnh sửa văn bản của mình. Các địa chỉ IP của Resolver endpoint và VPC endpoin trông giống nhau vì chúng đều từ khối CIDR VPC Cloud.\nTruy cập vào menu VPC (phần Endpoints), chọn S3 interface endpoint. Nhấp vào tab Subnets và xác nhận rằng các địa chỉ IP được trả về bởi lệnh Dig khớp với VPC endpoint: Hãy quay lại shell của bạn và sử dụng AWS CLI để kiểm tra danh sách các bucket S3 của bạn: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Kết thúc phiên làm việc của Session Manager của bạn: Trong phần này, bạn đã tạo một Interface Endpoint cho Amazon S3. Điểm cuối này có thể được truy cập từ on-premises thông qua Site-to-Site VPN hoặc AWS Direct Connect. Các điểm cuối Route 53 Resolver outbound giả lập chuyển tiếp các yêu cầu DNS từ on-premises đến một Private Hosted Zone đang chạy trên đám mây. Các điểm cuối Route 53 inbound nhận yêu cầu giải quyết và trả về một phản hồi chứa địa chỉ IP của Interface Endpoint VPC. Sử dụng DNS để giải quyết các địa chỉ IP của điểm cuối cung cấp tính sẵn sàng cao trong trường hợp một Availability Zone gặp sự cố.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/",
	"title": "Ngày 04 - Tối ưu Chi phí trên AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-11 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tối ưu chi phí trên AWS Chiến lược tối ưu chi phí Chọn đúng loại tài nguyên và Region phù hợp. Sử dụng các mô hình giá như Reserved Instances, Savings Plans, Spot Instances. Tắt hoặc lên lịch các tài nguyên không dùng. Tận dụng kiến trúc serverless để giảm chi phí vận hành. Liên tục rà soát hiệu quả chi phí bằng AWS Budgets và Cost Explorer. Gắn thẻ chi phí (Cost Allocation Tags) để theo dõi theo phòng ban. AWS Pricing Calculator calculator.aws\nTạo và chia sẻ ước tính chi phí cho các dịch vụ phổ biến. Giá thay đổi tùy Region. Tính năng chính:\nƯớc tính chi phí trước khi triển khai. So sánh giá giữa các Region. Xuất và chia sẻ bảng dự toán. Có sẵn template cho từng workload. Gói hỗ trợ AWS Support Plans Bốn cấp độ: Basic, Developer, Business, Enterprise. Có thể nâng cấp tạm thời khi gặp sự cố nghiêm trọng. So sánh các gói hỗ trợ Tính năng Basic Developer Business Enterprise Chi phí Miễn phí 29 USD/tháng 100 USD/tháng 15.000 USD/tháng Thời gian phản hồi Không áp dụng 12-24 giờ 1 giờ (khẩn) 15 phút (nghiêm trọng) Hỗ trợ kỹ thuật Diễn đàn Giờ hành chính 24/7 24/7 + TAM Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Tạo Budget từ template → 07-01 Hướng dẫn tạo Cost Budget → 07-02 Tạo Usage Budget → 07-03 Tạo Budget cho Reserved Instance (RI) → 07-04 Tạo Budget cho Savings Plans → 07-05 Dọn dẹp các Budget → 07-06 Lab 09 – AWS Support Plans Các gói hỗ trợ AWS → 09-01 Phân loại yêu cầu hỗ trợ → 09-02 Thay đổi gói hỗ trợ → 09-03 Quản lý ticket hỗ trợ → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/",
	"title": "Ngày 09 - Kết nối VPC &amp; Cân bằng tải",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-18 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học VPC Peering \u0026amp; Transit Gateway VPC Peering Cho phép kết nối riêng tư trực tiếp giữa hai VPC mà không qua Internet. Không hỗ trợ định tuyến chuyển tiếp (transitive) và không chấp nhận CIDR trùng nhau. Hạn chế của VPC Peering:\nKhông có peering chuyển tiếp. Không được phép trùng CIDR. Mỗi VPC tối đa 125 kết nối peering. Hỗ trợ peering liên vùng (cross-region). AWS Transit Gateway (TGW) Hoạt động như một hub kết nối nhiều VPC và mạng on-premises, đơn giản hóa kiến trúc mesh phức tạp. TGW Attachment gắn các subnet thuộc một AZ cụ thể vào TGW. Các subnet trong cùng AZ có thể truy cập TGW sau khi được attach. Lợi ích của Transit Gateway:\nHub kết nối tập trung. Đơn giản hóa kiến trúc mạng. Mở rộng tới hàng nghìn VPC. Hỗ trợ peering giữa các region. VPN \u0026amp; Direct Connect Site-to-Site VPN Thiết lập kết nối IPSec bảo mật giữa data center on-premises và AWS VPC. Bao gồm: Virtual Private Gateway (VGW): endpoint đa AZ do AWS quản lý. Customer Gateway (CGW): thiết bị hoặc appliance do khách hàng quản lý. AWS Direct Connect Cung cấp đường truyền riêng giữa data center on-prem và AWS. Độ trễ điển hình: 20–30 ms. Tại Việt Nam hiện có thông qua Hosted Connection (đối tác). Có thể điều chỉnh băng thông linh hoạt. Hands-On Labs Lab 10 – Hybrid DNS (Route 53 Resolver) Tạo Key Pair → 10-02.1 Khởi tạo CloudFormation Template → 10-02.2 Cấu hình Security Group → 10-02.3 Thiết lập hệ thống DNS → 10-05 Tạo Route 53 Outbound Endpoint → 10-05.1 Tạo Resolver Rules → 10-05.2 Tạo Inbound Endpoints → 10-05.3 Lab 19 – VPC Peering Khởi tạo CloudFormation Templates → 19-02.1 Tạo Security Group → 19-02.2 Tạo EC2 instance (test peering) → 19-02.3 Tạo kết nối Peering → 19-04 Cấu hình Route Table (Cross-VPC) → 19-05 Bật Cross-Peer DNS → 19-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/",
	"title": "Ngày 14 - EC2 Auto Scaling",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-25 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon EC2 Auto Scaling EC2 Auto Scaling tự động điều chỉnh số lượng instance EC2 dựa trên nhu cầu. Lợi ích\nCo giãn năng lực linh hoạt. Tăng tính sẵn sàng cho ứng dụng. Tối ưu chi phí. Thành phần chính\nAuto Scaling Group (ASG): nhóm logic chứa các EC2 instance. Launch Template / Configuration: định nghĩa thông số instance. Scaling Policy: quy tắc thêm/bớt instance. Scaling Policy Simple / Step Scaling: thêm/bớt instance khi vượt ngưỡng. Target Tracking: duy trì một metric (ví dụ CPU = 50%). Scheduled Scaling: scale theo lịch định sẵn. Predictive Scaling: dùng ML dự đoán và scale chủ động. Ví dụ Target Tracking:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Tích hợp với Load Balancer ASG thường đi kèm Elastic Load Balancer (ELB). Instance mới sẽ tự đăng ký, instance bị hủy sẽ tự hủy đăng ký. Best practices cho Auto Scaling:\nDùng nhiều AZ để tăng độ sẵn sàng. Thiết lập cooldown hợp lý. Giám sát metric trên CloudWatch. Sử dụng lifecycle hook cho tác vụ tùy chỉnh. Kiểm thử policy trước khi đưa vào production. Các mô hình giá của EC2 On-Demand: Trả theo giờ/giây. Linh hoạt nhất nhưng chi phí cao. Reserved Instances: Cam kết 1 hoặc 3 năm để được giảm giá; gắn với loại/family cụ thể. Savings Plans: Cam kết 1 hoặc 3 năm; linh hoạt hơn giữa các family. Spot Instances: Dùng công suất dư thừa, giảm tới 90%; có thể bị thu hồi sau 2 phút báo trước. Nên kết hợp nhiều mô hình giá trong Auto Scaling Group để tối ưu chi phí.\nSo sánh giá:\nMô hình Giảm giá Linh hoạt Cam kết On-Demand 0% Cao Không Reserved 40-60% Thấp 1-3 năm Savings Plans 40-60% Trung bình 1-3 năm Spot 50-90% Thấp Không Hands-On Labs Lab 09 – AWS Support Plans Các gói hỗ trợ AWS → 09-01 Phân loại yêu cầu hỗ trợ → 09-02 Thay đổi gói hỗ trợ → 09-03 Quản lý ticket hỗ trợ → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/",
	"title": "Ngày 19 - Disaster Recovery trên AWS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-02 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Disaster Recovery (DR) trên AWS Disaster Recovery là quá trình khôi phục dịch vụ CNTT sau sự cố lớn (mất điện, thiên tai, phần cứng hỏng, tấn công mạng).\nRTO (Recovery Time Objective): Thời gian cần để khôi phục dịch vụ. RPO (Recovery Point Objective): Mức dữ liệu tối đa có thể mất (theo thời gian). Chiến lược DR (tăng dần theo độ phức tạp \u0026amp; chi phí) Backup \u0026amp; Restore\nChỉ lưu backup (snapshot EBS/RDS, S3/Glacier). Khôi phục hạ tầng mới khi gặp sự cố. RTO: vài giờ tới vài ngày. RPO: phụ thuộc tần suất backup. Chi phí: thấp nhất. Pilot Light\nDuy trì các dịch vụ lõi ở trạng thái thu nhỏ trên AWS. Scale lên toàn bộ sản xuất khi DR. RTO: hàng giờ. RPO: vài phút. Chi phí: trung bình. Warm Standby\nHệ thống hoàn chỉnh chạy ở quy mô giảm trên AWS. Scale lên khi failover. RTO: phút – giờ. RPO: giây – phút. Chi phí: cao hơn. Multi-Site (Active/Active hoặc Active/Passive)\nMôi trường production chạy song song giữa on-prem và AWS, hoặc giữa nhiều Region AWS. Có thể chuyển hướng traffic ngay lập tức (Route 53, Global Accelerator). RTO/RPO: gần như bằng 0. Chi phí: cao nhất. So sánh chiến lược DR:\nChiến lược RTO RPO Chi phí Độ phức tạp Backup \u0026amp; Restore Giờ – Ngày Giờ $ Thấp Pilot Light Giờ Phút $$ Trung bình Warm Standby Phút Giây $$$ Trung bình-Cao Multi-Site Giây Gần 0 $$$$ Cao Best Practices cho DR Lập kế hoạch Xác định yêu cầu RTO và RPO. Tài liệu hóa quy trình khôi phục. Nhận diện hệ thống và phụ thuộc quan trọng. Thiết lập kế hoạch truyền thông. Triển khai Tự động hóa quy trình khôi phục. Sử dụng nhiều AZ và Region. Triển khai cơ chế sao chép dữ liệu. Kiểm thử backup định kỳ. Kiểm thử Thực hiện diễn tập DR thường xuyên. Thử nghiệm quy trình khôi phục. Đo lường RTO/RPO thực tế. Cập nhật tài liệu. Hands-On Labs Lab 14 – AWS VM Import/Export (Phần 2) Import máy ảo lên AWS → 14-02.3 Deploy instance từ AMI → 14-02.4 Thiết lập ACL cho S3 Bucket → 14-03.1 Export máy ảo từ instance → 14-03.2 Dọn dẹp tài nguyên trên AWS → 14-05 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/",
	"title": "Ngày 24 - SCPs, Identity Center &amp; KMS",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-09 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Service Control Policy (SCP) Xác định quyền tối đa cho tài khoản; chỉ giới hạn chứ không cấp quyền. Áp dụng cho tài khoản hoặc OU; ảnh hưởng tất cả user/role, kể cả root; Deny ghi đè Allow. Ví dụ SCP (cấm xóa bucket):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Tình huống dùng SCP:\nNgăn tài khoản rời khỏi organization. Giới hạn region được phép tạo tài nguyên. Ép buộc yêu cầu mã hóa. Ngăn tắt các dịch vụ bảo mật. Bắt buộc gắn tag nhất định cho tài nguyên. Best practice:\nBắt đầu với least privilege. Thử nghiệm ở môi trường non-prod trước. Dùng explicit deny cho kiểm soát quan trọng. Ghi chú mục đích từng SCP. Rà soát và cập nhật định kỳ. AWS Identity Center (trước là AWS SSO) Tập trung hóa truy cập vào tài khoản AWS và ứng dụng bên ngoài. Nguồn danh tính: built-in, AWS Managed Microsoft AD, AD on-prem (trust/AD Connector), hoặc IdP ngoài. Permission Set định nghĩa quyền cho user/group trên tài khoản đích (Identity Center tạo IAM role tương ứng). Có thể cấp nhiều permission set cho một người dùng. Tính năng Identity Center:\nSingle sign-on cho nhiều tài khoản AWS. Tích hợp Microsoft Active Directory. Hỗ trợ SAML 2.0. MFA. Quản lý permission tập trung. Ghi log audit qua CloudTrail. AWS Key Management Service (KMS) Dịch vụ quản lý khóa bảo vệ dữ liệu, tích hợp sâu với các dịch vụ AWS và hỗ trợ audit đầy đủ. Điểm nổi bật\nTạo/quản lý khóa mà không cần tự vận hành HSM. Kiểm soát truy cập chi tiết với IAM \u0026amp; key policy; mọi thao tác được log trong CloudTrail. Các nhóm khóa\nKhóa do khách hàng quản lý (CMK), khóa do AWS quản lý và khóa thuộc AWS-owned. Loại khóa KMS:\nSymmetric: Một khóa duy nhất để mã hóa/giải mã (AES-256). Asymmetric: Cặp khóa public/private (RSA, ECC). Tính năng KMS:\nTự động xoay vòng khóa. Key policy và grant. Envelope encryption. Tích hợp với dịch vụ AWS. Log CloudTrail. Khóa multi-region. Hands-On Labs Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Phần 1) Tạo Policy và Role → 33-2.1 Tạo Group và User → 33-2.2 Tạo KMS Key → 33-3 Tạo S3 Bucket → 33-4.1 Upload dữ liệu lên S3 → 33-4.2 Lab 30 – IAM Restriction Policy Tạo Restriction Policy → 30-3 Tạo IAM Limited User → 30-4 Kiểm tra giới hạn của IAM User → 30-5 Dọn dẹp tài nguyên → 30-6 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/",
	"title": "Ngày 29 - Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-16 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon ElastiCache Dịch vụ cache in-memory managed cho Redis và Memcached giúp giảm độ trễ và giảm tải database.\nĐộ trễ micro giây, Multi-AZ với failover, scale đơn giản, tích hợp mã hóa/xác thực, vận hành tự động. Redis: hỗ trợ cấu trúc dữ liệu phong phú, backup, replication, cluster mode. Memcached: cache key-value đơn giản, mở rộng ngang với auto-discovery. Use case điển hình: tăng tốc web/mobile, cache truy vấn DB, session store, leaderboard, pub/sub, queue.\nElastiCache for Redis – điểm nổi bật:\nCấu trúc dữ liệu: strings, lists, sets, sorted sets, hashes, bitmap, hyperloglog. Persistence: snapshot và AOF. Replication: mô hình primary-replica tự failover. Cluster Mode: phân mảnh dữ liệu trên nhiều shard. Pub/Sub: nhắn tin thời gian thực. Lua Scripting: chạy logic phía server. Geospatial: truy vấn tọa độ. ElastiCache for Memcached – điểm nổi bật:\nMulti-threaded: tận dụng đa lõi. Auto Discovery: client tự nhận node mới. Horizontal Scaling: thêm/bớt node dễ dàng. Đơn giản: không persistence, cấu hình nhẹ. So sánh Redis vs Memcached:\nTiêu chí Redis Memcached Cấu trúc dữ liệu Phong phú Đơn giản (key-value) Persistence Có Không Replication Có Không Multi-AZ Có Không Backup/Restore Có Không Pub/Sub Có Không Đa luồng Không Có Chiến lược Caching Cache-Aside (Lazy Loading) Ứng dụng kiểm tra cache trước, nếu miss thì đọc DB và ghi lại vào cache. Ưu: chỉ cache dữ liệu thực sự cần. Nhược: cache miss gây trễ, dữ liệu có thể cũ. Write-Through Ghi đồng thời vào cache và database. Ưu: dữ liệu đọc luôn tươi. Nhược: ghi chậm hơn, có thể lưu trữ dữ liệu ít dùng. Write-Behind (Write-Back) Ghi vào cache tức thì, đồng bộ xuống DB bất đồng bộ. Ưu: ghi rất nhanh, giảm tải DB. Nhược: rủi ro mất dữ liệu nếu cache lỗi, phức tạp hơn. Tình huống sử dụng Session Store:\n# Lưu session user vào Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Lấy session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Ghi điểm vào sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Lấy top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Đếm số lần gọi trong 60 giây pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Labs thực hành Lab 43 – AWS Database Migration Service (DMS) (Phần 3) Kiểm tra dữ liệu trên S3 → 43-12 Tạo migration serverless → 43-13 Tạo thông báo sự kiện → 43-14 Theo dõi log → 43-15 Xử lý sự cố: Memory Pressure → 43-16 Xử lý sự cố: Table Error → 43-17 "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.4-day34-2025-10-23/",
	"title": "Ngày 34 - FastAPI Clean Architecture",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-23 (Thứ Năm)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Tổng quan Clean Architecture Tách rõ phần cấu hình, model, route và core logic để dễ mở rộng/kiểm thử. Giữ main.py nhẹ: chỉ khởi tạo app, load config và mount router. Dùng Pydantic model để chuẩn hóa request/response, đảm bảo contract trùng OpenAPI. backend/\r├── main.py\r├── core/\r│ └── config.py\r├── models/\r│ └── book.py\r├── routes/\r│ └── books.py\r└── services/\r└── books.py Cấu hình \u0026amp; Dependency core/config.py đọc biến môi trường, gom cấu hình CORS, API prefix, debug flag. Tận dụng dependency injection của FastAPI để truyền service vào router. Giúp thay datasource (in-memory → PostgreSQL) mà không đổi interface hàm. CORS \u0026amp; Độ ổn định API CORS chỉ mở cho origin cần thiết (http://localhost:3000 trong giai đoạn dev). Bật allow_methods=[\u0026quot;GET\u0026quot;] cho slice đầu tiên để giảm bề mặt tấn công. Đảm bảo /openapi.json luôn truy cập được nhằm phục vụ công cụ contract testing. Bắt đầu đơn giản, refactor sau Dùng repository in-memory để demo nhanh, sau đó mới thêm DB thật. Ghi chú TODO rõ ràng để không quên khi sang sprint mới. Logging tối giản, tập trung các lỗi quan trọng (timeout, data mismatch). Labs thực hành Refactor main.py chỉ còn khởi tạo app và register router. Viết service get_book_detail(id) trả dữ liệu giả theo spec. Cấu hình CORSMiddleware khớp URL của frontend mock/production. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.4-day39-2025-10-30/",
	"title": "Ngày 39 - NMT &amp; Tóm Tắt Văn Bản",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-30 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nDịch Máy Neuron (NMT) Tổng Quan Kiến Trúc Câu đầu vào được chuyển đổi thành biểu diễn số và được mã hóa thành biểu diễn sâu bởi một encoder 6 lớp, sau đó được giải mã bởi một decoder 6 lớp thành bản dịch ở ngôn ngữ đích.\nCác Lớp Encoder và Decoder Các lớp bao gồm:\nSelf-attention: Giúp mô hình tập trung vào các phần khác nhau của đầu vào Các lớp feed-forward: Xử lý thông tin Lớp encoder-decoder attention (chỉ decoder): Sử dụng biểu diễn sâu từ lớp encoder cuối cùng Ví Dụ Cơ Chế Attention Tác Vụ Dịch: \u0026ldquo;The woman took the empty magazine out of her gun\u0026rdquo;\nNgôn Ngữ Đích: Czech\nTrực Quan Hóa Self-Attention Khi dịch \u0026ldquo;magazine\u0026rdquo;, cơ chế attention:\nTạo liên kết attention mạnh giữa \u0026lsquo;magazine\u0026rsquo; và \u0026lsquo;gun\u0026rsquo; Điều này giúp dịch chính xác \u0026ldquo;magazine\u0026rdquo; thành \u0026ldquo;zásobník\u0026rdquo; (hộp đạn súng) Thay vì \u0026ldquo;časopis\u0026rdquo; (tạp chí tin tức) Tại Sao Attention Quan Trọng Attention = cơ chế giúp mô hình tập trung vào các phần quan trọng nhất của đầu vào khi tạo ra đầu ra\nNói cách khác: Attention = xử lý thông tin có chọn lọc thay vì tiêu thụ mọi thứ cùng một lúc\nTrong NLP, attention cho phép mô hình quyết định từ nào ảnh hưởng mạnh nhất đến việc hiểu một từ khác trong câu.\nChi Tiết Triển Khai NMT Các Thành Phần Kiến Trúc Mô Hình: Đầu Vào: Input tokens (ngôn ngữ nguồn) Target tokens (ngôn ngữ đích) Bước 1: Tạo Bản Sao Tạo hai bản sao cho mỗi input và target tokens (cần thiết ở các vị trí khác nhau của mô hình)\nBước 2: Encoder Một bản sao của input tokens → encoder Chuyển đổi thành vector key và value Đi qua lớp embedding → LSTM Bước 3: Pre-attention Decoder Một bản sao của target tokens → pre-attention decoder Dịch chuỗi sang phải + thêm token start-of-sentence (teacher forcing) Đi qua lớp embedding → LSTM Đầu ra trở thành vector query Lưu ý: Encoder và pre-attention decoder có thể chạy song song (không có phụ thuộc)\nBước 4: Chuẩn Bị cho Attention Lấy các vector query, key, value Tạo padding mask để xác định padding tokens Sử dụng bản sao của input tokens cho bước này Bước 5: Lớp Attention Truyền queries, keys, values và mask vào lớp attention\nĐầu ra là context vectors và mask Bước 6: Post-attention Decoder Bỏ mask, truyền context vectors qua:\nLSTM Lớp Dense LogSoftmax Bước 7: Đầu Ra Mô hình trả về:\nLog probabilities Bản sao của target tokens (để tính loss) Tóm Tắt Văn Bản Tóm tắt = nén nội dung trong khi vẫn giữ các ý chính\nHai Loại: 1. Tóm Tắt Extractive Khái niệm: Chọn các câu quan trọng nhất từ văn bản gốc\nĐặc Điểm:\nKhông viết lại văn bản Giữ nguyên từ ngữ gốc Giống như \u0026ldquo;đánh dấu các câu chính\u0026rdquo; Quy Trình (TextRank Cổ Điển):\nTách thành các câu Chuyển đổi câu thành embeddings Tính toán độ tương tự (cosine) Tạo đồ thị (câu là nodes) Xếp hạng sử dụng TextRank Chọn các câu xếp hạng cao nhất Kết Quả: Tập con của văn bản gốc\n2. Tóm Tắt Abstractive Khái niệm: Viết lại các ý chính trong các câu mới\nĐặc Điểm:\nTạo ra các câu chưa từng xuất hiện trong bản gốc Hiểu nội dung → paraphrase Yêu cầu mô hình mạnh (seq2seq, Transformer) Ví Dụ: Bài báo gốc thảo luận về quy trình điều tra của công tố viên\u0026hellip;\nTóm tắt được tạo:\n\u0026ldquo;Công tố viên: Cho đến nay không có video nào được sử dụng trong cuộc điều tra vụ tai nạn.\u0026rdquo;\nCâu này không tồn tại trong bản gốc nhưng nắm bắt ý chính.\nTóm Tắt Extractive vs Abstractive Đặc Điểm Extractive Abstractive Cách tiếp cận Chọn câu hiện có Tạo câu mới Sáng tạo Thấp Cao Độ phức tạp Đơn giản hơn Phức tạp hơn Độ chính xác Trung thành hơn với nguồn Có thể gây lỗi Mô hình TextRank, dựa trên đồ thị Seq2seq, Transformer Pipeline TextRank Tóm tắt extractive từng bước:\nKết hợp các bài báo → văn bản đầy đủ Tách các câu Chuyển đổi câu → vectors (embeddings) Tạo ma trận tương tự Xây dựng đồ thị (câu = nodes, cạnh = tương tự) Xếp hạng nodes sử dụng thuật toán TextRank Chọn các câu xếp hạng cao nhất → Tóm tắt Đây là thuật toán cổ điển thống trị trước deep learning!\nÔn Tập Cú Pháp và Ngữ Nghĩa Cú Pháp – Cấu Trúc Câu Cú Pháp kiểm tra cách các từ kết hợp để tạo thành các câu chính xác ngữ pháp.\nBao Gồm: Thứ tự từ: Tiếng Anh sử dụng S–V–O (Chủ Từ–Động Từ–Tân Từ) Cấu trúc cụm từ: NP (Cụm Danh Từ), VP (Cụm Động Từ), PP (Cụm Giới Từ) Các mối quan hệ phụ thuộc: Cách các từ liên quan đến nhau Liên Quan NLP: Gắn nhãn POS Phân tích cú pháp Nhận dạng thực thể Dịch máy Trả lời câu hỏi Ngữ Nghĩa – Ý Nghĩa của Từ và Câu Ngữ Nghĩa tập trung vào ý nghĩa độc lập với ngữ cảnh bên ngoài.\nBao Gồm: Ngữ nghĩa từ vựng: Ý nghĩa của từ Ngữ nghĩa thành phần: Ý nghĩa của câu Từ đồng nghĩa / trái nghĩa: Ý nghĩa tương tự/đối lập Cấp tính / hạ tính: Mối quan hệ chung/cụ thể Liên Quan NLP: Nhúng từ Các biện pháp tương tự Tìm kiếm ngữ nghĩa Phân loại văn bản Thực Dụng – Ý Định Có Ngữ Cảnh Thực Dụng nghiên cứu ý nghĩa từ ngữ cảnh, ý định của người nói và kiến thức thế giới thực.\nBao Gồm: Hàm ý: Ý nghĩa ẩn Chỉ dẫn: Tham chiếu phụ thuộc ngữ cảnh (cái này/cái kia/ở đây/bạn) Hành động nói: Hứa, yêu cầu, xin lỗi Lịch sự, tính chính thức, châm biếm: Tông điệu và ý định Liên Quan NLP: Hệ thống hội thoại Chatbots Phát hiện cảm xúc và châm biếm Mô hình ngôn ngữ ngữ cảnh (BERT, GPT) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.4-day44-2025-11-06/",
	"title": "Ngày 44 - Các Loại Attention: Self, Masked, Encoder-Decoder",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-06 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nBa Loại Attention trong Transformers Transformer sử dụng attention theo ba cách khác nhau. Hiểu biết từng cách là rất quan trọng.\nLoại 1: Self-Attention (Encoder) Định Nghĩa: Mỗi vị trí attend tới tất cả các vị trí trong chuỗi tương tự.\nTrường Hợp Sử Dụng: Trong encoder, chúng ta muốn mỗi từ hiểu bối cảnh của nó bằng cách nhìn vào tất cả các từ khác.\nVí Dụ:\nCâu: \u0026#34;The cat sat on the mat\u0026#34;\rCho từ \u0026#34;cat\u0026#34;:\r- Attend tới \u0026#34;The\u0026#34;: 0.15 (mạo từ)\r- Attend tới \u0026#34;cat\u0026#34;: 0.40 (chính nó)\r- Attend tới \u0026#34;sat\u0026#34;: 0.20 (động từ)\r- Attend tới \u0026#34;on\u0026#34;: 0.10\r- Attend tới \u0026#34;the\u0026#34;: 0.08\r- Attend tới \u0026#34;mat\u0026#34;: 0.07\rKết quả: Ngữ cảnh \u0026#34;cat\u0026#34; = kết hợp có trọng số của cả 6 từ Tại Sao Hữu Ích:\nNắm bắt ngữ cảnh câu đầy đủ Có thể xác định các mối quan hệ (chủ ngữ-động từ, tính từ-danh từ, v.v.) Mỗi từ nhận thông tin từ toàn bộ câu Triển Khai:\nQ = K = V = Đầu vào (cùng một nguồn!)\rattention(Q, K, V) = softmax(Q×K^T / √d_k) × V Vì Q, K, V đến từ cùng một nơi, nó được gọi là \u0026ldquo;self-attention\u0026rdquo;.\nLoại 2: Masked Self-Attention (Decoder) Vấn Đề: Trong quá trình huấn luyện, nếu decoder có thể \u0026ldquo;nhìn thấy\u0026rdquo; các từ tương lai, nó gian lận!\nVí Dụ - Vấn Đề:\nTác Vụ: Dịch \u0026#34;Je suis heureux\u0026#34; → \u0026#34;I am happy\u0026#34;\rHuấn Luyện:\rBước 1: Dự đoán \u0026#34;am\u0026#34; bằng cách sử dụng... \u0026#34;am\u0026#34; (nó có thể nhìn thấy câu trả lời!)\rBước 2: Dự đoán \u0026#34;happy\u0026#34; bằng cách sử dụng \u0026#34;I am happy\u0026#34; (biết câu trả lời!)\rBước 3: Dự đoán \u0026#34;happy\u0026#34; đã xong (gian lận!)\rKết Quả: Mô hình huấn luyện hoàn hảo nhưng thất bại vào thời gian kiểm tra! Giải Pháp: Che (ẩn) các vị trí tương lai trong quá trình self-attention.\nMasked Self-Attention:\nThay vì: Chúng ta làm:\r[0.30, 0.33, 0.37] [0.30, -∞, -∞]\r[0.26, 0.37, 0.37] → [0.26, 0.37, -∞]\r[0.25, 0.36, 0.39] [0.25, 0.36, 0.39]\rSau softmax:\r[1.00, 0.00, 0.00]\r[0.30, 0.70, 0.00]\r[0.25, 0.36, 0.39]\r(chuẩn hóa) Ma Trận Mask:\nMask = [1, 0, 0]\r[1, 1, 0]\r[1, 1, 1]\rHoặc: -∞ cho các vị trí được che Hiệu Ứng:\nVị Trí 0: Attend tới vị trí 0 chỉ\rVị Trí 1: Attend tới vị trí 0, 1 chỉ\rVị Trí 2: Attend tới vị trí 0, 1, 2\rDecoder chỉ có thể sử dụng thông tin quá khứ! Tại Sao Điều Này Hoạt Động:\nTrong quá trình huấn luyện, có thể sử dụng tạo hình autoregressive Trong quá trình suy luận, tạo từ từng từ một một cách tự nhiên Ngăn chặn mô hình \u0026ldquo;nhìn thấy câu trả lời\u0026rdquo; Loại 3: Encoder-Decoder Attention Mục Đích: Decoder attend tới đầu ra encoder.\nVí Dụ:\nEncoder xử lý: \u0026#34;Je suis heureux\u0026#34; (Tiếng Pháp)\rTạo ra: Context vectors C\rDecoder xử lý: \u0026#34;\u0026#34; (bắt đầu rỗng)\rĐể tạo ra từ đầu tiên:\r- Query: từ decoder (tôi nên dịch cái gì?)\r- Key, Value: từ encoder (tôi nên nhìn vào các từ tiếng Pháp nào?)\rKết Quả: Decoder attend tới các từ tiếng Pháp để tạo ra tiếng Anh Khác Biệt Chính So Với Self-Attention:\nSelf-Attention: Encoder-Decoder:\rQ, K, V đều từ đầu vào Q từ decoder\rCùng chuỗi K, V từ encoder\rAttend trong bản thân Attend tới chuỗi khác Trường Hợp Sử Dụng:\nDecoder nhìn lại đầu ra encoder Cho phép dịch: Tiếng Pháp → Tiếng Anh Cho phép tóm tắt: Tài Liệu → Tóm Tắt Nói chung hữu ích cho các tác vụ seq2seq So Sánh: Cả Ba Loại Loại Nguồn Q Nguồn K, V Mục Đích Self-Attention Đầu vào Đầu vào Hiểu ngữ cảnh trong chuỗi tương tự Masked Self-Attention Đầu vào Đầu vào (tương lai che) Tạo hình autoregressive, ngăn gian lận Encoder-Decoder Decoder Encoder Hiểu xuyên chuỗi Masked Attention Chi Tiết Toán Học Trước khi che:\nAttention = softmax(Q×K^T / √d_k) × V Với che:\nScores = Q×K^T / √d_k\rMa trận Mask M:\rM[i,j] = 0 nếu j \u0026lt;= i (được phép)\rM[i,j] = -∞ nếu j \u0026gt; i (che tương lai)\rMasked_scores = Scores + M\rAttention = softmax(Masked_scores) × V Ví Dụ với Các Số Thực Điểm attention gốc (3×3):\n[0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] Ma trận mask:\n[0, -∞, -∞]\r[0, 0, -∞]\r[0, 0, 0] Sau khi thêm mask:\n[0.1, -∞, -∞]\r[0.4, 0.5, -∞]\r[0.7, 0.8, 0.9] Sau softmax (áp dụng exp và chuẩn hóa):\nexp(0.1) / exp(0.1) = 1.0, softmax([0.1]) = [1.0]\rVì vậy:\rHàng 0: [1.0, 0, 0]\rexp(0.4) ≈ 1.49, exp(0.5) ≈ 1.65\rHàng 1: [1.49/(1.49+1.65), 1.65/(1.49+1.65), 0] ≈ [0.47, 0.53, 0]\rHàng 2: softmax([0.7, 0.8, 0.9]) (tất cả được phép) Trọng số attention cuối cùng:\n[1.0, 0.0, 0.0]\r[0.47, 0.53, 0.0]\r[0.25, 0.33, 0.42] Hiểu Biết Chính: Vị trí 2 chỉ có thể sử dụng thông tin từ các vị trí 0, 1, 2 (không phải tương lai)\nLuồng Attention Transformer Hoàn Chỉnh ĐẦU VÀO: \u0026#34;Je suis heureux\u0026#34;\r↓\rCÁC LỚP ENCODER (lặp 6 lần):\r├─ Self-Attention: Mỗi từ tiếng Pháp attend tới tất cả các từ tiếng Pháp\r├─ Feed-Forward\r→ Đầu ra: C (vector ngữ cảnh tiếng Pháp)\rCÁC LỚP DECODER (lặp 6 lần):\r├─ Masked Self-Attention: Mỗi từ được tạo attend tới các từ trước đó\r├─ Encoder-Decoder Attention: Từ được tạo attend tới ngữ cảnh tiếng Pháp\r├─ Feed-Forward\r→ Đầu ra: Logits cho dự đoán từ tiếp theo\rĐẦU RA: \u0026#34;I am happy\u0026#34; Những Hiểu Biết Chính ✅ Self-Attention: Hiểu lưỡng chiều (encoder) ✅ Masked Attention: Tạo hình một chiều (decoder) ✅ Encoder-Decoder: Chuyển giao xuyên chuỗi ✅ Masking ngăn gian lận: Mô hình không thể sử dụng thông tin tương lai\nTại Sao Không Luôn Sử Dụng Cả Ba? BERT (Encoder-only): Chỉ sử dụng self-attention (lưỡng chiều, tốt cho phân loại) GPT (Decoder-only): Chỉ sử dụng masked self-attention (autoregressive, tốt cho tạo hình) T5 (Đầy Đủ): Sử dụng cả ba (cân bằng, tốt cho seq2seq) Tiếp Theo: Triển Khai Bây giờ chúng ta hiểu ba loại attention, chúng ta sẽ thấy cách triển khai chúng trong code!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.4-day49-2025-11-13/",
	"title": "Ngày 49 - T5: Text-to-Text Transfer Transformer",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-13 (Thứ Năm)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nT5: Mô Hình Transfer Learning Tối Ưu Công bố bởi Google vào năm 2019, T5 (Text-to-Text Transfer Transformer) thống nhất TẤT CẢ các tác vụ NLP thành một khung.\nHiểu biết chính: Mọi tác vụ NLP có thể được diễn đạt dưới dạng text-to-text.\nCuộc Cách Mạng T5 Trước T5: Các Mô Hình Riêng Theo Tác Vụ Phân Tích Cảm Xúc:\rĐầu Vào: \u0026#34;I love this movie!\u0026#34;\rĐầu Ra: Positive (phân loại)\rMô Hình: bert-sentiment-specific\rDịch Máy:\rĐầu Vào: \u0026#34;I love this movie!\u0026#34;\rĐầu Ra: \u0026#34;J\u0026#39;adore ce film!\u0026#34;\rMô Hình: marianmt-en-fr\rTóm Tắt:\rĐầu Vào: \u0026#34;The movie was great... [1000 từ]\u0026#34;\rĐầu Ra: \u0026#34;Great movie.\u0026#34;\rMô Hình: bart-summarization\rTrả Lời Câu Hỏi:\rĐầu Vào: \u0026#34;What is the capital?\u0026#34; + Đoạn Wikipedia\rĐầu Ra: \u0026#34;Paris\u0026#34; (trích xuất span)\rMô Hình: bert-qa-specific\rVấn Đề: Các loại đầu vào/đầu ra khác nhau, các mô hình khác nhau! Sau T5: Khung Thống Nhất Phân Tích Cảm Xúc:\rĐầu Vào: \u0026#34;sentiment: I love this movie!\u0026#34;\rĐầu Ra: \u0026#34;positive\u0026#34;\rDịch Máy:\rĐầu Vào: \u0026#34;translate English to French: I love this movie!\u0026#34;\rĐầu Ra: \u0026#34;J\u0026#39;adore ce film!\u0026#34;\rTóm Tắt:\rĐầu Vào: \u0026#34;summarize: The movie was great... [1000 từ]\u0026#34;\rĐầu Ra: \u0026#34;Great movie.\u0026#34;\rTrả Lời Câu Hỏi:\rĐầu Vào: \u0026#34;question: What is the capital? context: Paris is...\u0026#34;\rĐầu Ra: \u0026#34;Paris\u0026#34;\rGiải Pháp: MỘT mô hình, MỘT kiến trúc, MỘT quy trình huấn luyện!\rChỉ cần thay đổi tiền tố tác vụ! Kiến Trúc T5 Transformer Encoder-Decoder T5 (Cả Hai Lưỡng Chiều Đầy Đủ):\rĐầu Vào: \u0026#34;translate English to French: Hello world\u0026#34;\r↓\rEncoder (12 lớp transformers lưỡng chiều)\r├─ Lớp 1: Self-attention trên tất cả các token\r├─ Lớp 2: Self-attention trên tất cả các token\r├─ ...\r├─ Lớp 12: Self-attention trên tất cả các token\r└─ Đầu Ra: Đại diện ngữ cảnh\r↓\rDecoder (12 lớp, có thể chú ý đến encoder)\r├─ Lớp 1: Self-attention (che), Encoder-attention\r├─ Lớp 2: Self-attention (che), Encoder-attention\r├─ ...\r├─ Lớp 12: Self-attention (che), Encoder-attention\r└─ Đầu Ra: \u0026#34;Bonjour le monde\u0026#34;\rSự Khác Biệt Chính So Với BERT:\r├─ BERT: Encoder only (hiểu)\r├─ T5: Encoder + Decoder (hiểu và tạo)\r└─ GPT: Decoder only (tạo) Kích Thước Mô Hình T5-small: 60 triệu tham số\r├─ Encoder: 6 lớp, kích thước ẩn 512\r├─ Decoder: 6 lớp, kích thước ẩn 512\r└─ Nhanh, dấu chân bộ nhớ nhỏ\rT5-base: 220 triệu tham số\r├─ Encoder: 12 lớp, kích thước ẩn 768\r├─ Decoder: 12 lớp, kích thước ẩn 768\r└─ Cân bằng tốt\rT5-large: 770 triệu tham số\r├─ Encoder: 24 lớp, kích thước ẩn 1024\r├─ Decoder: 24 lớp, kích thước ẩn 1024\r└─ Hiệu suất cao\rT5-3B và T5-11B: Thậm chí lớn hơn\r├─ Được sử dụng cho các tác vụ rất khó khăn\r└─ Hiệu suất ấn tượng trên mọi thứ Hệ Thống Tiền Tố Tác Vụ Thiên tài của T5: Các tiền tố tác vụ đơn giản hướng dẫn mô hình\nVí Dụ Tiền Tố Tác Vụ:\r1. Phân Loại (Cảm Xúc):\rĐầu Vào: \u0026#34;sentiment: I loved this movie!\u0026#34;\rĐầu Ra: \u0026#34;positive\u0026#34;\r2. Dịch Thuật:\rĐầu Vào: \u0026#34;translate English to French: Hello\u0026#34;\rĐầu Ra: \u0026#34;Bonjour\u0026#34;\r3. Tóm Tắt:\rĐầu Vào: \u0026#34;summarize: Tài liệu dài... [500 từ] ...kết thúc\u0026#34;\rĐầu Ra: \u0026#34;Tóm tắt các điểm chính\u0026#34;\r4. Trả Lời Câu Hỏi:\rĐầu Vào: \u0026#34;question: Who wrote Hamlet? context: Shakespeare...\u0026#34;\rĐầu Ra: \u0026#34;Shakespeare\u0026#34;\r5. Paraphrase:\rĐầu Vào: \u0026#34;paraphrase: The cat is on the mat.\u0026#34;\rĐầu Ra: \u0026#34;A feline rests on the floor mat.\u0026#34;\r6. Entailment:\rĐầu Vào: \u0026#34;nli: A person is riding a bike. A man cycles.\u0026#34;\rĐầu Ra: \u0026#34;entailment\u0026#34; hoặc \u0026#34;neutral\u0026#34; hoặc \u0026#34;contradiction\u0026#34;\r7. Zero-shot Classification:\rĐầu Vào: \u0026#34;znli: I liked it. premise: The speaker liked something.\u0026#34;\rĐầu Ra: \u0026#34;entailment\u0026#34;\r8. Lựa Chọn Nhiều:\rĐầu Vào: \u0026#34;multiple_choice: Paris is the capital of?\r(A) France (B) Germany (C) Italy\u0026#34;\rĐầu Ra: \u0026#34;A\u0026#34;\rTại Sao Nó Hoạt Động:\r├─ Cùng một mô hình học tất cả các mẫu này\r├─ Tiền tố tác vụ hoạt động như một hướng dẫn\r├─ Có thể mở rộng quy mô cho bất kỳ tác vụ text-in/text-out nào\r└─ Thanh lịch! T5 Pre-training: SPAN CORRUPTION T5 giới thiệu một mục tiêu pre-training mới: Span Corruption\nCách Nó Hoạt Động Gốc: \u0026#34;The quick brown fox jumps over the lazy dog.\u0026#34;\rQuy Trình Span Corruption:\rBước 1: Chọn ngẫu nhiên các spans để hỏng hóc\r├─ Chọn 15% của các token\r├─ Nhóm thành các spans\r└─ Ví Dụ: [quick brown] và [lazy]\rBước 2: Thay thế các spans bằng các token sentinel\r├─ \u0026#34;The [X] fox jumps over the [Y] dog.\u0026#34;\r├─ [X] là một phần giữ chỗ độc đáo cho span đầu tiên\r├─ [Y] là một phần giữ chỗ độc đáo cho span thứ hai\rBước 3: Dự đoán các spans bị mất\rĐầu Vào: \u0026#34;The [X] fox jumps over the [Y] dog.\u0026#34;\rĐầu Ra: \u0026#34;[X] quick brown [Y] lazy [Z]\u0026#34;\r(bao gồm token bắt đầu [Z])\rTác Vụ: Tái cấu trúc các spans bị hỏng hóc! Tại Sao Span Corruption \u0026gt; MLM BERT\u0026#39;s MLM:\r├─ Mask các token riêng lẻ: \u0026#34;The [MASK] brown [MASK] jumps\u0026#34;\r├─ Dự đoán từng cái một cách độc lập\r├─ Vấn Đề: Dễ hơn hỏng hóc văn bản thực tế\r└─ Các token không tương quan\rT5\u0026#39;s Span Corruption:\r├─ Hỏng hóc các spans cấp độ từ: \u0026#34;The [X] [Y] fox jumps\u0026#34;\r├─ Tạo toàn bộ spans\r├─ Lợi Ích: Khó hơn, thực tế hơn\r└─ Các spans có tương quan (quan trọng cho việc tạo!)\rKết Quả: T5 tốt hơn với các tác vụ tạo! Công Thức Toán Học Loss span corruption:\rĐối với mỗi span s bị hỏng hóc có độ dài L:\rLoss = -∑(log P(y_i | y_\u0026lt;i, x))\rTrong Đó:\r├─ y_i = token i của span bị hỏng hóc\r├─ y_\u0026lt;i = các token được tạo trước đó\r├─ x = đầu vào với các spans bị hỏng hóc\r└─ Sum qua tất cả các token trong span\rVề cơ bản đây là ngôn ngữ mô hình loss\rNhưng trên các spans thay vì các token riêng lẻ! Dữ Liệu Pre-training T5 Corpus C4 T5 được huấn luyện trên C4: Colossal Clean Crawled Corpus\nThống Kê C4:\r├─ Nguồn: 750 tỷ tài liệu web\r├─ Kích Thước: 800 GB văn bản\r├─ 200 tỷ tokens\r├─ Xử Lý: Loại bỏ trùng lặp, lọc, làm sạch\r├─ Ngôn Ngữ: Chủ yếu tiếng Anh (nhưng tồn tại các phiên bản đa ngôn ngữ)\rCách nó được tạo:\r├─ Lấy Common Crawl (ảnh chụp web)\r├─ Bộ Lọc: Xóa nội dung xấu\r├─ Làm Sạch: Sửa HTML, loại bỏ boilerplate\r├─ Loại Bỏ Trùng Lặp: Xóa các bản sao gần đúng\r├─ Kết Quả: Corpus 800GB chất lượng cao!\rSo Sánh:\r├─ BERT pre-training: 3.3 tỷ từ pieces (~13GB)\r├─ T5 pre-training: 200 tỷ tokens (~800GB)\r└─ T5 được huấn luyện trên 60x nhiều dữ liệu! Pre-training Đa Tác Vụ T5 giới thiệu pre-training trên nhiều tác vụ đồng thời\nTrong quá trình pre-training, T5 thấy các mục tiêu đa dạng:\r50% Span Corruption (tác vụ chính):\r├─ \u0026#34;The [X] fox jumps over [Y] dog\u0026#34;\r├─ Dự đoán: \u0026#34;[X] quick brown [Y] lazy\u0026#34;\r50% Các mục tiêu riêng theo tác vụ:\r├─ Dịch (15%): \u0026#34;translate en to fr: Hello\u0026#34;\r├─ Tóm Tắt (15%): \u0026#34;summarize: Article... \u0026#34;\r├─ QA (15%): \u0026#34;question: What... context: ...\u0026#34;\r├─ Các tác vụ khác (5%): Các tác vụ NLP khác nhau\rTại Sao?\r├─ Tiếp xúc với mô hình để đa dạng tác vụ sớm\r├─ Chuyển giao tốt hơn sang các tác vụ hạ nguồn\r├─ Mô hình học cùng trọng số hoạt động cho nhiều thứ\r└─ Dẫn đến các đại diện khái quát hơn So Sánh T5 với BERT T5 BERT\r── ────\rKiến Trúc Enc-Dec Enc only\rLoại Che Span Token\rDữ Liệu Pre-train 800GB (C4) 13GB (Wiki+Books)\rQuy Mô 220M-11B 110M-340M\rTạo Văn Bản Xuất Sắc Không Thể Tạo\rHiểu Biết Tốt Xuất Sắc\rThời Gian Huấn Luyện ~31 ngày (TPU) ~4 ngày (TPU)\rTốt Nhất Cho Tạo Văn Bản Hiểu Biết\rLinh Hoạt Text-to-text Fine-tuning riêng theo tác vụ\rCái Nào Để Sử Dụng?\r├─ Dịch Thuật: T5 (xử lý chuỗi tốt hơn)\r├─ Phân Loại: BERT (nhanh hơn, đơn giản hơn)\r├─ Tóm Tắt: T5 (tác vụ tạo)\r├─ Cảm Xúc: BERT (tác vụ phân loại)\r├─ Trả Lời Câu Hỏi: Cả hai hoạt động, nhưng T5 linh hoạt hơn Hiệu Suất T5 Benchmark GLUE (Hiểu Biết) Tác Vụ T5-base Trước Đây Tốt Nhất\rCảm Xúc (SST-2) 94.9% 92.1%\rTương Tự (STS-B) 89.2% 87.1%\rSuy Luận (RTE) 93.5% 91.0%\rTrả Lời Câu Hỏi (QNLI) 95.1% 93.2%\rParaphrase (MRPC) 89.2% 87.1%\rCải Thiện Trung Bình: +2-3% Điểm BLEU (Tạo Văn Bản) Tác Vụ T5-base Trước Đây Tốt Nhất\rAnh sang Đức 28.4 23.1\rAnh sang Pháp 41.0 35.2\rAnh sang Romanian 34.2 29.1\rT5 tốt hơn nhiều ở việc tạo! Các Biến Thể T5 T5\r├─ T5-small (60M)\r├─ T5-base (220M)\r├─ T5-large (770M)\r├─ T5-3B\r├─ T5-11B\r└─ mT5 (đa ngôn ngữ, 13 biến thể)\rMỗi cái tỷ lệ khác nhau:\r├─ Nhỏ hơn = Nhanh hơn, ít bộ nhớ\r└─ Lớn hơn = Hiệu suất tốt hơn, nhiều tham số hơn Tại Sao T5 Quan Trọng ✅ Thống Nhất: Một mô hình, một kiến trúc, nhiều tác vụ ✅ Tính Đơn Giản Text-to-Text: Không thiết kế các kiến trúc riêng theo tác vụ ✅ Span Corruption: Mục tiêu pre-training tốt hơn ✅ Dữ Liệu Lớn: Corpus 800GB cho thấy lợi ích mở rộng quy mô ✅ Linh Hoạt: Có thể giải quyết bất kỳ tác vụ tạo nào ✅ Baseline Mạnh Mẽ: Hiệu suất tốt nhất trên nhiều benchmark\nCác Lợi Ích Chính Tiền Tố Tác Vụ: Cách đơn giản nhưng mạnh mẽ để hướng dẫn các mô hình Span Corruption: Tốt hơn token masking cho việc tạo Dữ Liệu Pre-training Lớn: Dữ liệu Hơn = Chuyển giao tốt hơn Encoder-Decoder: Hoàn hảo cho các tác vụ seq2seq Khung Thống Nhất: Đơn Giản Hóa Hệ Thống NLP Sự Phát Triển của Transfer Learning Word2Vec (2013) → ELMo (2015) → BERT (2018) → T5 (2019) → GPT-3 (2020) → Hiện Tại\rQuy Mô: Đơn Giản → Trung Bình → Lớn → Rất Lớn → Khổng Lồ → Khổng Lồ Hơn?\rKiến Trúc: Embeddings → BiLSTM → BiTransformer → Enc-Dec → Dec-only\rDữ Liệu: Triệu → Tỷ → 13GB → 800GB → 570GB → Tỷ tỷ?\rHiệu Suất: Cơ Bản → Tốt → Xuất Sắc → Tốt Hơn → Tuyệt Vời → Siêu Nhân? Tiếp Theo: Fine-tuning Ngày 50: Cách lấy T5 (hoặc BERT) và fine-tune cho các tác vụ cụ thể T5 là mô hình chung sinh rất tốt. Chỉ với một tiền tố tác vụ và 100-1000 ví dụ, bạn có thể xây dựng các hệ thống hoạt động tốt hơn các mô hình chuyên dụng từ 2 năm trước!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.4-s3-onprem/",
	"title": "Truy cập S3 từ môi trường truyền thống",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, bạn sẽ tạo một Interface Endpoint để truy cập Amazon S3 từ môi trường truyền thống mô phỏng. Interface Endpoint sẽ cho phép bạn định tuyến đến Amazon S3 qua kết nối VPN từ môi trường truyền thống mô phỏng của bạn.\nTại sao nên sử dụng Interface Endpoint:\nCác Gateway endpoints chỉ hoạt động với các tài nguyên đang chạy trong VPC nơi chúng được tạo. Interface Endpoint hoạt động với tài nguyên chạy trong VPC và cả tài nguyên chạy trong môi trường truyền thống. Khả năng kết nối từ môi trường truyền thống của bạn với aws cloud có thể được cung cấp bởi AWS Site-to-Site VPN hoặc AWS Direct Connect. Interface Endpoint cho phép bạn kết nối với các dịch vụ do AWS PrivateLink cung cấp. Các dịch vụ này bao gồm một số dịch vụ AWS, dịch vụ do các đối tác và khách hàng AWS lưu trữ trong VPC của riêng họ (gọi tắt là Dịch vụ PrivateLink endpoints) và các dịch vụ Đối tác AWS Marketplace. Đối với workshop này, chúng ta sẽ tập trung vào việc kết nối với Amazon S3. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/",
	"title": "Tuần 4 - Dịch vụ Lưu trữ trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-09-29 đến 2025-10-03\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 4 Tuần này tập trung vào các dịch vụ lưu trữ của AWS, từ S3 cho tới giải pháp hybrid và chiến lược DR.\nNội dung chính Amazon S3 và các lớp lưu trữ. S3 Static Website Hosting. S3 Glacier cho lưu trữ dài hạn. AWS Snow Family. AWS Storage Gateway. Chiến lược Disaster Recovery. AWS Backup. Labs thực hành Lab 13: AWS Backup. Lab 14: AWS VM Import/Export. Lab 24: AWS Storage Gateway. Lab 25: Amazon FSx. Lab 57: Amazon S3 \u0026amp; CloudFront. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/",
	"title": "Ngày 05 - AWS Well-Architected Framework",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-12 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nKhám phá AWS Well-Architected Framework Bộ nguyên tắc thiết kế và phương pháp thực hành tốt nhất để xây dựng kiến trúc cloud tin cậy, bảo mật, hiệu quả và tiết kiệm chi phí. Công cụ Well-Architected trên Console hỗ trợ tự đánh giá và đề xuất hướng cải thiện. Sáu trụ cột của Well-Architected Framework 1. Vận hành xuất sắc (Operational Excellence) Tập trung vận hành và giám sát hệ thống. Liên tục cải thiện quy trình. Tự động hóa thay đổi. Phản ứng kịp thời trước sự kiện. 2. Bảo mật (Security) Bảo vệ thông tin và hệ thống. Quản lý danh tính và quyền truy cập. Thiết lập cơ chế giám sát phát hiện. Bảo vệ hạ tầng. Giữ an toàn cho dữ liệu. 3. Độ tin cậy (Reliability) Tự động phục hồi khi gặp sự cố. Mở rộng ngang để tăng khả năng chịu lỗi. Kiểm thử kịch bản khôi phục. Quản lý thay đổi bằng tự động hóa. 4. Hiệu năng (Performance Efficiency) Sử dụng tài nguyên tính toán hiệu quả. Chọn đúng loại tài nguyên. Giám sát hiệu năng. Đưa ra quyết định dựa trên dữ liệu. 5. Tối ưu chi phí (Cost Optimization) Tránh chi tiêu không cần thiết. Hiểu rõ mô hình sử dụng. Chọn dịch vụ phù hợp. Tối ưu liên tục theo thời gian. 6. Phát triển bền vững (Sustainability) Giảm tác động đến môi trường. Hiểu dấu chân carbon của hệ thống. Tối đa hóa mức sử dụng tài nguyên. Ưu tiên dịch vụ managed. Ôn lại Best Practices Nguyên tắc thiết kế Ngừng đoán dung lượng: Dùng auto scaling. Kiểm thử ở quy mô sản xuất: Dễ dàng nhân bản môi trường. Tự động hóa thử nghiệm kiến trúc: Áp dụng hạ tầng như mã (IaC). Cho phép kiến trúc tiến hóa: Thiết kế linh hoạt để thay đổi. Ra quyết định dựa trên dữ liệu: Luôn giám sát \u0026amp; đo lường. Cải thiện qua game day: Luyện tập kịch bản sự cố. Tổng kết Tuần 1 Tuần này đã hoàn thành kiến thức nền tảng về AWS:\n✅ Hiểu về Cloud Computing và lợi ích\n✅ Nắm được AWS Global Infrastructure\n✅ Biết cách sử dụng AWS Management Tools\n✅ Học các chiến lược tối ưu chi phí\n✅ Nắm AWS Well-Architected Framework\nLabs đã hoàn tất: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/",
	"title": "Ngày 10 - Elastic Load Balancing",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-19 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Elastic Load Balancing (ELB) Tổng quan Dịch vụ fully-managed phân phối lưu lượng tới nhiều target (EC2, container, v.v.). Hỗ trợ giao thức HTTP, HTTPS, TCP, TLS. Có thể triển khai ở subnet public hoặc private. Cung cấp DNS name; chỉ Network Load Balancer hỗ trợ IP tĩnh. Tích hợp health check và ghi log truy cập (lưu S3). Hỗ trợ sticky session (session affinity). Các loại chính: Application, Network, Classic và Gateway Load Balancer. Application Load Balancer (ALB) Hoạt động ở tầng 7 (HTTP/HTTPS). Hỗ trợ định tuyến theo path (ví dụ /mobile vs /desktop). Target: EC2, Lambda, địa chỉ IP, container (ECS/EKS). Tính năng nổi bật của ALB:\nĐịnh tuyến theo host name. Định tuyến theo path. Định tuyến dựa trên HTTP header. Định tuyến theo query string parameter. Hỗ trợ WebSocket. Hỗ trợ HTTP/2. Network Load Balancer (NLB) Hoạt động ở tầng 4 (TCP/TLS). Hỗ trợ IP tĩnh, xử lý hàng triệu request/giây. Target: EC2, địa chỉ IP, container (ECS/EKS). Điểm mạnh của NLB:\nĐộ trễ cực thấp. Cung cấp địa chỉ IP tĩnh. Giữ nguyên nguồn IP truy cập. Hỗ trợ kết nối TCP dài hạn. Có thể chấm dứt TLS (TLS termination). Gateway Load Balancer (GWLB) Hoạt động ở tầng 3 (gói IP). Sử dụng giao thức GENEVE trên cổng 6081. Định tuyến lưu lượng đến các virtual appliance như firewall, công cụ monitor. Danh sách đối tác: aws.amazon.com/elasticloadbalancing/partners Khám phá AWS Advanced Networking – Specialty Study Guide Sách hướng dẫn chính thức bao quát chủ đề kỳ thi, nguyên tắc thiết kế mạng trên AWS và các tình huống kiến trúc thực tế. Hands-On Labs Lab 20 – AWS Transit Gateway Chuẩn bị môi trường → 20-02 Tạo Transit Gateway → 20-03 Tạo TGW Attachment → 20-04 Tạo TGW Route Table → 20-05 Thêm route TGW vào Route Table của VPC → 20-06 Tổng kết Tuần 2 Tuần này đã hoàn thành kiến thức về AWS Networking:\n✅ Amazon VPC và Subnet\n✅ Security Group và NACL\n✅ VPC Peering và Transit Gateway\n✅ VPN và Direct Connect\n✅ Elastic Load Balancing (ALB, NLB, GWLB)\nLabs đã hoàn tất: 4 labs (VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/",
	"title": "Ngày 15 - Lightsail, EFS &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-09-26 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Amazon Lightsail Dịch vụ compute đơn giản với giá cố định hàng tháng (bắt đầu ~3,5 USD/tháng). Bao gồm băng thông đi kèm với giá thấp hơn EC2. Lý tưởng cho workload nhỏ, môi trường dev/test. Hỗ trợ snapshot để sao lưu. Chạy trong VPC được quản lý và có thể kết nối VPC tiêu chuẩn qua peering (một lần nhấp). Trường hợp dùng Lightsail:\nỨng dụng web đơn giản. Trang WordPress. Môi trường phát triển/thử nghiệm. Ứng dụng doanh nghiệp nhỏ. Học tập và thử nghiệm. So sánh Lightsail và EC2:\nTiêu chí Lightsail EC2 Giá Cố định hàng tháng Trả theo dùng Độ phức tạp Đơn giản Nhiều tùy chọn Khả năng mở rộng Giới hạn Không giới hạn Đối tượng Dự án nhỏ Doanh nghiệp Amazon EFS (Elastic File System) Dịch vụ hệ thống file NFSv4 do AWS quản lý, nhiều EC2 có thể mount đồng thời. Tự động scale tới hàng petabyte. Trả tiền theo dung lượng thực tế sử dụng (khác với EBS phải provision). Có thể mount từ on-prem thông qua VPN hoặc Direct Connect. Tính năng EFS:\nTruy cập đồng thời từ nhiều instance. Tự động mở rộng. Dịch vụ cấp độ Region (đa AZ). Quản lý vòng đời. Mã hóa dữ liệu khi lưu trữ và truyền tải. Các lớp lưu trữ EFS:\nStandard: File truy cập thường xuyên. Infrequent Access (IA): Chi phí thấp cho file ít truy cập. One Zone: Một AZ để tiết kiệm chi phí. Amazon FSx Các hệ thống file được quản lý, mở rộng cho Windows, Lustre, NetApp ONTAP. AWS lo phần thiết lập, mở rộng, sao lưu. Truy cập từ EC2, máy chủ on-prem hoặc người dùng qua giao thức SMB/NFS. Các biến thể FSx:\nFSx for Windows File Server Hệ thống file Windows gốc. Hỗ trợ giao thức SMB. Tích hợp Active Directory. Hỗ trợ DFS namespace. FSx for Lustre Phù hợp workload HPC. Machine Learning, mô phỏng. Độ trễ dưới mili giây. Tích hợp S3. FSx for NetApp ONTAP Hỗ trợ đa giao thức (NFS, SMB, iSCSI). Giảm trùng lặp dữ liệu, nén. Snapshots và replication. AWS Application Migration Service (MGN) Dịch vụ migrate/replicate máy chủ vật lý hoặc ảo lên AWS để DR hoặc hiện đại hóa. Liên tục sao chép máy nguồn sang instance staging EC2 nhẹ. Khi cut-over, MGN tạo EC2 đầy đủ chức năng từ dữ liệu đã replicate. Các giai đoạn migration:\nCài agent lên máy nguồn. Sao chép liên tục vào AWS. Kiểm thử bằng instance test không ảnh hưởng. Cutover sang production. Khám phá Microsoft Workloads on AWS Playlist tuyển chọn về triển khai, tối ưu và best practices khi chạy workload Microsoft trên AWS. Tổng kết Tuần 3 Tuần này đã hoàn thành kiến thức về AWS Compute:\n✅ Amazon EC2 và các loại instance\n✅ AMI, EBS, Instance Store\n✅ EC2 Auto Scaling\n✅ Các mô hình giá của EC2\n✅ Lightsail, EFS, FSx\nLabs đã hoàn tất: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/",
	"title": "Ngày 20 - AWS Backup &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-03 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Backup Dịch vụ backup tập trung giúp tự động hóa và quản trị bảo vệ dữ liệu ở quy mô lớn. Khả năng chính Quản lý tập trung: Định nghĩa và áp chính sách backup cho nhiều dịch vụ. Hỗ trợ đa dịch vụ: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3,\u0026hellip; Lịch và vòng đời: Tự động hóa lịch backup và retention. Tuân thủ: Đáp ứng yêu cầu governance và audit. Lợi ích Đơn giản vận hành: Không cần script tùy biến hay công cụ rời rạc. Tiết kiệm thời gian: Tự động bảo vệ dựa trên policy. Báo cáo \u0026amp; audit: Theo dõi trạng thái backup và tuân thủ. Backup Vault Lock Cơ chế đảm bảo tính bất biến, ngăn chỉnh sửa/xóa backup đã bảo vệ nhằm đáp ứng yêu cầu tuân thủ nghiêm ngặt. Tính năng nổi bật của AWS Backup:\nSao chép backup liên vùng. Backup chéo tài khoản. Backup plan (chính sách) linh hoạt. Quản lý vòng đời (lưu trữ lạnh, xóa theo hạn). Mã hóa dữ liệu khi lưu trữ. Gắn thẻ để điều khiển chính sách backup theo tag. Ví dụ Backup Plan:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Khám phá AWS Skill Builder Các learning plan chọn lọc và nội dung chuyên sâu dành cho chuyên gia lưu trữ: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 – AWS Backup Tạo S3 Bucket → 13-02.1 Triển khai hạ tầng mẫu → 13-02.2 Tạo Backup Plan → 13-03 Thiết lập thông báo → 13-04 Kiểm tra khôi phục → 13-05 Dọn dẹp tài nguyên → 13-06 Lab 25 – Amazon FSx (File Systems) Tạo File System SSD Multi-AZ → 25-2.2 Tạo File System HDD Multi-AZ → 25-2.3 Tạo File Share mới → 25-3 Kiểm thử hiệu năng → 25-4 Giám sát hiệu năng → 25-5 Bật tính năng Data Deduplication → 25-6 Bật Shadow Copies → 25-7 Quản lý phiên người dùng \u0026amp; file mở → 25-8 Bật quota người dùng → 25-9 Scale thông lượng → 25-11 Mở rộng dung lượng lưu trữ → 25-12 Xóa môi trường → 25-13 Tổng kết Tuần 4 Tuần này đã hoàn tất các chủ đề về AWS Storage:\n✅ Amazon S3 và các lớp lưu trữ\n✅ S3 Static Website và CORS\n✅ AWS Snow Family\n✅ AWS Storage Gateway\n✅ Chiến lược Disaster Recovery\n✅ AWS Backup\nLabs đã hoàn tất: 5 labs (Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/",
	"title": "Ngày 25 - AWS Security Hub &amp; Automation",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-10 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Security Hub Tổng hợp và ưu tiên hóa các phát hiện bảo mật, posture across account/dịch vụ. Khả năng\nKiểm tra tự động, chuẩn hóa findings, workflow xử lý ưu tiên. Hỗ trợ chuẩn tuân thủ: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Tích hợp\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer và nhiều công cụ đối tác. Kết quả\nGiảm thời gian thu thập, tập trung khắc phục; cái nhìn hợp nhất và nâng cao hygiene bảo mật. Tính năng Security Hub:\nTheo dõi posture bảo mật liên tục. Kiểm tra tuân thủ tự động. Tổng hợp findings đa tài khoản. Tích hợp 50+ dịch vụ AWS \u0026amp; đối tác. Custom insight và dashboard. Tự động khắc phục qua EventBridge. Chuẩn bảo mật được hỗ trợ:\nAWS Foundational Security Best Practices: hơn 50 control. CIS AWS Foundations Benchmark: chuẩn ngành. PCI DSS: tiêu chuẩn thẻ thanh toán. NIST: khung bảo mật NIST. Security Automation Dịch vụ AWS phục vụ tự động hóa:\nAWS Config: Theo dõi thay đổi cấu hình tài nguyên. Amazon EventBridge: Tự động hóa dựa trên sự kiện. AWS Lambda: Hàm serverless xử lý remediation. AWS Systems Manager: Tự động vá lỗi và quản lý tuân thủ. Mẫu tự động hóa phổ biến:\nTự động khắc phục tài nguyên không tuân thủ. Ứng phó sự cố tự động. Kiểm tra quy tắc security group. Cưỡng chế mã hóa. Đảm bảo tuân thủ tag. Khám phá AWS Certified Security – Specialty: All-in-One Exam Guide (SCS-C01) Tài liệu ôn luyện toàn diện cho chứng chỉ Security Specialty. Hands-On Labs Lab 18 – AWS Security Hub Bật Security Hub → 18-02 Đánh giá từng bộ tiêu chí → 18-03 Dọn dẹp tài nguyên → 18-04 Lab 22 – AWS Lambda Automation with Slack Tạo VPC → 22-2.1 Tạo Security Group → 22-2.2 Tạo EC2 Instance → 22-2.3 Cấu hình Slack Incoming Webhook → 22-2.4 Tạo tag cho instance → 22-3 Tạo role cho Lambda → 22-4 Hàm Stop Instance → 22-5.1 Hàm Start Instance → 22-5.2 Kiểm tra kết quả → 22-6 Dọn dẹp → 22-7 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Phần 2) Sử dụng tag với CLI → 27-2.2 Tạo Resource Group → 27-3 Dọn dẹp tài nguyên → 27-4 Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Phần 2) Tạo CloudTrail → 33-5.1 Ghi log vào CloudTrail → 33-5.2 Tạo Amazon Athena → 33-5.3 Query bằng Athena → 33-5.4 Kiểm tra \u0026amp; chia sẻ dữ liệu S3 đã mã hóa → 33-6 Dọn dẹp → 33-7 Lab 44 – IAM Advanced Role Control Tạo IAM Group → 44-2 Tạo IAM User → 44-3.1 Kiểm tra quyền → 44-3.2 Tạo IAM Role Admin → 44-4.1 Cấu hình Switch Role → 44-4.2 Giới hạn Switch Role theo IP → 44-4.3.1 Giới hạn Switch Role theo thời gian → 44-4.3.2 Dọn dẹp → 44-5 Tổng kết Tuần 5 Tuần này đã hoàn tất các chủ đề về AWS Security:\n✅ Shared Responsibility Model\n✅ AWS IAM (Users, Groups, Roles, Policies)\n✅ Amazon Cognito\n✅ AWS Organizations \u0026amp; SCPs\n✅ AWS Identity Center\n✅ AWS KMS\n✅ AWS Security Hub\nLabs đã hoàn tất: 8 labs (Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/",
	"title": "Ngày 30 - Database Migration &amp; Best Practices",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-17 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học AWS Database Migration Service (DMS) AWS DMS hỗ trợ di chuyển cơ sở dữ liệu lên AWS nhanh chóng, bảo mật và giảm tối đa downtime.\nĐiểm nổi bật:\nHomogeneous Migration: cùng engine (ví dụ Oracle → Oracle). Heterogeneous Migration: khác engine (Oracle → Aurora). Continuous Replication: giữ đồng bộ nguồn và đích. Schema Conversion: dùng AWS Schema Conversion Tool (SCT). Kiểu migration:\nFull Load: di chuyển toàn bộ dữ liệu hiện tại một lần. Full Load + CDC: tải ban đầu và đồng bộ thay đổi liên tục (Change Data Capture). CDC Only: chỉ replicate phần thay đổi mới. Nguồn hỗ trợ:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Đích hỗ trợ:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Best Practices cho Database Tối ưu hiệu năng RDS/Aurora:\nChọn đúng loại instance. Bật Enhanced Monitoring. Tối ưu câu truy vấn và index. Dùng Read Replica cho workload đọc nhiều. Bật Performance Insights để phân tích bottleneck. Redshift:\nChọn distribution key phù hợp. Dùng sort key cho cột hay lọc. Vacuum \u0026amp; analyze định kỳ. Tận dụng nén cột. Cấu hình workload management (WLM). ElastiCache:\nChọn node type đúng nhu cầu. Dùng cluster mode cho Redis. Cài đặt chính sách loại bỏ (eviction). Theo dõi cache hit rate. Dùng connection pooling. Bảo mật Mã hóa khi lưu (Encryption at Rest): bật cho toàn bộ database. Mã hóa khi truyền: dùng kết nối SSL/TLS. Cô lập mạng: triển khai trong private subnet. IAM Authentication: tận dụng cho RDS/Aurora khi có thể. Secrets Manager: lưu thông tin đăng nhập an toàn. Security Group: giới hạn truy cập tối thiểu. Audit Logging: bật CloudWatch Logs và CloudTrail. High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nBật Multi-AZ cho môi trường production. Cấu hình backup tự động. Thường xuyên kiểm tra quy trình khôi phục. Sử dụng Aurora Global Database cho DR đa vùng. Tạo read replica ở vùng khác nếu cần. Redshift:\nBật snapshot tự động. Sao chép snapshot sang vùng khác. Kết hợp Redshift Spectrum với data lake. Thiết lập cơ chế copy snapshot cross-region. ElastiCache:\nRedis: bật Multi-AZ với failover tự động. Kích hoạt backup/restore. Dùng cluster mode để mở rộng. Thêm retry logic ở mức ứng dụng. Tối ưu chi phí Right-sizing: chọn instance đúng tải. Reserved Instances: cam kết 1–3 năm. Aurora Serverless: cho workload biến thiên. Redshift Serverless: phù hợp phân tích không liên tục. Tối ưu lưu trữ: chọn loại storage thích hợp. Lifecycle Policy: lưu trữ lạnh lên S3/Glacier. Giám sát chi phí: dùng Cost Explorer \u0026amp; Budgets. Khám phá thêm The Data Warehouse Toolkit Tài liệu kinh điển về dimensional modeling và các mẫu thiết kế data warehouse. Tổng kết Week 6 Tuần này hoàn thành trọn bộ kiến thức AWS Database Services:\n✅ Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP)\n✅ Amazon RDS \u0026amp; Aurora\n✅ Amazon Redshift\n✅ Amazon ElastiCache\n✅ AWS Database Migration Service\nLabs đã hoàn thành: 2 labs (RDS \u0026amp; EC2 Integration, Database Migration Service)\nTổng kết 6 tuần đầu (8/9 - 17/10/2025) 30 ngày làm việc đã hoàn tất:\nWeek 1: Cloud Computing Fundamentals AWS basics, hạ tầng, công cụ quản trị, tối ưu chi phí. Week 2: AWS Networking Services VPC, subnet, security group, load balancing, hybrid connectivity. Week 3: AWS Compute Services EC2, AMI, storage, auto scaling, pricing models. Week 4: AWS Storage Services S3, Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR. Week 5: AWS Security \u0026amp; Identity IAM, Cognito, Organizations, KMS, Security Hub. Week 6: AWS Database Services RDS, Aurora, Redshift, ElastiCache, DMS. Tổng số labs: 25+ labs.\nKế hoạch tiếp theo: Bắt đầu Week 7 từ ngày 20/10/2025 (Thứ Hai).\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/1.7.5-day35-2025-10-24/",
	"title": "Ngày 35 - Contract Testing &amp; Retrospective",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-24 (Thứ Sáu)\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nGhi chú Bài học Contract Testing với Schemathesis Chạy schemathesis run --checks all --workers 4 --url http://127.0.0.1:8000/openapi.yaml để tạo bộ test tự động. Schemathesis sinh nhiều trường hợp ngẫu nhiên (happy path, edge case, thiếu field). Đảm bảo backend không trả sai schema khi frontend chuyển sang gọi API thật. Lợi ích Không cần viết tay test case phức tạp. Giảm rủi ro mismatch sau refactor. Hoạt động như quality gate trong CI pipeline. Sai sót \u0026amp; cách khắc phục Sai lầm Nguyên nhân Cách xử lý Tạo cả error.tsx và not-found.tsx Thừa xử lý Chỉ giữ not-found.tsx Dùng --base-url trong Schemathesis Sai câu lệnh Dùng --url đúng chuẩn Timeout khi đọc /openapi.json CORS hoặc phản hồi chậm Dùng file YAML trực tiếp Over-engineer backend Tách quá nhiều file sớm Bắt đầu đơn giản, refactor sau Workflow chuẩn đã được validate 1. Define Contract (OpenAPI)\r2. Mock API (Prism)\r3. Build Frontend với mock data\r4. Implement Backend theo spec\r5. Chuyển sang API thật\r6. Contract Testing (Schemathesis) Hỗ trợ frontend và backend phát triển song song. Giảm xung đột, tăng tốc demo và giữ chất lượng ổn định. Key Insights Contract-first giữ spec đồng bộ, giảm lỗi integration. Vertical slice cho phép release từng phần và lấy feedback sớm. Tự động hóa (Prism, Schemathesis) giảm effort test thủ công. Bắt đầu đơn giản, refactor dần khi nhu cầu mở rộng. Labs thực hành Chạy Schemathesis với spec mới nhất và ghi nhận kết quả. Cập nhật README workflow để cả team tham khảo. Chuẩn bị backlog cho vertical slice tiếp theo dựa trên feedback demo. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/1.8.5-day40-2025-10-31/",
	"title": "Ngày 40 - Đánh Giá MT &amp; Chiến Lược Decoding",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-10-31 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nĐiểm BLEU – Đánh Giá Dựa Trên Precision BLEU (Bilingual Evaluation Understudy) là thuật toán được thiết kế để đánh giá chất lượng dịch máy.\nCách BLEU Hoạt Động Khái Niệm Cốt Lõi: So sánh bản dịch ứng viên với một hoặc nhiều bản dịch tham chiếu (thường là bản dịch của con người)\nPhạm Vi Điểm: 0 đến 1\nGần 1 = bản dịch tốt hơn Gần 0 = bản dịch tệ hơn Tính Toán Điểm BLEU BLEU Vanilla (Có Vấn Đề) Ví Dụ:\nỨng viên: \u0026ldquo;I I am I\u0026rdquo; Tham chiếu 1: \u0026ldquo;Eunice said I\u0026rsquo;m hungry\u0026rdquo; Tham chiếu 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; Quy Trình:\nĐếm có bao nhiêu từ của ứng viên xuất hiện trong bất kỳ tham chiếu nào Chia cho tổng số từ của ứng viên Kết quả: 4/4 = 1.0 (điểm hoàn hảo!)\nVấn Đề: Bản dịch này tệ nhưng lại được điểm hoàn hảo! Một mô hình chỉ xuất ra các từ phổ biến sẽ đạt điểm tốt.\nBLEU Được Sửa Đổi (Tốt Hơn) Thay Đổi Chính: Sau khi khớp một từ, loại bỏ nó khỏi tham chiếu\nVí Dụ Tương Tự:\n\u0026ldquo;I\u0026rdquo; (đầu tiên) → khớp → loại bỏ \u0026ldquo;I\u0026rdquo; khỏi tham chiếu → đếm = 1 \u0026ldquo;I\u0026rdquo; (thứ hai) → không còn khớp → đếm = 1 \u0026ldquo;am\u0026rdquo; → khớp → loại bỏ \u0026ldquo;am\u0026rdquo; → đếm = 2 \u0026ldquo;I\u0026rdquo; (thứ ba) → không còn khớp → đếm = 2 Kết quả: 2/4 = 0.5 (thực tế hơn!)\nHạn Chế của BLEU ❌ Không xem xét ý nghĩa ngữ nghĩa\nChỉ kiểm tra khớp từ ❌ Không xem xét cấu trúc câu\n\u0026ldquo;Ate I was hungry because\u0026rdquo; vs \u0026ldquo;I ate because I was hungry\u0026rdquo; Cả hai đều được điểm giống nhau! ✅ Vẫn là metric được áp dụng rộng rãi nhất mặc dù có hạn chế\nĐiểm ROUGE – Đánh Giá Dựa Trên Recall ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nBLEU vs ROUGE Metric Tập Trung Tính Toán BLEU Precision Bao nhiêu từ ứng viên có trong tham chiếu? ROUGE Recall Bao nhiêu từ tham chiếu có trong ứng viên? Tính Toán Điểm ROUGE-N Ví Dụ:\nỨng viên: \u0026ldquo;I I am I\u0026rdquo; Tham chiếu 1: \u0026ldquo;Younes said I am hungry\u0026rdquo; (5 từ) Tham chiếu 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; (5 từ) Quy Trình cho Tham chiếu 1:\n\u0026ldquo;Younes\u0026rdquo; → không khớp → đếm = 0 \u0026ldquo;said\u0026rdquo; → không khớp → đếm = 0 \u0026ldquo;I\u0026rdquo; → khớp → đếm = 1 \u0026ldquo;am\u0026rdquo; → khớp → đếm = 2 \u0026ldquo;hungry\u0026rdquo; → không khớp → đếm = 2 Điểm ROUGE cho Ref 1: 2/5 = 0.4\nNếu có nhiều tham chiếu: Tính cho mỗi cái, lấy giá trị lớn nhất\nĐiểm F1 – Kết Hợp BLEU và ROUGE Vì BLEU = precision và ROUGE = recall, chúng ta có thể tính điểm F1:\nCông Thức:\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\rF1 = 2 × (BLEU × ROUGE) / (BLEU + ROUGE) Ví Dụ:\nBLEU = 0.5 ROUGE = 0.4 F1 = 2 × (0.5 × 0.4) / (0.5 + 0.4) = 4/9 ≈ 0.44 Beam Search Decoding Vấn Đề: Chọn từ có xác suất cao nhất ở mỗi bước không đảm bảo chuỗi tốt nhất tổng thể\nGiải Pháp: Beam search tìm các chuỗi có khả năng cao nhất trên một cửa sổ cố định\nCách Beam Search Hoạt Động Độ Rộng Beam (B): Số lượng chuỗi cần giữ ở mỗi bước\nQuy Trình: Bước 1: Bắt đầu với SOS token Lấy xác suất cho từ đầu tiên:\nI: 0.5 am: 0.4 hungry: 0.1 Giữ top B=2: \u0026ldquo;I\u0026rdquo; và \u0026ldquo;am\u0026rdquo;\nBước 2: Tính Xác Suất Có Điều Kiện Cho \u0026ldquo;I\u0026rdquo;:\nI am: 0.5 × 0.5 = 0.25 I I: 0.5 × 0.1 = 0.05 Cho \u0026ldquo;am\u0026rdquo;:\nam I: 0.4 × 0.7 = 0.28 am hungry: 0.4 × 0.2 = 0.08 Giữ top B=2: \u0026ldquo;am I\u0026rdquo; (0.28) và \u0026ldquo;I am\u0026rdquo; (0.25)\nBước 3: Lặp Lại Tiếp tục cho đến khi tất cả B chuỗi đạt EOS token\nBước 4: Chọn Tốt Nhất Chọn chuỗi có xác suất tổng thể cao nhất\nĐặc Điểm Beam Search Ưu Điểm:\nTốt hơn greedy decoding (B=1) Tìm các chuỗi tốt hơn toàn cục Được sử dụng rộng rãi trong production Nhược Điểm:\nTốn bộ nhớ (lưu trữ B chuỗi) Tốn kém về mặt tính toán (chạy mô hình B lần mỗi bước) Phạt các chuỗi dài (tích của nhiều xác suất) Giải Pháp cho Chuỗi Dài: Chuẩn hóa theo độ dài: chia xác suất cho số từ\nMinimum Bayes Risk (MBR) Decoding Khái Niệm: Tạo nhiều mẫu và tìm sự đồng thuận\nQuy Trình MBR: Bước 1: Tạo Nhiều Mẫu Tạo ~30 mẫu ngẫu nhiên từ mô hình\nBước 2: So Sánh Tất Cả Các Cặp Với mỗi mẫu, so sánh với tất cả các mẫu khác sử dụng metric tương tự (ví dụ: ROUGE)\nBước 3: Tính Độ Tương Tự Trung Bình Với mỗi ứng viên, tính độ tương tự trung bình với tất cả các ứng viên khác\nBước 4: Chọn Tốt Nhất Chọn mẫu có độ tương tự trung bình cao nhất (rủi ro thấp nhất)\nCông Thức MBR E* = argmax_E [ trung bình ROUGE(E, E\u0026#39;) cho tất cả E\u0026#39; ] Trong đó:\nE = bản dịch ứng viên E\u0026rsquo; = tất cả các ứng viên khác Mục tiêu: Tìm E tối đa hóa ROUGE trung bình với mọi E' Ví Dụ MBR (4 Ứng Viên) Bước 1: Tính điểm ROUGE theo cặp\nROUGE(C1, C2), ROUGE(C1, C3), ROUGE(C1, C4) Trung bình = R1 Bước 2: Lặp lại cho C2, C3, C4\nLấy R2, R3, R4 Bước 3: Chọn cao nhất\nChọn ứng viên có max(R1, R2, R3, R4) Đặc Điểm MBR Ưu Điểm:\nChính xác hơn về mặt ngữ cảnh so với random sampling Tìm bản dịch đồng thuận Có thể vượt trội beam search Nhược Điểm:\nYêu cầu tạo nhiều mẫu (tốn kém) Yêu cầu so sánh O(n²) Khi Nào Sử Dụng:\nKhi cần bản dịch chất lượng cao Khi chi phí tính toán chấp nhận được Khi đầu ra beam search không nhất quán Tóm Tắt: Các Chiến Lược Decoding Phương Pháp Mô Tả Ưu Điểm Nhược Điểm Greedy Chọn xác suất cao nhất mỗi bước Nhanh, đơn giản Chuỗi không tối ưu Beam Search Giữ top-B chuỗi Chất lượng tốt hơn Chi phí bộ nhớ + tính toán Random Sampling Lấy mẫu từ phân phối Đầu ra đa dạng Chất lượng không nhất quán MBR Đồng thuận từ các mẫu Chất lượng cao Rất tốn kém Tóm Tắt Các Metric Đánh Giá Metric Loại Tập Trung Tốt Nhất Cho BLEU Precision Ứng viên → Tham chiếu MT chung ROUGE Recall Tham chiếu → Ứng viên Tóm tắt F1 Trung bình điều hòa Cả precision \u0026amp; recall Cái nhìn cân bằng Lưu Ý Quan Trọng: Tất cả các metric này:\n❌ Không xem xét ngữ nghĩa ❌ Không xem xét cấu trúc câu ✅ Chỉ đếm khớp n-gram Thay Thế Hiện Đại: Sử dụng các metric neural hoặc đánh giá của con người cho các ứng dụng quan trọng!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/1.9.5-day45-2025-11-07/",
	"title": "Ngày 45 - Decoder Transformer &amp; Triển Khai GPT2",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-07 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nXây Dựng Decoder Transformer: Kiến Trúc GPT2 Bây giờ hãy xem cách tất cả những phần này kết hợp lại trong mã thực tế!\nCấu Trúc Transformer Decoder (kiểu GPT2) Đầu Vào: Câu được tokenize [1, 2, 3, 4, 5]\r↓\rLớp Embedding: Chuyển đổi tokens thành vectors\r↓\rThêm Positional Encoding: Thêm thông tin vị trí\r↓\r┌──────────────────────────────────┐\r│ Decoder Block (N lần) │\r│ ├─ Masked Self-Attention │\r│ ├─ Residual + LayerNorm │\r│ ├─ Feed-Forward │\r│ └─ Residual + LayerNorm │\r└──────────────────────────────────┘\r↓\rLớp Linear: Chiếu tới kích thước vocab\r↓\rSoftmax: Chuyển đổi thành xác suất\r↓\rĐầu Ra: Xác suất cho từ tiếp theo Triển Khai PyTorch Bước 1: Word Embedding + Positional Encoding import torch import torch.nn as nn class TransformerDecoder(nn.Module): def __init__(self, vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1): super().__init__() # 1. Lớp embedding self.embedding = nn.Embedding(vocab_size, d_model) # 2. Positional encoding (được học) self.positional_encoding = nn.Embedding(max_seq_len, d_model) # 3. Các decoder blocks (lặp N lần) self.decoder_layers = nn.ModuleList([ DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]) # 4. Lớp đầu ra self.final_layer = nn.Linear(d_model, vocab_size) self.softmax = nn.Softmax(dim=-1) self.d_model = d_model def forward(self, input_ids, mask=None): # input_ids shape: [batch_size, seq_length] batch_size, seq_len = input_ids.shape # 1. Nhúng các tokens x = self.embedding(input_ids) # [batch, seq_len, d_model] # 2. Thêm positional encoding positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) pos_encoding = self.positional_encoding(positions) x = x + pos_encoding # [batch, seq_len, d_model] # 3. Truyền qua các lớp decoder for decoder_layer in self.decoder_layers: x = decoder_layer(x, mask) # 4. Chiếu tới vocab logits = self.final_layer(x) # [batch, seq_len, vocab_size] return logits Bước 2: Decoder Block class DecoderBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super().__init__() # 1. Masked multi-head attention self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) self.norm1 = nn.LayerNorm(d_model) # 2. Feed-forward network self.feed_forward = FeedForward(d_model, d_ff, dropout) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask=None): # x shape: [batch, seq_len, d_model] # Masked Self-Attention + Residual + Norm attn_output = self.self_attention(x, x, x, mask) # Q=K=V x = x + self.dropout(attn_output) x = self.norm1(x) # Feed-Forward + Residual + Norm ff_output = self.feed_forward(x) x = x + self.dropout(ff_output) x = self.norm2(x) return x Bước 3: Multi-Head Attention class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads, dropout): super().__init__() assert d_model % num_heads == 0 self.num_heads = num_heads self.d_k = d_model // num_heads # Các phép chiếu tuyến tính cho Q, K, V self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) # Phép chiếu đầu ra self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, Q, K, V, mask=None): batch_size = Q.shape[0] # 1. Các phép chiếu tuyến tính và chia thành nhiều đầu Q = self.W_q(Q) # [batch, seq_len, d_model] K = self.W_k(K) V = self.W_v(V) # Reshape cho multi-head: [batch, seq_len, num_heads, d_k] Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Bây giờ: [batch, num_heads, seq_len, d_k] # 2. Scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, num_heads, seq_len, seq_len] # 3. Áp dụng mask (cho causal masking trong decoder) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 4. Softmax attention_weights = torch.softmax(scores, dim=-1) attention_weights = self.dropout(attention_weights) # 5. Nhân với value context = torch.matmul(attention_weights, V) # [batch, num_heads, seq_len, d_k] # 6. Ghép các đầu context = context.transpose(1, 2).contiguous() context = context.view(batch_size, -1, self.d_model) # 7. Phép chiếu tuyến tính cuối cùng output = self.W_o(context) return output Bước 4: Feed-Forward Network class FeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout): super().__init__() self.linear1 = nn.Linear(d_model, d_ff) # 512 → 2048 self.linear2 = nn.Linear(d_ff, d_model) # 2048 → 512 self.relu = nn.ReLU() self.dropout = nn.Dropout(dropout) def forward(self, x): x = self.linear1(x) # Mở rộng x = self.relu(x) # Non-linearity x = self.dropout(x) # Regularization x = self.linear2(x) # Nén return x Bước 5: Causal Mask (để ngăn chặn attend vào tương lai) def create_causal_mask(seq_len, device): \u0026#34;\u0026#34;\u0026#34; Tạo một mask ngăn chặn attention tới các vị trí tương lai. Đầu Ra: [1, 0, 0, 0] [1, 1, 0, 0] [1, 1, 1, 0] [1, 1, 1, 1] Vị trí i chỉ có thể attend tới các vị trí 0...i \u0026#34;\u0026#34;\u0026#34; mask = torch.tril(torch.ones(seq_len, seq_len, device=device)) return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len] # Sử Dụng: mask = create_causal_mask(seq_len=10, device=\u0026#39;cuda\u0026#39;) Vòng Lặp Huấn Luyện def train_transformer(model, train_loader, epochs=10, learning_rate=0.0001): optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): total_loss = 0 for batch_idx, (input_ids, target_ids) in enumerate(train_loader): # Forward pass logits = model(input_ids) # logits: [batch, seq_len, vocab_size] # target_ids: [batch, seq_len] # Tính toán loss loss = loss_fn( logits.view(-1, vocab_size), target_ids.view(-1) ) # Backward pass optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\u0026#34;Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\u0026#34;) Suy Luận (Tạo Văn Bản) def generate_text(model, start_token, max_length=50, device=\u0026#39;cuda\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Tạo văn bản tự động sử dụng transformer được huấn luyện. \u0026#34;\u0026#34;\u0026#34; model.eval() generated = [start_token] with torch.no_grad(): for _ in range(max_length): # Chuẩn bị đầu vào input_ids = torch.tensor(generated, device=device).unsqueeze(0) # Forward pass logits = model(input_ids) # Lấy logits cho vị trí cuối cùng last_logits = logits[0, -1, :] # [vocab_size] # Lấy mẫu hoặc tham lam next_token = torch.argmax(last_logits).item() # Tham lam # Hoặc: next_token = torch.multinomial(softmax(last_logits), 1).item() # Lấy mẫu generated.append(next_token) if next_token == end_token: break return generated Ví Dụ Hoàn Chỉnh Hoạt Động # Khởi tạo mô hình model = TransformerDecoder( vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1 ).to(\u0026#39;cuda\u0026#39;) # Tạo dữ liệu giả batch_size, seq_len = 32, 128 input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(\u0026#39;cuda\u0026#39;) # Forward pass output = model(input_ids) print(f\u0026#34;Output shape: {output.shape}\u0026#34;) # [32, 128, 10000] # Tạo causal mask mask = create_causal_mask(seq_len, \u0026#39;cuda\u0026#39;) # Forward với mask output_masked = model(input_ids, mask) print(f\u0026#34;Masked output shape: {output_masked.shape}\u0026#34;) # Tạo văn bản generated = generate_text(model, start_token=101, max_length=20) print(f\u0026#34;Generated sequence: {generated}\u0026#34;) Tóm Tắt Các Thành Phần Chính Thành Phần Mục Đích Kích Thước Embedding Token → Vector vocab_size → d_model Positional Encoding Thêm thông tin vị trí d_model Multi-Head Attention Học mối quan hệ d_model → d_model Feed-Forward Phép biến đổi phi tuyến d_model → d_ff → d_model LayerNorm Ổn định huấn luyện per-element Output Layer Chiếu tới vocab d_model → vocab_size Tại Sao Kiến Trúc Này Hoạt Động ✅ Xử Lý Song Song: Tất cả các vị trí được xử lý cùng nhau (nhanh!) ✅ Phụ Thuộc Dài Hạn: Attention trực tiếp tới bất kỳ vị trí nào (không có vanishing gradients!) ✅ Có Thể Diễn Giải: Có thể hình dung các mô hình attention ✅ Có Thể Mở Rộng: Có thể lớn lên tới hàng tỷ tham số\nCác Biến Thể GPT2 GPT-2 Small: 117M tham số GPT-2 Medium: 345M tham số GPT-2 Large: 762M tham số GPT-2 XL: 1.5B tham số Tất cả sử dụng kiến trúc decoder giống nhau, chỉ được mở rộng!\nBước Tiếp Theo Huấn Luyện Trước: Huấn luyện trên kho ngữ liệu văn bản lớn (Wikipedia, Sách, v.v.) Tinh Chỉnh: Điều chỉnh cho các tác vụ cụ thể (dịch, phân loại, v.v.) Đánh Giá: Đo chất lượng (perplexity, BLEU, đánh giá của con người) Triển Khai: Sử dụng cho các ứng dụng thực tế Tóm Tắt Tuần 9 Chúng ta đã bao quát:\n✅ Tại sao transformers thay thế RNNs ✅ Kiến trúc transformer hoàn chỉnh ✅ Cơ chế scaled dot-product attention ✅ Self, masked, và encoder-decoder attention ✅ Chi tiết triển khai và code Đây là nền tảng của NLP hiện đại! Tất cả các mô hình tiên tiến (BERT, GPT, T5, Claude, ChatGPT) đều dựa trên kiến trúc transformer.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/1.10.5-day50-2025-11-14/",
	"title": "Ngày 50 - Fine-tuning &amp; Ứng Dụng Thực Tế",
	"tags": [],
	"description": "",
	"content": "Ngày: 2025-11-14 (Thứ Sáu)\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTuần 10 Hoàn Thành: Từ Lý Thuyết Đến Sản Xuất Bây giờ bạn hiểu transfer learning, BERT và T5. Ngày cuối cùng này bao gồm cách thực sự triển khai các mô hình này.\nFine-tuning: Nghệ Thuật và Khoa Học Bức Tranh Lớn Mô Hình Được Huấn Luyện Trước (ví dụ: BERT-base)\r├─ Đã biết tiếng Anh\r├─ Đã hiểu ngữ pháp\r├─ Đã có một số kiến thức thế giới\r└─ Có thể được thích ứng với các tác vụ cụ thể!\rThêm Classification Head\r├─ Đơn Giản: Lấy đại diện token [CLS]\r├─ Thêm: Dense layer (768 → hidden_size)\r├─ Thêm: Classification layer (hidden_size → num_classes)\r└─ Kết Quả: Mô hình riêng theo tác vụ\rFine-tune trên dữ liệu của bạn\r├─ Đánh giá bộ phim: 5,000 ví dụ\r├─ Thời gian huấn luyện: 2-3 giờ trên GPU đơn\r├─ Kết Quả: Độ chính xác 94-96%\r└─ Triển khai! Chiến Lược Fine-tuning Chiến Lược 1: Fine-tuning Đầy Đủ Cập nhật tất cả các tham số\n# Pseudocode pretrained_model = load_bert_base() # Không có lớp đóng băng! for epoch in range(3): for batch in training_data: logits = pretrained_model(batch) loss = classification_loss(logits, batch.labels) loss.backward() optimizer.step() # Kết Quả: Hiệu suất tốt nhất # Thời Gian: 3+ giờ trên GPU # Bộ Nhớ: Yêu cầu gradient cho tất cả 110M tham số Khi Nào Sử Dụng:\nTập dữ liệu lớn (10,000+ ví dụ) Tính toán đủ (GPU/TPU) Tác vụ rất khác so với pre-training Chiến Lược 2: Đóng Băng Lớp Đóng băng các lớp sớm, fine-tune các lớp sau\n# Đóng băng 10 lớp đầu tiên for param in model.bert.encoder.layer[:10].parameters(): param.requires_grad = False # Fine-tune lớp 11-12 và phần đầu phân loại for param in model.bert.encoder.layer[10:].parameters(): param.requires_grad = True # Kết Quả: Nhanh, hiệu suất tốt # Thời Gian: 1-2 giờ trên GPU # Bộ Nhớ: Chỉ gradient cho 2 lớp + head Khi Nào Sử Dụng:\nTập dữ liệu trung bình (1,000-10,000 ví dụ) Tính toán hạn chế Tác vụ hơi khác so với pre-training Chiến Lược 3: Progressive Unfreezing Dần dần mở khóa các lớp từ trên xuống dưới\nEpoch 1: Chỉ fine-tune phần đầu phân loại\r├─ Đóng Băng: Tất cả 12 lớp\r├─ Huấn Luyện: Classification head\r└─ Tỷ Lệ Học: 1e-3\rEpoch 2: Mở Khóa Lớp Cuối Cùng 1\r├─ Đóng Băng: Lớp 0-10\r├─ Huấn Luyện: Lớp 11 + head\r└─ Tỷ Lệ Học: 1e-4\rEpoch 3: Mở Khóa 2 Lớp Cuối Cùng\r├─ Đóng Băng: Lớp 0-9\r├─ Huấn Luyện: Lớp 10-11 + head\r└─ Tỷ Lệ Học: 1e-4\r...Tiếp tục cho đến khi tất cả mở khóa\rKết Quả: Thường hiệu suất tốt nhất!\rThời Gian: 5+ giờ, nhưng đáng giá cho các tác vụ quan trọng Lựa Chọn Siêu Tham Số Tỷ Lệ Học Hướng Dẫn Chung:\rĐối Với Fine-tuning Đầy Đủ:\r├─ Bắt Đầu Với: 5e-5 (rất nhỏ!)\r├─ Thử: 2e-5, 3e-5, 5e-5, 1e-4\r└─ Không Sử Dụng: Tỷ Lệ Học \u0026gt; 1e-4 (quên thảm họa)\rĐối Với Đóng Băng Lớp:\r├─ Lớp Đóng Băng: Không Tỷ Lệ Học (không cập nhật)\r├─ Lớp Fine-tuned: 1e-4 - 1e-3\r└─ Phần Đầu Phân Loại: Có thể sử dụng cao hơn một chút\rTại Sao Lại Nhỏ?\r├─ Trọng số được huấn luyện trước đã tốt\r├─ Không muốn phá hủy kiến thức\r├─ Những thay đổi nhỏ an toàn hơn Kích Thước Lô Tác Động Kích Thước Lô:\rCác Lô Nhỏ (8-16):\r├─ Ưu Điểm: Hoạt động với bộ nhớ hạn chế\r├─ Nhược Điểm: Gradient ồn ào hơn, không ổn định\r├─ Sử Dụng Khi: GPU Nhỏ (\u0026lt; 8GB VRAM)\rCác Lô Trung Bình (32):\r├─ Ưu Điểm: Cân Bằng Tốt\r├─ Nhược Điểm: Sử Dụng Bộ Nhớ Trung Bình\r├─ Sử Dụng Khi: GPU Tiêu Chuẩn (8-16GB VRAM)\rCác Lô Lớn (64-256):\r├─ Ưu Điểm: Đào Tạo Ổn Định, Khái Quát Hóa Tốt\r├─ Nhược Điểm: Yêu Cầu Nhiều Bộ Nhớ Hoặc Tích Lũy Gradient\r├─ Sử Dụng Khi: TPUs, 24GB+ VRAM, Hoặc Tích Lũy Gradient Số Lượng Epoch Tác Vụ Phân Loại:\r├─ Điển Hình: 3-5 epoch\r├─ Tại Sao: Mô hình hội tụ nhanh\r└─ Giám Sát: Dừng sớm nếu xác thực ngừng cải thiện\rTác Vụ Tạo (T5):\r├─ Điển Hình: 10-20 epoch\r├─ Tại Sao: Tác vụ phức tạp hơn, hội tụ chậm hơn\r└─ Giám Sát: Điểm BLEU xác thực\rNguyên Tắc Ngón Tay:\r├─ Nhiều Dữ Liệu Hơn → Ít Epoch Hơn (ví dụ: 2 epoch cho 100K ví dụ)\r├─ Ít Dữ Liệu Hơn → Nhiều Epoch Hơn (ví dụ: 5 epoch cho 1K ví dụ) Ứng Dụng Fine-tuning Phổ Biến 1. Phân Tích Cảm Xúc Tác Vụ: Phân loại bài đánh giá là dương/âm\rDữ Liệu: 5,000 bài đánh giá bộ phim có nhãn\r├─ 80%: Huấn luyện (4,000)\r├─ 20%: Xác thực (1,000)\rFine-tuning:\r├─ Mô Hình: BERT-base\r├─ Epoch: 3\r├─ Kích Thước Lô: 32\r├─ Tỷ Lệ Học: 2e-5\r├─ Thời Gian Huấn Luyện: 30 phút\rKết Quả:\r├─ Độ Chính Xác: 94.2%\r├─ Độ Chính Xác: 94.5%\r├─ Gợi Lại: 93.9%\r└─ Tốt hơn nhiều so với huấn luyện từ đầu (78%)! 2. Nhận Dạng Tên Thực Thể (NER) Tác Vụ: Xác định người, địa điểm, tổ chức trong văn bản\rVí Dụ:\r\u0026#34;John Smith works at Google in New York.\u0026#34;\rNhãn: [B-PER, I-PER, O, O, B-ORG, O, B-LOC, I-LOC]\rThách Thức: Phân loại cấp token, không phải cấp câu\rGiải Pháp:\r├─ Nhận BERT token embeddings\r├─ Thêm lớp tuyến tính cho mỗi token\r├─ Giải mã bằng CRF (Trường Ngẫu Nhiên Có Điều Kiện)\rThời Gian Fine-tuning: 1-2 giờ\rHiệu Suất: Điểm F1 92% 3. Trả Lời Câu Hỏi Tác Vụ: Tìm span trả lời trong một đoạn văn\rĐầu Vào:\rCâu Hỏi: \u0026#34;What is the capital of France?\u0026#34;\rĐoạn Văn: \u0026#34;Paris is the capital and most populous city of France...\u0026#34;\rĐầu Ra:\rCâu Trả Lời: \u0026#34;Paris\u0026#34;\rCách Nó Hoạt Động:\r├─ Mã Hóa câu hỏi + đoạn văn cùng nhau\r├─ Đối Với Mỗi Token, Dự Đoán: \u0026#34;Đây có phải là bắt đầu của câu trả lời không?\u0026#34;\r├─ Đối Với Mỗi Token, Dự Đoán: \u0026#34;Đây có phải là kết thúc của câu trả lời không?\u0026#34;\r├─ Trích Xuất Span Giữa Khả Năng Cao Nhất Bắt Đầu Và Kết Thúc\rThời Gian Fine-tuning: 2-3 giờ\rHiệu Suất: Điểm F1 89% trên SQuAD 4. Tóm Tắt Văn Bản Tác Vụ: Nén các tài liệu dài\rSử Dụng T5:\rĐầu Vào:\r\u0026#34;The quick brown fox jumps over the lazy dog. This sentence contains all 26 letters of English alphabet.\rIt\u0026#39;s often used as a test string in computers.\u0026#34;\rFine-tuning Với T5:\r├─ Tiền Tố: \u0026#34;summarize:\u0026#34;\r├─ Huấn Luyện Đầy Đủ: 10-20 epoch\r├─ Kích Thước Lô: 16\r├─ Tỷ Lệ Học: 5e-5\rĐầu Ra:\r\u0026#34;A pangram sentence commonly used in computing.\u0026#34;\rHiệu Suất: Điểm ROUGE 35-40 (so với 20-25 baseline) 5. Tương Tự Văn Bản Ngữ Nghĩa Tác Vụ: Đánh Giá Hai Câu Giống Nhau Bao Nhiêu (0-5)\rCâu A: \u0026#34;The cat sat on the mat\u0026#34;\rCâu B: \u0026#34;A feline rested on the rug\u0026#34;\rNhãn: 4.5 (rất tương tự)\rFine-tuning:\r├─ Lấy Token [CLS] Từ Cả Hai Câu\r├─ Mã Hóa Cùng Nhau\r├─ Regression Head: Dense Layer Để Xuất Điểm (0-5)\r├─ Loss: Lỗi Bình Phương Trung Bình (MSE)\rKết Quả:\r├─ Tương Quan Với Phán Đoán Của Con Người: 0.88 (rất tốt!)\r├─ Tương Quan Spearman: 87% Những Cạm Bẫy Fine-tuning Cần Tránh ❌ Cạm Bẫy 1: Tỷ Lệ Học Quá Cao Vấn Đề: Mô hình quên kiến thức được huấn luyện trước!\rVí Dụ:\rTỷ Lệ Học: 1e-3\rSau 1 Epoch: Loss = 0.5 (tốt)\rSau 2 Epoch: Loss = 2.0 (khủng khiếp!)\rSau 3 Epoch: Loss = 5.0 (tệ hơn!)\rTại Sao: Các cập nhật lớn phá hủy trọng số hữu ích\rGiải Pháp:\r├─ Sử Dụng Tỷ Lệ Học Nhỏ Hơn 1-2 bậc\r├─ Bắt Đầu Với 2e-5, Tăng Chỉ Nếu Hội Tụ Quá Chậm\r└─ Giám Sát: Loss Xác Thực Nên Giảm ❌ Cạm Bẫy 2: Quá Ít Epoch Vấn Đề: Mô Hình Không Thích Ứng Với Tác Vụ Mới\rVí Dụ:\rDữ Liệu: 5,000 ví dụ\rEpoch: 1\rHiệu Suất: Độ chính xác 88%\rCùng Mô Hình Với 3 Epoch:\rHiệu Suất: Độ chính xác 94%!\rTại Sao: 1 Epoch = Mỗi Ví Dụ Thấy Một Lần\rKhông đủ để học các mẫu riêng theo tác vụ\rGiải Pháp:\r├─ Sử Dụng Ít Nhất 3-5 Epoch\r├─ Giám Sát Độ Chính Xác Xác Thực\r├─ Dừng Sớm Khi Độ Chính Xác Xác Thực Ngừng Cải Thiện ❌ Cạm Bẫy 3: Overfitting Trên Dữ Liệu Nhỏ Vấn Đề: Mô Hình Ghi Nhớ Thay Vì Khái Quát Hóa\rVí Dụ:\rDữ Liệu Huấn Luyện: 100 ví dụ\rĐộ Chính Xác Huấn Luyện: 99.8%\rĐộ Chính Xác Kiểm Tra: 72.0%\rMô Hình Ghi Nhớ!\rGiải Pháp:\r├─ Thêm Dropout: Bỏ 10-20% Neuron Ngẫu Nhiên\r├─ Dừng Sớm: Dừng Khi Độ Chính Xác Xác Thực Tắc\r├─ Tăng Cường Dữ Liệu: Tạo Thêm Ví Dụ Từ Các Ví Dụ Hiện Có\r├─ Giảm Kích Thước Mô Hình: Sử Dụng BERT-small Thay Vì BERT-large ❌ Cạm Bẫy 4: Không Tinh Chỉnh Siêu Tham Số Vấn Đề: Siêu Tham Số Mặc Định Không Tối Ưu\rVí Dụ:\rTỷ Lệ Học Mặc Định (1e-4): Độ chính xác 92%\rTỷ Lệ Học Tinh Chỉnh (3e-5): Độ chính xác 95%!\rGiải Pháp:\r├─ Thử 3-5 Tỷ Lệ Học Khác Nhau\r├─ Thử 2-3 Kích Thước Lô Khác Nhau\r├─ Thử 3-5 Epoch\r├─ Chạy Tập Xác Thực Nhỏ Trên Mỗi Kết Hợp\r└─ Chọn Kết Hợp Tốt Nhất Cho Huấn Luyện Đầy Đủ Cân Nhắc Triển Khai Kích Thước Mô Hình vs Tốc Độ Đối Với Triển Khai Sản Xuất:\rBERT-base (110M):\r├─ Kích Thước Mô Hình: 440 MB\r├─ Thời Gian Suy Luận: 100-150 ms mỗi ví dụ\r├─ Độ Chính Xác Tốt\r└─ Có thể Phù Hợp Trên Hầu Hết Các Máy Chủ\rBERT-large (340M):\r├─ Kích Thước Mô Hình: 1.3 GB\r├─ Thời Gian Suy Luận: 300-500 ms mỗi ví dụ\r├─ Độ Chính Xác Tốt Hơn\r└─ Cần Phần Cứng Tốt Hơn\rDistilBERT (40M):\r├─ Kích Thước Mô Hình: 160 MB (60% nhỏ hơn!)\r├─ Thời Gian Suy Luận: 30-50 ms (3x nhanh hơn!)\r├─ Độ Chính Xác Hơi Thấp Hơn (97% của BERT)\r└─ Hoàn Hảo Cho Các Thiết Bị Di Động/Edge!\rCây Quyết Định:\r├─ Độ Chính Xác Tới Hạn? → Sử Dụng BERT-base Hoặc BERT-large\r├─ Tốc Độ Tới Hạn? → Sử Dụng DistilBERT Hoặc Lượng Tử Hóa\r├─ Cân Bằng? → Sử Dụng BERT-base Kỹ Thuật Tối Ưu Hóa Trước Triển Khai:\r1. Lượng Tử Hóa (8-bit Thay Vì 32-bit):\r├─ Kích Thước Mô Hình: 1/4 Của Gốc\r├─ Tốc Độ Suy Luận: Nhanh Hơn 2-4 Lần\r└─ Độ Chính Xác: 95-99% Của Độ Chính Xác Đầy Đủ\r2. Chưng Cất Kiến Thức:\r├─ Huấn Luyện Mô Hình Nhỏ Trên Đầu Ra Của Mô Hình Lớn\r├─ Kích Thước: Nhỏ Hơn 10 Lần\r├─ Tốc Độ: Nhanh Hơn 10 Lần\r└─ Độ Chính Xác: 98% Của Mô Hình Giáo Viên\r3. Cắt Tỉa:\r├─ Loại Bỏ Trọng Số Không Quan Trọng\r├─ Kích Thước: Nhỏ Hơn 30-50%\r├─ Tốc Độ: Nhanh Hơn 2-3 Lần\r└─ Độ Chính Xác: 98% Của Mô Hình Đầy Đủ\r4. TorchScript/ONNX:\r├─ Biên Dịch Mô Hình Cho Sản Xuất\r├─ Tốc Độ: Nhanh Hơn 1.5-2 Lần\r└─ Không Phụ Thuộc Framework (TensorFlow, PyTorch, v.v.) Ví Dụ Thế Giới Thực: Xây Dựng Bộ Phân Loại Cảm Xúc Đường Ống Hoàn Chỉnh Bước 1: Chuẩn Bị Dữ Liệu\r├─ Tải: 5,000 bài đánh giá bộ phim có nhãn\r├─ Tách: 80% huấn luyện, 20% kiểm tra\r├─ Tokenize: Chuyển Đổi Thành Token BERT\r└─ Dataloader: Tạo Các Lô Có Kích Thước 32\rBước 2: Tải Mô Hình Được Huấn Luyện Trước\r├─ Tải Xuống: BERT-base Từ HuggingFace\r├─ Thêm Classification Head: 768 → 2 (nhị phân)\r└─ Di Chuyển Đến: GPU\rBước 3: Fine-tune\r├─ Tối Ưu Hóa: AdamW (tốt nhất cho transformers)\r├─ Tỷ Lệ Học: 2e-5\r├─ Epoch: 3\r├─ Vòng Lặp Huấn Luyện: Forward Pass → Loss → Backward → Cập Nhật\rBước 4: Đánh Giá\r├─ Độ chính xác xác thực: 94.2%\r├─ Độ chính xác kiểm tra: 93.8%\r└─ Các Số Liệu Từng Lớp: Độ Chính Xác 94%, Gợi Lại 94%\rBước 5: Lưu \u0026amp; Triển Khai\r├─ Lưu: model.pt, tokenizer, config.json\r├─ Kiểm Tra Trên Đánh Giá Mới: \u0026#34;Best movie ever!\u0026#34; → Positive ✓\r└─ Triển Khai Đến Sản Xuất!\rTổng Thời Gian: 2-3 giờ\rTổng Chi Phí: ~$2-5 Trên GPU Đám Mây\rHiệu Suất: Tiên Tiến Nhất! Lợi Thế Transfer Learning So Sánh Baseline (Huấn Luyện Từ Đầu):\r├─ Thời Gian Huấn Luyện: 2-4 tuần\r├─ Dữ Liệu Được Yêu Cầu: 100,000+ ví dụ\r├─ Độ Chính Xác: 82-85%\r├─ Chi Phí: $1000-10000 Trong Tính Toán\r└─ Khó: Yêu Cầu Chuyên Môn ML\rTransfer Learning (BERT Fine-tuning):\r├─ Thời Gian Huấn Luyện: 2-3 giờ\r├─ Dữ Liệu Được Yêu Cầu: 100-1000 ví dụ\r├─ Độ Chính Xác: 92-95%\r├─ Chi Phí: $1-10 Trong Tính Toán\r└─ Khó: Có thể sử dụng Thư Viện HuggingFace!\rTăng Tốc Độ: 200-300x nhanh hơn!\rGiảm Dữ Liệu: Cần 100x ít dữ liệu!\rKết Quả Tốt Hơn: Độ chính xác cao hơn 10-15%! Những Lợi Ích Chính ✅ Transfer learning là thực tế: Hoạt động tốt cho các vấn đề thực tế ✅ Fine-tuning rất đơn giản: Thêm head + huấn luyện 3-5 epoch ✅ Tỷ Lệ Học Quan Trọng: Sử Dụng 1-2 bậc nhỏ hơn ✅ Tránh Overfitting: Giám Sát Xác Thực, Sử Dụng Dừng Sớm ✅ Cân Nhắc Triển Khai: Tối Ưu Hóa Cho Các Ràng Buộc Của Bạn ✅ HuggingFace Là Bạn Của Bạn: Sử Dụng Các Mô Hình Được Huấn Luyện Trước + Thư Viện\nHành Trình NLP Của Bạn Bạn Đã Học:\nTuần 1-7 (Nền Tảng): Cơ Bản, Xử Lý Văn Bản, Embeddings Tuần 8 (Nền Tảng NLP): Cơ Bản Ngôn Ngữ, Tìm Kiếm Giọng Nói, Seq2seq, Chú Ý Tuần 9 (Transformers): Self-Attention, Scaled Dot-Product, Cơ Chế Chú Ý, Triển Khai Tuần 10 (Transfer Learning): Transfer Learning, BERT, MLM, T5, Fine-tuning\nTừ Hiểu Biết Ngôn Ngữ Đến Xây Dựng Hệ Thống Sản Xuất!\nCác Bước Tiếp Theo Để Trở Thành Chuyên Gia NLP:\nXây Dựng Dự Án: Fine-tune Các Mô Hình Trên Dữ Liệu Thực Thử Các Mô Hình Khác Nhau: RoBERTa, ELECTRA, XLNet, GPT-2 Khám Phá Các Kỹ Thuật Nâng Cao: Prompt Tuning, In-context Learning, RAG Cập Nhật: Đọc Các Bài Viết, Theo Dõi Nghiên Cứu (Papers with Code, Twitter) Đóng Góp: Các Dự Án NLP Nguồn Mở Tài Nguyên HuggingFace: https://huggingface.co/ (Models \u0026amp; Thư Viện) BERT Paper: \u0026ldquo;BERT: Pre-training of Deep Bidirectional Transformers\u0026rdquo; (Devlin et al., 2018) T5 Paper: \u0026ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u0026rdquo; (Raffel et al., 2019) Attention Paper: \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., 2017) Chúc Mừng! Bạn Đã Hoàn Thành 10 Tuần Đào Tạo Toàn Diện NLP. Bây Giờ Bạn Hiểu:\nTransformers Hoạt Động Như Thế Nào (Toán \u0026amp; Triển Khai) Cách Transfer Learning Cho Phép Phát Triển Nhanh Chóng Cách Fine-tune Các Mô Hình Cho Các Tác Vụ Cụ Thể Cách Triển Khai Các Mô Hình Trong Sản Xuất Bây Giờ Hãy Xây Dựng Một Cái Gì Đó Tuyệt Vời! 🚀\nTương Lai Của NLP Không Phải Là Xây Dựng Các Mô Hình Từ Đầu—Đó Là Áp Dụng Sáng Tạo Các Mô Hình Được Huấn Luyện Trước Để Giải Quyết Các Vấn Đề Thực Tế. Bạn Có Nền Tảng Để Làm Chính Xác Điều Đó.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.5-week5/",
	"title": "Tuần 5 - Bảo mật &amp; Danh tính trên AWS",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-06 đến 2025-10-10\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 5 Tuần này tập trung vào bảo mật và quản lý danh tính trên AWS.\nNội dung chính Mô hình Trách nhiệm chia sẻ. AWS IAM (Users, Groups, Roles, Policies). Amazon Cognito. AWS Organizations \u0026amp; SCPs. AWS Identity Center (SSO). AWS KMS. AWS Security Hub. Labs thực hành Lab 18: AWS Security Hub. Lab 22: AWS Lambda Automation with Slack. Lab 27: AWS Resource Groups \u0026amp; Tagging. Lab 28: IAM Cross-Region Role \u0026amp; Policy. Lab 30: IAM Restriction Policy. Lab 33: AWS KMS \u0026amp; CloudTrail Integration. Lab 44: IAM Advanced Role Control. Lab 48: IAM Access Keys \u0026amp; Roles. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "Khi bạn tạo một Interface Endpoint hoặc cổng, bạn có thể đính kèm một chính sách điểm cuối để kiểm soát quyền truy cập vào dịch vụ mà bạn đang kết nối. Chính sách VPC Endpoint là chính sách tài nguyên IAM mà bạn đính kèm vào điểm cuối. Nếu bạn không đính kèm chính sách khi tạo điểm cuối, thì AWS sẽ đính kèm chính sách mặc định cho bạn để cho phép toàn quyền truy cập vào dịch vụ thông qua điểm cuối.\nBạn có thể tạo chính sách chỉ hạn chế quyền truy cập vào các S3 bucket cụ thể. Điều này hữu ích nếu bạn chỉ muốn một số Bộ chứa S3 nhất định có thể truy cập được thông qua điểm cuối.\nTrong phần này, bạn sẽ tạo chính sách VPC Endpoint hạn chế quyền truy cập vào S3 bucket được chỉ định trong chính sách VPC Endpoint.\nKết nối tới EC2 và xác minh kết nối tới S3. Bắt đầu một phiên AWS Session Manager mới trên máy chủ có tên là Test-Gateway-Endpoint. Từ phiên này, xác minh rằng bạn có thể liệt kê nội dung của bucket mà bạn đã tạo trong Phần 1: Truy cập S3 từ VPC. aws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Nội dung của bucket bao gồm hai tệp có dung lượng 1GB đã được tải lên trước đó.\nTạo một bucket S3 mới; tuân thủ mẫu đặt tên mà bạn đã sử dụng trong Phần 1, nhưng thêm \u0026lsquo;-2\u0026rsquo; vào tên. Để các trường khác là mặc định và nhấp vào Create. Tạo bucket thành công. Policy mặc định cho phép truy cập vào tất cả các S3 Buckets thông qua VPC endpoint.\nTrong giao diện Edit Policy, sao chép và dán theo policy sau, thay thế yourbucketname-2 với tên bucket thứ hai của bạn. Policy này sẽ cho phép truy cập đến bucket mới thông qua VPC endpoint, nhưng không cho phép truy cập đến các bucket còn lại. Chọn Save để kích hoạt policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Cấu hình policy thành công.\nTừ session của bạn trên Test-Gateway-Endpoint instance, kiểm tra truy cập đến S3 bucket bạn tạo ở bước đầu aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; Câu lệnh trả về lỗi bởi vì truy cập vào S3 bucket không có quyền trong VPC endpoint policy.\nTrở lại home directory của bạn trên EC2 instance cd~ Tạo file fallocate -l 1G test-bucket2.xyz Sao chép file lên bucket thứ 2 aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; Thao tác này được cho phép bởi VPC endpoint policy.\nSau đó chúng ta kiểm tra truy cập vào S3 bucket đầu tiên\naws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt;\nCâu lệnh xảy ra lỗi bởi vì bucket không có quyền truy cập bởi VPC endpoint policy.\nTrong phần này, bạn đã tạo chính sách VPC Endpoint cho Amazon S3 và sử dụng AWS CLI để kiểm tra chính sách. Các hoạt động AWS CLI liên quan đến bucket S3 ban đầu của bạn thất bại vì bạn áp dụng một chính sách chỉ cho phép truy cập đến bucket thứ hai mà bạn đã tạo. Các hoạt động AWS CLI nhắm vào bucket thứ hai của bạn thành công vì chính sách cho phép chúng. Những chính sách này có thể hữu ích trong các tình huống khi bạn cần kiểm soát quyền truy cập vào tài nguyên thông qua VPC Endpoint.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nĐảm bảo truy cập Hybrid an toàn đến S3 bằng cách sử dụng VPC endpoint Tổng quan AWS PrivateLink cung cấp kết nối riêng tư đến các dịch vụ aws từ VPCs hoặc trung tâm dữ liệu (on-premise) mà không làm lộ lưu lượng truy cập ra ngoài public internet.\nTrong bài lab này, chúng ta sẽ học cách tạo, cấu hình, và kiểm tra VPC endpoints để cho phép workload của bạn tiếp cận các dịch vụ AWS mà không cần đi qua Internet công cộng.\nChúng ta sẽ tạo hai loại endpoints để truy cập đến Amazon S3: gateway vpc endpoint và interface vpc endpoint. Hai loại vpc endpoints này mang đến nhiều lợi ích tùy thuộc vào việc bạn truy cập đến S3 từ môi trường cloud hay từ trung tâm dữ liệu (on-premise).\nGateway - Tạo gateway endpoint để gửi lưu lượng đến Amazon S3 hoặc DynamoDB using private IP addresses. Bạn điều hướng lưu lượng từ VPC của bạn đến gateway endpoint bằng các bảng định tuyến (route tables) Interface - Tạo interface endpoint để gửi lưu lượng đến các dịch vụ điểm cuối (endpoints) sử dụng Network Load Balancer để phân phối lưu lượng. Lưu lượng dành cho dịch vụ điểm cuối được resolved bằng DNS. Nội dung Tổng quan về workshop Chuẩn bị Truy cập đến S3 từ VPC Truy cập đến S3 từ TTDL On-premises VPC Endpoint Policies (làm thêm) Dọn dẹp tài nguyên "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Xin chúc mừng bạn đã hoàn thành xong lab này! Trong lab này, bạn đã học về các mô hình kiến trúc để truy cập Amazon S3 mà không sử dụng Public Internet.\nBằng cách tạo Gateway endpoint, bạn đã cho phép giao tiếp trực tiếp giữa các tài nguyên EC2 và Amazon S3, mà không đi qua Internet Gateway. Bằng cách tạo Interface endpoint, bạn đã mở rộng kết nối S3 đến các tài nguyên chạy trên trung tâm dữ liệu trên chỗ của bạn thông qua AWS Site-to-Site VPN hoặc Direct Connect. Dọn dẹp Điều hướng đến Hosted Zones trên phía trái của bảng điều khiển Route 53. Nhấp vào tên của s3.us-east-1.amazonaws.com zone. Nhấp vào Delete và xác nhận việc xóa bằng cách nhập từ khóa \u0026ldquo;delete\u0026rdquo;. Disassociate Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. 4.Mở console của CloudFormation và xóa hai stack CloudFormation mà bạn đã tạo cho bài thực hành này:\nPLOnpremSetup PLCloudSetup Xóa các S3 bucket Mở bảng điều khiển S3 Chọn bucket chúng ta đã tạo cho lab, nhấp chuột và xác nhận là empty. Nhấp Delete và xác nhận delete. "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.6-week6/",
	"title": "Tuần 6 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-13 đến 2025-10-17\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 6 Tuần này tập trung vào hệ sinh thái cơ sở dữ liệu trên AWS: từ dịch vụ quan hệ managed, NoSQL chuyên dụng, bộ nhớ đệm in-memory cho tới data warehouse phân tích.\nNội dung chính Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Labs thực hành Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTrong suốt thời gian thực tập tại [Tên công ty/tổ chức] từ [ngày bắt đầu] đến [ngày kết thúc], tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia [mô tả ngắn gọn dự án hoặc công việc chính], qua đó cải thiện kỹ năng [liệt kê kỹ năng: lập trình, phân tích, viết báo cáo, giao tiếp…].\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ✅ ☐ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ☐ ☐ ✅ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Nâng cao tính kỹ luật, chấp hành nghiêm chỉnh nội quy của công ty hoặc bất kỳ trong một tổ chức nào Cải thiện trong cách tư duy giải quyết vấn đề Học cách giao tiếp tốt hơn trong giao tiếp hằng ngày và trong công việc, xử lý tình huống "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Lưu ý: Các thông tin dưới đây chỉ nhằm mục đích tham khảo, vui lòng không sao chép nguyên văn cho bài báo cáo của bạn kể cả warning này.\nTại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu hoặc team bonding để mọi người hiểu nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi. Mình đánh giá cao việc mentor cho phép mình thử và tự xử lý vấn đề thay vì chỉ đưa đáp án.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Đề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Bạn có muốn tiếp tục chương trình này trong tương lai? Góp ý khác (tự do chia sẻ): "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.7-week7/",
	"title": "Tuần 7 - Vertical Slice Delivery",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-20 đến 2025-10-24\nTrạng thái: \u0026ldquo;Hoàn thành\u0026rdquo;\nTổng quan tuần 7 Tuần này hoàn thiện vertical slice 0 cho dự án Ebook Demo, tập trung vào contract-first development và tự động hóa kiểm thử để có thể demo end-to-end sớm.\nNội dung chính Vertical Slice Architecture \u0026amp; phạm vi slice 0 Contract-first development với OpenAPI + Prism mock Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture và cấu hình CORS Schemathesis contract testing \u0026amp; retrospective Labs thực hành Checklist demo vertical slice 0 Mock API bằng Prism và kết nối Next.js Refactor backend FastAPI theo clean architecture Chạy Schemathesis và cập nhật workflow chuẩn "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Lê Trọng Nhân\nSố điện thoại: SE190515\nEmail: nhanle221199@gmail.com\nTrường: Đại học FPT TP.HCM\nNgành: Công nghệ thông tin\nLớp: SE190525\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 12/08/2025 đến ngày 12/11/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.8-week8/",
	"title": "Tuần 8 - Xử Lý Ngôn Ngữ Tự Nhiên &amp; Deep Learning",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-10-27 to 2025-10-31\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTổng Quan Tuần 8 Tuần này cung cấp cái nhìn sâu rộng về Xử Lý Ngôn Ngữ Tự Nhiên (NLP), bao gồm nền tảng ngôn ngữ học, ứng dụng NLP hiện đại, kiến trúc sequence-to-sequence, và phương pháp đánh giá. Từ hiểu biết về âm vị học đến triển khai hệ thống dịch máy, tuần này kết nối lý thuyết và thực hành trong NLP.\nCác Chủ Đề Chính Nền Tảng Ngôn Ngữ Học Các thành phần cốt lõi: Âm Vị Học, Âm Vị Học, Hình Thái Học, Cú Pháp, Ngữ Nghĩa, Thực Dụng Hiểu cấu trúc ngôn ngữ ảnh hưởng đến thiết kế NLP như thế nào Ứng Dụng NLP Công cụ tìm kiếm và nhận dạng ý định Quảng cáo trực tuyến với NER và trích xuất mối quan hệ Trợ lý giọng nói và nhận dạng giọng nói Chatbot với pipeline NLU/NLG Hệ thống dịch máy Tóm tắt văn bản (extractive \u0026amp; abstractive) Kiến Trúc Deep Learning Mô hình Seq2seq với kiến trúc encoder-decoder LSTM chi tiết: forget gate, input gate, cell state, output gate Cơ chế Attention và self-attention Triển khai Neural Machine Translation (NMT) Đánh Giá \u0026amp; Decoding Điểm BLEU (dựa trên precision) Điểm ROUGE (dựa trên recall) Điểm F1 để đánh giá MT Beam search decoding Minimum Bayes Risk (MBR) sampling Phòng Thí Nghiệm Thực Hành Xây dựng workflow voicebot và chatbot Triển khai LSTM cho sequence modeling Tạo encoder-decoder với attention Neural machine translation end-to-end Đánh giá chất lượng dịch với BLEU/ROUGE Triển khai beam search và MBR "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.9-week9/",
	"title": "Tuần 9 - Kiến Trúc &amp; Triển Khai Transformer",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-03 to 2025-11-07\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTóm Tắt Tuần 9 Tuần này khám phá kiến trúc Transformer, một mô hình cách mạng thay thế RNNs trong NLP. Chúng ta sẽ hiểu tại sao cần transformers, cách chúng hoạt động bên trong, và triển khai chúng từ đầu. Từ các cơ chế attention đến thiết kế encoder-decoder đầy đủ, tuần này kết nối lý thuyết và triển khai thực tế.\nCác Chủ Đề Chính Hạn Chế của RNNs \u0026amp; Giới Thiệu Transformer Thắt cổ chai xử lý tuần tự trong RNNs Vấn đề Vanishing Gradient Thắt cổ chai thông tin với chuỗi dài Tại sao Attention là tất cả bạn cần Kiến Trúc Transformer Cấu trúc Encoder-Decoder Lớp Multi-head Attention Positional Encoding Residual Connections \u0026amp; Layer Normalization Feed-forward Networks Cơ Chế Attention Scale Dot-product Attention (cơ chế lõi) Self-attention (cùng một câu) Masked Attention (Decoder) Encoder-Decoder Attention Multi-head Attention để tính toán song song Transformer Decoder \u0026amp; GPT2 Positional Embeddings Decoder Block Implementation Feed-forward Layer Design Tính toán Xác suất Output Ứng Dụng \u0026amp; Các Mô Hình GPT-2 (Generative Pre-trained Transformer) BERT (Bidirectional Encoder Representations) T5 (Text-to-Text Transfer Transformer) Ứng dụng: Dịch, Phân loại, QA, Tóm tắt, Phân tích Cảm xúc Mục Tiêu Học Tập ✅ Hiểu hạn chế của RNN và tại sao transformers giải quyết chúng ✅ Nắm bắt kiến trúc transformer hoàn chỉnh ✅ Triển khai các cơ chế attention từ đầu ✅ Xây dựng transformer decoder (kiểu GPT2) ✅ Nhận biết các ứng dụng transformer và các mô hình tiên tiến Chia Nhỏ Theo Ngày Ngày Tập Trung Chủ Đề 41 Vấn Đề RNN Xử lý tuần tự, Vanishing Gradients, Thắt cổ chai thông tin 42 Tổng Quan Kiến Trúc Encoder-Decoder, Multi-head Attention, Positional Encoding 43 Lõi Attention Công thức Scale Dot-product, Phép toán ma trận, Hiệu quả GPU 44 Các Loại Attention Self-attention, Masked Attention, Encoder-Decoder Attention 45 Triển Khai Decoder Kiến trúc GPT2, Các khối xây dựng, Hướng dẫn Code Điều Kiện Tiên Quyết Hiểu biết sâu về RNNs, LSTMs, và attention từ Tuần 8 Thoải mái với phép toán ma trận và đại số tuyến tính Kiến thức PyTorch hoặc TensorFlow rất hữu ích Các Bước Tiếp Theo Học bài báo \u0026ldquo;Attention is All You Need\u0026rdquo; (Vaswani et al., 2017) Triển khai các thành phần transformer từng bước Thử nghiệm với các mô hình được huấn luyện trước (BERT, GPT-2, T5) "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/1-worklog/1.10-week10/",
	"title": "Tuần 10 - Transfer Learning, BERT &amp; T5",
	"tags": [],
	"description": "",
	"content": "Tuần: 2025-11-10 to 2025-11-14\nTrạng Thái: \u0026ldquo;Hoàn Thành\u0026rdquo;\nTóm Tắt Tuần 10 Tuần cuối cùng này tập trung vào transfer learning và các mô hình tiên tiến (BERT và T5). Chúng ta sẽ học cách pre-training trên các tập dữ liệu lớn không được gắn nhãn cải thiện hiệu suất tác vụ hạ nguồn. Từ masked language modeling đến multi-task learning, tuần này tổng hợp mọi thứ thành các hệ thống NLP sẵn sàng sản xuất.\nCác Chủ Đề Chính Cơ Bản Transfer Learning Tiếp cận dựa trên tính năng vs fine-tuning Pre-training và các tác vụ hạ nguồn Dữ liệu có gắn nhãn vs không gắn nhãn Lợi ích: đào tạo nhanh hơn, dự đoán tốt hơn, tập dữ liệu nhỏ hơn Kiến Trúc BERT Bidirectional Encoder Representations Các transformer đa lớp lưỡng chiều 12 lớp, 12 attention heads, 110M tham số (BERT-base) Kiến trúc encoder-only Mục Tiêu Pre-training Masked Language Modeling (MLM): che 15% tokens, dự đoán chúng Next Sentence Prediction (NSP): dự đoán nếu các câu theo sau nhau Kết hợp loss: MLM + NSP Đầu Vào BERT Token embeddings Segment embeddings (câu A vs B) Positional embeddings Token [CLS] để phân loại Token [SEP] để phân tách Mô Hình T5 Text-to-Text Transfer Transformer Kiến trúc Encoder-Decoder Multi-task training với các tiền tố tác vụ Tiên tiến nhất trên 10+ benchmarks Ứng Dụng Fine-tuning Phân Tích Cảm Xúc Named Entity Recognition (NER) Question Answering Phát Hiện Paraphrase Tương Tự Ngữ Cảnh Phân Loại Văn Bản Chia Nhỏ Theo Ngày Ngày Tập Trung Chủ Đề 46 Transfer Learning Feature-based, Fine-tuning, Pre-training objectives 47 BERT Architecture Design, Pre-training, Bidirectionality 48 Pre-training Tasks MLM, NSP, Loss functions, Input representation 49 T5 Model Encoder-Decoder, Multi-tasking, Task prefixes 50 Fine-tuning \u0026amp; Apps Downstream tasks, Practical applications Mục Tiêu Học Tập ✅ Hiểu các nguyên tắc transfer learning ✅ Nắm bắt bối cảnh lưỡng chiều của BERT ✅ Triển khai masked language modeling ✅ Học multi-task training với T5 ✅ Fine-tune models cho các tác vụ hạ nguồn So Sánh Mô Hình Mô Hình Loại Pre-training Bối Cảnh Tham Số Word2Vec Static Skip-gram/CBOW Fixed window N/A ELMo Dynamic Language modeling Bi-directional LSTM 94M GPT Causal Language modeling Uni-directional 117M BERT Masked MLM + NSP Bi-directional 110M T5 Multi-task Masked span Both directions 220M-11B Những Hiểu Biết Chính ✅ Pre-training quan trọng: Các mô hình được huấn luyện trên 800GB văn bản vượt trội hơn rất nhiều so với những mô hình được huấn luyện từ đầu ✅ Lưỡng chiều giúp: Bối cảnh lưỡng chiều của BERT cải thiện sự hiểu biết ✅ Multi-task learning hoạt động: T5 xử lý nhiều tác vụ với một mô hình ✅ Fine-tuning hiệu quả: Mất vài ngày thay vì vài tháng ✅ Dữ liệu không gắn nhãn là mạnh: Pre-training không sử dụng nhãn, chỉ văn bản thô\nĐiều Kiện Tiên Quyết Hiểu biết hoàn chỉnh về kiến trúc transformer (Tuần 9) Quen thuộc với các tác vụ NLP (phân loại, NER, QA) Kiến thức PyTorch hoặc TensorFlow rất hữu ích Các Bước Tiếp Theo Triển khai BERT fine-tuning cho tác vụ của bạn Thử nghiệm với T5 cho các vấn đề text-to-text Sử dụng thư viện Hugging Face transformers Khám phá các mô hình lớn hơn (RoBERTa, ELECTRA, XLNet) Triển khai các mô hình được huấn luyện trước trong sản xuất "
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hugo_aws/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
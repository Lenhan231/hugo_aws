[
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.1-blog1/",
	"title": "Accelerate Your Data and AI Flow by Connecting Amazon SageMaker Unified Studio to Visual Studio Code",
	"tags": [],
	"description": "",
	"content": "Accelerate Your Data and AI Flow by Connecting Amazon SageMaker Unified Studio to Visual Studio Code by Lauren Mullennex, Anagha Barve, Anchit Gupta, and Bhargava Varadharajan — September 12, 2025 · Amazon SageMaker AI · Amazon SageMaker Unified Studio · Announcements · Intermediate (200) · Technical How-to\nDevelopers and machine learning (ML) engineers can now connect their local Visual Studio Code (VS Code) editor directly to Amazon SageMaker Unified Studio. This capability lets you preserve your existing development workflow and personalized integrated development environment (IDE) configuration while still taking advantage of AWS analytics and AI/ML services in a single, unified data-and-AI studio. The integration provides seamless access from your laptop to scalable infrastructure for data processing, SQL analytics, and ML workloads. By linking your local IDE to SageMaker Unified Studio, you can optimize data and AI development pipelines without disrupting the best practices already in place.\nIn this post, we show how to connect your local copy of VS Code to SageMaker Unified Studio so you can build end-to-end data and AI workflows while continuing to work from your favorite IDE.\nSolution overview The architecture relies on three core components:\nLocal workstation – Your development machine runs VS Code with the AWS Toolkit for Visual Studio Code and Microsoft Remote SSH installed. Use the Toolkit extension to browse SageMaker Unified Studio spaces and select the environment you want to work in. SageMaker Unified Studio – As part of the next generation of Amazon SageMaker, Unified Studio is a single experience where you can find and access your data, then work with familiar AWS tools for SQL analytics, data preparation, model development, and generative-AI application development. AWS Systems Manager – A secure, scalable remote-access and management service that links your local VS Code environment to SageMaker Unified Studio spaces and streamlines your data and AI workflows. The diagram below shows how your local IDE interacts with SageMaker Unified Studio spaces.\nPrerequisites To try the remote IDE experience, make sure you have:\nAccess to a SageMaker Unified Studio domain with internet connectivity. For domains configured in VPC-only mode, the domain must reach the internet through a proxy or NAT gateway. If your domain is completely isolated, follow the remote IDE support guide. You can create a domain via the quick setup or manual setup workflow. An SSO user authenticated through IAM Identity Center. See the user management documentation to configure SSO access. Access to, or the ability to create, a SageMaker Unified Studio project. A compute space (JupyterLab or Code Editor) with at least 8 GB of memory. In this walkthrough we use an ml.t3.large instance. SageMaker Distribution image version 2.8 or later is supported. The latest stable build of VS Code on your local machine with Microsoft Remote SSH (version 0.74.0 or later) and the AWS Toolkit extension (version 3.74.0). Implement the solution Complete the following steps to enable remote access and connect to a space from VS Code. A SageMaker Unified Studio space must have remote access turned on before you can reach it from your IDE.\nOpen your JupyterLab or Code Editor space. If it is already running, stop the space and choose Configure space so you can enable remote access. Turn on Remote access and choose Save and restart. In your local VS Code, open the AWS Toolkit pane. On the SageMaker Unified Studio tab, choose Sign in, then supply your domain URL, for example https://\u0026lt;domain-id\u0026gt;.sagemaker.\u0026lt;region\u0026gt;.on.aws. You are redirected to a browser to authorize the AWS IDE extensions. Choose Open to launch the new browser tab. Choose Allow access so the extensions can reach your project from VS Code. When you see Request approved, you have remote access to the domain. Return to your local VS Code window to keep working on ETL jobs, data pipelines, ML training and deployment, or generative-AI applications. To connect to a project for data processing and ML development, perform these steps:\nChoose Select a project to view data assets and compute resources. All projects in the domain are listed, but you can only open the projects where you are a member.\nYou can view only one domain and one project at a time. To switch projects or sign out of the domain, select the ellipsis icon. You can also review the data and compute resources you created earlier. Connect to a JupyterLab or Code Editor space by choosing the connect icon. If the icon is hidden, remote access might be disabled for that space. If the space status is “Stopped,” hover over it and choose connect—this enables remote access, starts the space, and opens the connection. If the space is “Running,” stop it, enable remote access, and reconnect from the Toolkit.\nVS Code opens a second window that connects to your SageMaker Unified Studio space over remote SSH. Go to Explorer to see the notebooks, files, and scripts inside the space. From the AWS Toolkit you can also browse the data sources that belong to the project.\nUse your custom VS Code setup with SageMaker Unified Studio resources When VS Code connects to SageMaker Unified Studio, you keep all of your personal hotkeys, settings, and snippets. If you rely on snippets to insert common analytics or ML patterns, they continue to work while you run on SageMaker Unified Studio’s managed infrastructure.\nIn the following example we run analytics snippets: show-databases queries Amazon Athena to list available databases, show-glue-tables lists the tables stored in the AWS Glue Data Catalog, and query-ecommerce uses Spark SQL to fetch data for analysis.\nYou can also use snippets to automate ML model builds and training on SageMaker AI. The screenshot below shows snippets that handle data processing, configure training, and launch a SageMaker AI training job. This approach lets data practitioners keep their familiar dev setup while tapping into the managed data and AI resources inside SageMaker Unified Studio.\nDisable remote access in SageMaker Unified Studio If you are an administrator and need to disable this feature, add the following policy statement to the project’s IAM role:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;DenyStartSessionForSpaces\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:StartSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sagemaker:*:*:space/*/*\u0026#34; } ] } Clean up By default, SageMaker Unified Studio stops idle resources such as JupyterLab and Code Editor spaces after 1 hour. If you created a dedicated SageMaker Unified Studio domain just for this walkthrough, remember to delete the domain when you finish.\nConclusion Connecting your local IDE directly to Amazon SageMaker Unified Studio eliminates the friction of jumping between on-device development and scalable data-and-AI infrastructure. Because you keep your personalized IDE configuration, you no longer need to switch mindsets between environments. Whether you are processing large datasets, training foundation models, or building generative-AI applications, you can remain in your local setup while accessing all of SageMaker Unified Studio’s capabilities. Connect your IDE to SageMaker Unified Studio today to streamline data workflows and speed up ML development.\nAbout the authors Lauren Mullennex\rLauren is a Senior Specialist Solutions Architect for GenAI/ML at AWS. She has more than a decade of experience across machine learning, DevOps, and infrastructure, and is the author of a published computer-vision book. Outside of work you can find her traveling and hiking with her two dogs.\rBhargava Varadharajan\rBhargava is a Senior Software Engineer at AWS, where he builds AI and ML products such as SageMaker Studio, Studio Lab, and Unified Studio. For more than five years he has focused on turning complex AI/ML workflows into seamless experiences. When he is not designing large-scale systems, Bhargava pursues his goal of visiting all 63 U.S. National Parks and looks for adventure through hiking, soccer, and skiing. He also splits his free time between DIY projects and nurturing curiosity through books.\rAnagha Barve\rAnagha is a Software Development Manager at AWS, where she leads the Amazon SageMaker Unified Studio team.\rAnchit Gupta\rAnchit is a Senior Product Manager for Amazon SageMaker Unified Studio. She focuses on delivering products that make it easier to build machine learning solutions. In her spare time, she enjoys cooking, playing board and card games, and traveling.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/",
	"title": "Day 01 - Introduction to Cloud Computing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-08 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes What Is Cloud Computing? The on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Benefits of Cloud Computing Pay only for what you use, optimizing cost efficiency. Accelerate development through automation and managed services. Scale resources up or down as needed. Deploy applications globally in minutes. Why AWS? AWS has been the global cloud leader for 13 consecutive years (as of 2023). Unique culture, vision, and long-term customer obsession. AWS pricing philosophy: customers should pay less over time for the same resources. Every AWS Leadership Principle is focused on delivering real customer value. How to Get Started with AWS There are many learning paths—self-study is completely possible. Register an AWS Free Tier account to explore. Recommended course platforms: Udemy A Cloud Guru Explore AWS learning paths: AWS Learning Paths "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/",
	"title": "Day 06 - Amazon VPC Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-15 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Networking Services on AWS Amazon Virtual Private Cloud (VPC) Amazon Virtual Private Cloud (Amazon VPC) allows you to launch AWS resources into a virtual network you define. A VPC exists within a single Region. When creating a VPC, you must define an IPv4 CIDR block (required) and optionally an IPv6 one. The default limit is 5 VPCs per Region per Account. Commonly used to separate environments such as Production, Development, and Staging. To achieve full resource isolation, use separate AWS Accounts rather than multiple VPCs. Subnets A subnet resides within one Availability Zone. The subnet CIDR must be a subset of the parent VPC\u0026rsquo;s CIDR block. AWS reserves 5 IP addresses in each subnet: network, broadcast, router, DNS, and future use. Reserved IP Addresses Example (10.0.0.0/24):\n10.0.0.0 - Network address 10.0.0.1 - VPC router 10.0.0.2 - DNS server 10.0.0.3 - Reserved for future use 10.0.0.255 - Broadcast address Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking Basics Create VPC → 03-03.1 Create Subnet → 03-03.2 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/",
	"title": "Day 11 - Amazon EC2 Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-22 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Compute on AWS Amazon Elastic Compute Cloud (EC2) Amazon EC2 provides resizable compute capacity in the cloud, similar to a virtual or physical server. It supports workloads such as web hosting, applications, databases, authentication services, and other general-purpose server tasks. Instance Types\nEC2 configurations are defined by instance types, not custom hardware. Each type specifies: CPU (Intel, AMD, ARM – Graviton 1/2/3) / GPU Memory Network Storage Instance Type Categories:\nGeneral Purpose: T3, T4g, M5, M6i (balanced compute, memory, networking) Compute Optimized: C5, C6i, C7g (high-performance processors) Memory Optimized: R5, R6i, X2 (fast performance for memory-intensive workloads) Storage Optimized: I3, D2, H1 (high sequential read/write to local storage) Accelerated Computing: P4, G5, Inf1 (GPU/FPGA for ML, graphics) Amazon Machine Images (AMI) AMI (Amazon Machine Image) is a template that defines the software configuration of an instance, including OS, apps, and settings. Types of AMIs: Provided by AWS (Amazon Linux, Windows, Ubuntu, etc.) AWS Marketplace AMIs Custom AMIs created by users Benefits of Custom AMIs\nFaster instance launch and setup Simplified backup and restore Consistent environment across multiple instances AMI Components:\nRoot volume template (OS and applications) Launch permissions Block device mapping Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Setup Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/",
	"title": "Day 16 - Amazon S3 Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-29 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Storage Services on AWS Amazon Simple Storage Service (S3) Amazon S3 is an object storage service designed to store and retrieve any amount of data from anywhere on the web. It offers virtually unlimited scalability, high availability, strong security, and excellent performance.\nCore S3 Features Buckets and Objects: Data is stored as objects inside buckets. Each object can be up to 5 TB. Availability and Durability: S3 is designed for 99.99% availability and 99.999999999% (11 nines) durability. Security: Multiple layers of security including IAM, bucket policies, ACLs, and encryption. Scalability: Automatically scales storage and request throughput without performance degradation. S3 Object Structure:\nKey: Object name/path Value: Object data Version ID: For versioning Metadata: System and user metadata Access Control: Permissions S3 Access Points Access Points simplify managing data access for shared datasets in S3.\nPer-application access control: Each access point has its own policy. Operational simplicity: Eases permission management for shared datasets used by many applications. Network controls: Can be configured to accept requests only from specific VPCs. S3 Storage Classes Choose among storage classes optimized for different access patterns and cost profiles:\nS3 Standard: For frequently accessed data; highest availability and performance. S3 Intelligent-Tiering: Automatically moves objects between tiers to optimize cost. S3 Standard-IA (Infrequent Access): Lower cost for infrequently accessed data with millisecond retrieval. S3 One Zone-IA: Like Standard-IA but stored in a single AZ. S3 Glacier Flexible Retrieval: Low-cost archival with minutes-to-hours retrieval. S3 Glacier Deep Archive: Lowest-cost archival with ~12-hour retrieval. Storage Class Comparison:\nClass Durability Availability Min Storage Retrieval Standard 11 9\u0026rsquo;s 99.99% None Instant Intelligent-Tiering 11 9\u0026rsquo;s 99.9% None Instant Standard-IA 11 9\u0026rsquo;s 99.9% 30 days Instant One Zone-IA 11 9\u0026rsquo;s 99.5% 30 days Instant Glacier Flexible 11 9\u0026rsquo;s 99.99% 90 days Minutes-hours Glacier Deep Archive 11 9\u0026rsquo;s 99.99% 180 days 12 hours Hands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 1) Create S3 Bucket → 57-2.1 Load Data → 57-2.2 Enable Static Website → 57-3 Configure Public Access Block → 57-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/",
	"title": "Day 21 - Shared Responsibility &amp; IAM Basics",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-06 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Security Shared Responsibility Model In cloud computing, security is a shared responsibility between the cloud provider and the customer. Customers must securely configure services, apply best practices, and use security controls from the hypervisor exposure upward to application/data layers. The split of responsibilities varies by service model: Infrastructure-level services Partially managed services Fully managed services AWS Responsibilities (Security OF the Cloud):\nPhysical security of data centers Hardware and network infrastructure Virtualization infrastructure Managed service operations Customer Responsibilities (Security IN the Cloud):\nData encryption Network configuration Access management Application security Operating system patches (for EC2) AWS Identity and Access Management (IAM) Root Account Has unrestricted access to all AWS services/resources and can remove any attached permissions. Best practices: Create and use an IAM Administrator user for daily operations. Lock away root credentials (dual control). Keep the root user\u0026rsquo;s email and domain valid and renewed. Enable MFA on root account IAM Overview IAM controls access to AWS services and resources in your account. Principals include: root user, IAM users, federated users, IAM roles, assumed-role sessions, AWS services, and anonymous users. Notes: IAM users are not separate AWS accounts. New IAM users start with no permissions. Grant permissions by attaching policies to users, groups, or roles. Use IAM groups to manage many users (groups cannot be nested). Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 1) Create EC2 Instance → 48-1.1 Create S3 Bucket → 48-1.2 Generate IAM User and Access Key → 48-2.1 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/",
	"title": "Day 26 - Database Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-13 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Database Concepts Review A database is an organized (or semi-structured) collection of information stored on storage devices to support concurrent access by multiple users or programs with different goals. Sessions A session spans from the moment a client connects to the DBMS until the connection is terminated. Primary Key A primary key uniquely identifies each row in a relational table. Foreign Key A foreign key in one table references the primary key of another table, creating a relationship between them. Index An index accelerates data retrieval at the cost of extra writes and storage to maintain the index structure. Indexes locate data without scanning every row; they can be defined over one or more columns. Index Types:\nB-Tree: General purpose, balanced tree structure Hash: Fast equality lookups Bitmap: Efficient for low-cardinality columns Full-Text: Text search optimization Partitioning Partitioning splits a large table into smaller, independent pieces (partitions), potentially placed on different storage. Benefits: better query performance, easier maintenance, and scalability. Common types: Range (e.g., by date) List Hash Composite (combination) Partitioning Example:\n-- Range partitioning by date CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan A query plan details how the DBMS will execute an SQL statement (access paths, joins, sorts). Types: Estimated plan (before execution) Actual plan (from executed query) Key operators: table scan, index seek/scan, nested loops, hash/merge join, sort, aggregate, filter. Database Logs Database logs record all changes (INSERT/UPDATE/DELETE) and operations. Typical log types: transaction, redo, undo, binary logs. Uses: recovery, integrity, consistency/durability (ACID), replication, performance analysis. Buffers A buffer pool caches pages read from disk to minimize I/O. Management strategies: Replacement: LRU, FIFO, Clock Write policies: immediate vs. deferred Prefetching to warm the cache Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 1) Create a VPC → 05-2.1 Create EC2 Security Group → 05-2.2 Create RDS Security Group → 05-2.3 Create DB Subnet Group → 05-2.4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/",
	"title": "Day 31 - Vertical Slice Kickoff",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-20 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Project Context Ebook Demo – Vertical Slice 0 Goal: deliver an end-to-end demo of the book-detail experience before scaling the full system. Approach: adopt a Vertical Slice Architecture so every slice is built as a complete feature instead of layer-by-layer. Benefits: immediate demos, earlier bug discovery, and tighter coordination between frontend and backend. Slice Architecture User → Frontend → API → Database → Response → UI Each slice bundles UI, API contracts, backend logic, and demo data. Components can be swapped independently without breaking the rest of the system. Vertical Slice Architecture Core Principles Build features around the user journey instead of isolated layers. Keep the scope tight so each slice can demo quickly and gather feedback. Define slice ownership clearly to ease future extensions. Benefits Accelerates value delivery—you can show stakeholders right away. Lowers integration risk because every slice self-validates. Enables multiple slices to progress in parallel. Key Insights Vertical slices act as the foundation before expanding to additional features. Each slice needs a clear checklist (finished UI, validated contract, accurate backend responses). Treat every slice like a “mini product” with its own lifecycle to keep quality high. Hands-On Labs Define the scope of slice 0 (book-detail flow, minimum viable data). Draw the data-flow diagram and nail down the frontend/backend boundary. Standardize the demo checklist (contract, mock, UI, backend). "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/1.8.1-day36-2025-10-27/",
	"title": "Day 36 - NLP Foundations &amp; Applications",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-27 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nWhat is Natural Language Processing? Natural Language Processing (NLP) is a field of Artificial Intelligence that focuses on enabling computers to understand, interpret, generate, and interact with human language.\nNLP integrates computational linguistics, machine learning, and deep learning to process large-scale text and speech data.\nTypical NLP Tasks: Text classification Sentiment analysis Named Entity Recognition (NER) Machine translation Part-of-speech (POS) tagging Speech recognition Core Linguistic Components in NLP Phonetics – The Sounds of Human Speech Phonetics studies the physical properties of speech sounds.\nThree Branches: Articulatory phonetics: how sounds are produced (tongue, lips, vocal folds…) Acoustic phonetics: physical sound properties (frequency, amplitude, duration) Auditory phonetics: how humans perceive sounds NLP Relevance: Used in speech recognition, speech synthesis (TTS), acoustic modeling.\nPhonology – Sound Systems of Languages Phonology studies how sounds function within a particular language. It deals with phonemes, stress patterns, allowable sound combinations.\nNLP Relevance: Grapheme-to-phoneme conversion, pronunciation modeling.\nMorphology – Structure of Words Morphology studies how words are formed from smaller units called morphemes.\nExamples: Prefixes: un-, re-, pre- Suffixes: -ing, -ed, -ness Roots/stems: run, happy, form NLP Relevance:\nStemming Lemmatization Tokenization Vocabulary building for BoW models NLP Applications Search Engines Your daily searches in the search engines are facilitated by NLP for query understanding and result ranking.\nSearch Intent Recognition Example When someone searches for \u0026ldquo;glass coffee tables\u0026rdquo;, the intent engine determines that the word \u0026ldquo;glass\u0026rdquo; likely refers to the value of attribute \u0026lsquo;Top Material\u0026rsquo; in coffee tables. It then directs the search engine accordingly to show the coffee tables category with the \u0026lsquo;Top Material\u0026rsquo; attribute set to \u0026lsquo;glass\u0026rsquo;.\nOnline Advertising NLP enables targeted ads by analyzing online behavior through multiple components:\n1. Named Entity Recognizer (NER) Identifies selected information elements called Named Entities (NE). Due to unavailability of labeled data, semi-supervised approaches are adapted to detect project use-case specific entities.\n2. Relationship Extraction One of the classical NLP tasks which aims at extracting semantic relationships from unstructured or semi-structured text documents.\n3. Moment Recognizer (MoRec) Enables analysts to understand forum discussions in the knowledge discovery phase by processing unstructured discussion text and extracting knowledge in terms of events. Events can be defined and configured depending on the use-case under investigation.\nVoice Assistants Siri, Alexa, and Google Assistant use NLP to understand and respond to your voice commands.\nMachine Translation Services like Google Translate rely on NLP to convert text from one language to another.\nChatbots Customer service chatbots use NLP to interact with users and provide assistance.\nText Summarization NLP algorithms can condense long articles into brief summaries.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/1.9.1-day41-2025-11-03/",
	"title": "Day 41 - RNN Problems &amp; Why Transformers Are Needed",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-03 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nThe Problem with RNNs: Sequential Processing Bottleneck RNNs have dominated NLP for years, but they have fundamental limitations that transformers solve. Let\u0026rsquo;s explore these issues.\nProblem 1: Sequential Computation How RNNs Process Information RNNs must process inputs one step at a time, sequentially:\nTranslation Example (English → French):\nInput: \u0026#34;I am happy\u0026#34;\rTime Step 1: Process \u0026#34;I\u0026#34;\rTime Step 2: Process \u0026#34;am\u0026#34; Time Step 3: Process \u0026#34;happy\u0026#34; Impact:\nIf your sentence has 5 words → 5 sequential steps required If your sentence has 1000 words → 1000 sequential steps required Cannot parallelize! Must wait for step t-1 before computing step t Why This Matters Modern GPUs and TPUs are designed for parallel computation Sequential RNNs cannot take advantage of this parallelization Training becomes much slower than necessary Longer sequences = exponentially longer training time Problem 2: Vanishing Gradient Problem The Root Cause When RNNs backpropagate through many time steps, gradients get multiplied repeatedly:\nGradient Flow Through T Steps:\n∂Loss/∂h₀ = ∂Loss/∂hₜ × (∂hₜ/∂hₜ₋₁) × (∂hₜ₋₁/∂hₜ₋₂) × ... × (∂h₁/∂h₀) If each ∂hᵢ/∂hᵢ₋₁ \u0026lt; 1 (which it often is):\nAfter T multiplications: gradient ≈ 0.5^100 ≈ 0 (for T=100) Gradient vanishes to zero Model can\u0026rsquo;t learn long-range dependencies Concrete Example Sentence: \u0026ldquo;The students who studied hard\u0026hellip; passed the exam\u0026rdquo;\nEarly word \u0026ldquo;students\u0026rdquo; needs to influence prediction of \u0026ldquo;passed\u0026rdquo; But gradient has vanished by the time it reaches \u0026ldquo;students\u0026rdquo; Model fails to learn this relationship! Current Workarounds LSTMs and GRUs help a little with gates, but:\nStill have issues with very long sequences (\u0026gt;100-200 words) Cannot fully solve the problem Still require sequential processing Problem 3: Information Bottleneck The Compression Issue Sequence-to-Sequence Architecture:\nEncoder: Word₁ → h₁ → h₂ → h₃ → hₜ (final hidden state)\rDecoder: hₜ → Word₁\u0026#39; → Word₂\u0026#39; → Word₃\u0026#39; → ... The Bottleneck: All information from the entire input sequence is compressed into a single vector hₜ (the final hidden state).\nWhy This Fails Example Sentence: \u0026ldquo;The government of the United States of America announced\u0026hellip;\u0026rdquo;\nWhen encoding this 8-word sentence:\nFirst word \u0026ldquo;The\u0026rdquo; is processed Information flows through states: h₁ → h₂ → h₃ → \u0026hellip; → h₈ By h₈, information about \u0026ldquo;The\u0026rdquo; has been diluted/lost Only h₈ is passed to decoder Decoder has limited context about early words The Consequence Long sequences lose information Important early context gets diluted Model struggles with long documents Translation quality degrades for long sentences Summary: Why RNNs Have Fundamental Issues Issue Impact Current Solution Sequential Processing Cannot parallelize, slow training N/A - Fundamental to RNN design Vanishing Gradients Can\u0026rsquo;t learn long-range dependencies LSTM/GRU gates (partial fix) Information Bottleneck Early information gets lost Attention mechanism (partial fix) The Transformer Solution: \u0026ldquo;Attention is All You Need\u0026rdquo; Introduced in 2017 by Google researchers (Vaswani et al.), transformers completely replace RNNs with attention mechanisms.\nKey Differences Aspect RNNs Transformers Processing Sequential (one word at a time) Parallel (all words at once) Dependency Each word depends on previous state All words attend to all words Training Speed Slow (sequential) Fast (parallel) Long Sequences Vanishing gradients No sequential bottleneck Long-range Deps. Difficult Easy (direct attention) How Transformers Work (Brief Overview) No RNN: Remove sequential hidden states completely Pure Attention: Let each word \u0026ldquo;attend to\u0026rdquo; every other word Positional Encoding: Add position information since we don\u0026rsquo;t have sequential order Parallel Processing: Process entire sequence at once Example:\nSentence: \u0026#34;I am happy\u0026#34;\rInstead of:\rStep 1: Process \u0026#34;I\u0026#34; → h₁\rStep 2: Process \u0026#34;am\u0026#34; with h₁ → h₂\rStep 3: Process \u0026#34;happy\u0026#34; with h₂ → h₃\rTransformer Does:\rParallel: \u0026#34;I\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rParallel: \u0026#34;am\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rParallel: \u0026#34;happy\u0026#34; attends to {\u0026#34;I\u0026#34;, \u0026#34;am\u0026#34;, \u0026#34;happy\u0026#34;}\rAll at once! ⚡ Why Everyone Talks About Transformers ✅ Speed: Can train much faster on GPUs/TPUs (parallel) ✅ Scalability: Can handle very long sequences (no bottleneck) ✅ Long-range: Direct attention solves gradient problems ✅ Versatility: Works for translation, classification, QA, summarization, chatbots\u0026hellip; ✅ Performance: Achieves state-of-the-art on nearly every NLP task\nApplications of Transformers Transformers are used for:\nTranslation (Neural Machine Translation) - High quality, fast Text Summarization (Abstractive \u0026amp; Extractive) Named Entity Recognition (NER) - Identify entities better Question Answering - Understand context better Chatbots \u0026amp; Voice Assistants Sentiment Analysis - Understand emotion/opinion Auto-completion - Smart suggestions Classification - Classify text into categories Market Intelligence - Analyze market sentiment State-of-the-Art Transformer Models GPT-2 (Generative Pre-trained Transformer) Created by: OpenAI Type: Decoder-only transformer Speciality: Text generation Famous for: Generating human-like text (even fooled journalists in 2019!) BERT (Bidirectional Encoder Representations from Transformers) Created by: Google AI Type: Encoder-only transformer Speciality: Text understanding \u0026amp; representations Use: Classification, NER, QA T5 (Text-to-Text Transfer Transformer) Created by: Google Type: Full encoder-decoder (like original transformer) Speciality: Multi-task learning Super versatile: Single model handles translation, classification, QA, summarization, regression Next Steps Now that we understand why transformers are revolutionary, we\u0026rsquo;ll learn:\nThe complete transformer architecture How attention mechanisms work mathematically How to implement a transformer decoder How to use pre-trained models (BERT, GPT-2, T5) Key Insight: The transformer replaced RNNs because it solved three fundamental problems through pure attention and parallel processing. This was a paradigm shift in NLP.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/1.10.1-day46-2025-11-10/",
	"title": "Day 46 - Transfer Learning Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-10 (Monday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nTransfer Learning: Why It Matters Transfer learning is THE most important technique in modern NLP. It\u0026rsquo;s why a model trained on Wikipedia can be fine-tuned in days instead of months.\nThe Classical Approach vs Transfer Learning Classical Machine Learning Task: Predict movie rating from review\rStep 1: Collect labeled data (movies + ratings)\rStep 2: Initialize model randomly\rStep 3: Train from scratch for weeks/months\rStep 4: Deploy\rProblem: Slow and requires lots of labeled data Transfer Learning Approach Task: Predict movie rating from review\rStep 1: Use pre-trained model (trained on 800GB of text)\rStep 2: Fine-tune on your small dataset (100 examples)\rStep 3: Train for hours/days\rStep 4: Deploy with better performance!\rBenefit: Fast and works with little data Two Forms of Transfer Learning 1. Feature-Based Transfer Learning Concept: Use pre-trained embeddings as fixed features\nExample:\nPre-training Task: Learn word2vec embeddings from Wikipedia\rStep 1: Train Word2Vec on Wikipedia\r\u0026#34;The cat sat on the mat\u0026#34;\rLearns: embedding[\u0026#34;cat\u0026#34;] = [0.2, -0.5, 0.8, ...]\rStep 2: Use these embeddings for a different task\rTask: Sentiment analysis on product reviews\rReview: \u0026#34;Great product!\u0026#34;\rEmbed: \u0026#34;Great\u0026#34; → [0.1, 0.3, 0.9, ...]\r\u0026#34;product\u0026#34; → [0.5, -0.2, 0.1, ...]\rFeed embeddings to simple classifier Pros:\nSimple to implement Fast inference (pre-computed embeddings) Works with small downstream models Cons:\nFixed embeddings don\u0026rsquo;t adapt to new domain Context not captured well (same embedding for \u0026ldquo;bank\u0026rdquo; everywhere) 2. Fine-tuning Transfer Learning Concept: Use pre-trained model weights and update them for new task\nExample:\nPre-training Task: Predict next word on Wikipedia (Language Modeling)\rStep 1: Train transformer on 800GB Wikipedia data\rModel learns: linguistic patterns, world knowledge, grammar\rStep 2: Fine-tune on downstream task\rMovie Review → Rating Prediction\rOption A: Update all layers\r├─ Last layer: Fine-tune (most task-specific)\r├─ Middle layers: Fine-tune (some task-specific)\r└─ First layers: Fine-tune (general language)\rOption B: Freeze some layers, train new ones\r├─ Layer 1-10: FROZEN (keep pre-trained weights)\r├─ Layer 11-12: Fine-tune\r└─ New classification head: Train from scratch Pros:\nWeights adapt to new task Better performance than feature-based Fast convergence Cons:\nComputational cost (need to update all parameters) Risk of catastrophic forgetting Pre-training Data vs Downstream Task The Scale Difference Pre-training: 800 GB of text\r├─ Wikipedia: 13 GB\r├─ Books: 200 GB\r├─ Web crawl: 500+ GB\r└─ Total knowledge learned: Massive\rDownstream: 100 - 10,000 examples\r├─ Movie reviews: 5,000 examples\r├─ Medical texts: 1,000 examples\r└─ Customer feedback: 500 examples Key Insight: Pre-training has 80,000x more data than typical downstream tasks! This explains why transfer learning works so well.\nPre-training Objectives Objective 1: Language Modeling Task: Predict next word given previous words\nInput: \u0026#34;The quick brown\u0026#34;\rOutput: \u0026#34;fox\u0026#34;\rLoss: Cross-entropy between predicted and actual next word\rExample:\r\u0026#34;Learning from deeplearning.ai is like watching the _____\u0026#34;\rModel predicts: \u0026#34;sunset\u0026#34;\rTarget: \u0026#34;sunset\u0026#34;\rLoss = 0 (perfect prediction!) Why it works:\nModel learns grammar, syntax, semantics Model learns common patterns Model learns world knowledge Objective 2: Masked Language Modeling Task: Predict masked (hidden) words\nInput: \u0026#34;The quick [MASK] fox\u0026#34;\rOutput: \u0026#34;brown\u0026#34;\rThis is what BERT uses!\rMore complex:\r\u0026#34;The [MASK] brown [MASK] fox [MASK]\u0026#34;\rPredict all three: \u0026#34;quick\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;here\u0026#34; Advantages of Transfer Learning 1. Reduce Training Time Classical Training: Transfer Learning:\r┌─────────────────────┐ ┌──────────┐\r│ Pre-training: 3 mo │ │ Pre-train│ (already done!)\r│ (From scratch!) │ │ (reuse) │\r│ │ └──────────┘\r│ Fine-tuning: 1 mo │ ┌──────────┐\r│ (On task data) │ │Fine-tune │ 1-7 days!\r│ │ │ (on task)│\r│ Total: 4 MONTHS │ └──────────┘\r└─────────────────────┘ Total: ~1 WEEK Speedup: 15-30x faster!\n2. Improve Predictions Small dataset (100 examples) with classical approach:\r├─ Model overfits (memorizes the 100 examples)\r├─ 60% accuracy on test set\r└─ Essentially useless\rSmall dataset (100 examples) with transfer learning:\r├─ Start from pre-trained model (knows language!)\r├─ Fine-tune carefully\r├─ 85% accuracy on test set\r└─ Much better!\rImprovement: 25% better! 3. Use Smaller Datasets Classical: \u0026#34;You need 10,000 labeled examples\u0026#34;\rTransfer: \u0026#34;100-1000 labeled examples is enough!\u0026#34;\rWhy?\rPre-trained model already knows language\rYou just need to teach it your specific task\rLess data needed Pre-training vs Fine-tuning Data Labeled vs Unlabeled Data Unlabeled Data (used in pre-training):\r\u0026#34;The quick brown fox jumps over...\u0026#34;\r(No labels needed! Just raw text)\rLabeled Data (used in fine-tuning):\r\u0026#34;The quick brown fox jumps over...\u0026#34; → Positive (label)\r(Requires manual annotation)\rInsight: Unlabeled data \u0026gt;\u0026gt; Labeled data in quantity\rPre-training can use trillions of tokens! Transfer Learning Strategy Strategy 1: Light Fine-tuning Freeze most layers, train only last layer\r├─ Use case: Large pre-trained model + small dataset\r├─ Layers frozen: 1-11\r├─ Layers trained: 12 + classification head\r├─ Training time: 1-2 days\r└─ Risk of underfitting: Low Strategy 2: Full Fine-tuning Update all layers\r├─ Use case: Medium pre-trained model + medium dataset\r├─ Layers frozen: None\r├─ Layers trained: All 12 + classification head\r├─ Training time: 3-7 days\r└─ Risk of overfitting: Medium Strategy 3: Progressive Unfreezing Unfreeze layers gradually\r├─ Day 1: Unfreeze last layer, train\r├─ Day 2: Unfreeze last 2 layers, train\r├─ Day 3: Unfreeze last 3 layers, train\r├─ ...\r├─ Training time: 7+ days\r└─ Best performance: Often! When Transfer Learning Fails ❌ Domain mismatch: Pre-training on English, fine-tune on Chinese ❌ Catastrophic forgetting: Update all weights too aggressively ❌ Too much fine-tuning: Use too high learning rate ❌ Poor feature extraction: Pre-train task too different from downstream\nHistorical Timeline 2013: Word2Vec\r└─ First successful transfer learning\r└─ Simple word embeddings\r2015: ELMo\r└─ Bidirectional LSTM\r└─ Context-aware embeddings\r2018: GPT\r└─ Transformer-based language model\r└─ Unidirectional (left-to-right)\r2018: BERT\r└─ Bidirectional transformers!\r└─ Masked language modeling\r└─ Huge performance improvements\r2019: T5\r└─ Text-to-text transfer transformer\r└─ Multi-task learning\r└─ State-of-the-art on everything!\r2020+: GPT-2, GPT-3, RoBERTa, ELECTRA, XLNet...\r└─ Scaling laws discovered\r└─ Bigger models → Better performance Key Takeaways ✅ Transfer learning is the standard: Most NLP done today uses it ✅ Pre-training is crucial: 800GB of pre-training \u0026raquo; 10KB of fine-tuning ✅ Speed advantage is massive: Days instead of months ✅ Data efficiency: Works with far less labeled data ✅ Domain adaptation: Can transfer across languages, domains, tasks\nWhat We\u0026rsquo;ll Learn Next BERT: How bidirectional context improves understanding MLM: Masked language modeling pre-training T5: Text-to-text framework for all NLP tasks Fine-tuning: Practical tricks for downstream tasks This foundation (transfer learning) is why one person can now build state-of-the-art NLP systems in weeks!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/4.1-event1/",
	"title": "Event 1 - Vietnam Cloud Day 2025",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Date \u0026amp; Time: Thursday, 18 September 2025 | 9:00 – 17:00 VNT\nLocation: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRegistration Status: Closed\nEvent Overview Vietnam Cloud Day 2025 was a comprehensive AWS event designed for builders and enterprise leaders, featuring keynote addresses from government speakers, AWS executives, and industry leaders. The event showcased AWS\u0026rsquo;s latest services and strategic initiatives for AI and cloud modernization through two main tracks: a live telecast track and in-person breakout sessions.\nAgenda Live Telecast Track Time (VNT) Session Speaker 7:35 - 9:00 Registration - 9:00 - 9:20 Opening Hon. Government Speaker 9:20 - 9:40 Keynote Address Eric Yeo, Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS 9:40 - 10:00 Customer Keynote 1 Dr. Jens Lottner, CEO, Techcombank 10:00 - 10:20 Customer Keynote 2 Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network 10:20 - 10:50 AWS Keynote Jaime Valles, Vice President, General Manager Asia Pacific and Japan, AWS 11:00 – 11:40 Panel Discussion: Navigating the GenAI Revolution Moderator: Jeff Johnson, Managing Director, ASEAN, AWS Panel Discussion Details: Navigating the GenAI Revolution: Strategies for Executive Leadership This discussion delved into how executive leaders can effectively steer their organizations through the rapid advancements in generative AI. Executive panelists shared their insights and personal journeys on:\nFostering a culture of innovation Aligning AI initiatives with business objectives Managing organizational changes that accompany AI integration Panelists:\nVu Van, Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh, Chairman, Nexttech Group Dieter Botha, CEO, TymeX Breakout Tracks (In-Person Sessions) Track 1: AI \u0026amp; Analytics Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Jun Kai Loke, AI/ML Specialist SA, AWS 13:30 - 14:00 Building a Unified Data Foundation on AWS for AI and Analytics Workloads Kien Nguyen, Solutions Architect, AWS 14:00 - 14:30 Building the Future: Gen AI Adoption and Roadmap on AWS Jun Kai Loke, AI/ML Specialist SA, AWS; Tamelly Lim, Storage Specialist SA, AWS 14:30 - 15:00 AI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation Binh Tran, Senior Solutions Architect, AWS 15:00 - 15:30 Tea Break - 15:30 - 16:00 Securing Generative AI Applications with AWS: Fundamentals and Best Practices Taiki Dang, Solutions Architect, AWS 16:00 - 16:30 Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers Michael Armentano, Principal WW GTM Specialist, AWS Session Details Building a Unified Data Foundation on AWS for AI and Analytics Workloads\nThis session delved into strategies and best practices for constructing a unified, scalable data foundation on AWS. Participants learned how to leverage AWS services to create a robust data infrastructure that can handle the demands of modern data-driven applications. Key topics covered:\nData ingestion, storage, processing, and governance Effective data management and utilization for advanced analytics and AI initiatives Building the Future: Gen AI Adoption and Roadmap on AWS\nAWS presented its comprehensive vision, emerging trends, and strategic roadmap for the adoption of Generative AI (GenAI) technologies. The discussion covered key AWS services and initiatives designed to empower organizations in leveraging GenAI to drive innovation and efficiency.\nAI-Driven Development Lifecycle (AI-DLC) - Shaping the Future of Software Implementation\nThe AI-Driven Development Lifecycle (AI-DLC) is a transformative, AI-centric approach reshaping the future of software implementation by fully embedding AI as a central collaborator in the entire software development lifecycle. Unlike traditional methods that retrofit AI as an assistant to existing human-driven processes, AI-DLC integrates AI-powered execution with human oversight and dynamic team collaboration to:\nDrastically improve software development speed Enhance code quality Foster innovation Securing Generative AI Applications with AWS: Fundamentals and Best Practices\nThis session explored the unique security challenges at each layer of the generative AI stack—infrastructure, models, and applications. Participants learned how AWS integrates built-in security measures such as:\nEncryption Zero-trust architecture Continuous monitoring Fine-grained access controls These measures safeguard generative AI workloads, ensuring data confidentiality and integrity throughout the AI lifecycle.\nBeyond Automation: AI Agents as Your Ultimate Productivity Multipliers\nThis session presented a paradigm shift where AI agents aren\u0026rsquo;t just tools, but intelligent partners actively driving business forward. Key concepts included:\nAI agents that learn, adapt, and execute complex tasks autonomously Transformation of operations from manual processes to unprecedented efficiency Exponential productivity multiplication through AI power Track 2: Cloud Migration \u0026amp; Modernization Focus Time (VNT) Session Speaker 13:15 - 13:30 Opening \u0026amp; Track Introduction Hung Nguyen Gia, Head of Solutions Architect, AWS 13:30 - 14:00 Completing a Large-Scale Migration and Modernisation with AWS Son Do, Technical Account Manager, AWS; Nguyen Van Hai, Director of Software Engineering, Techcombank 14:00 - 14:30 Modernizing Applications with Generative AI-Powered Tools Phuc Nguyen, Solutions Architect, AWS; Alex Tran, AI Director, OCB 14:30 - 15:00 Panel Discussion: Application Modernization - Accelerating Business Transformation Moderator: Hung Nguyen Gia, Head of Solutions Architect, AWS 15:00 - 15:30 Break - 15:30 - 16:00 Transforming VMware with AI-driven Cloud Modernisation Hung Hoang, Customer Solutions Manager, AWS 16:00 - 16:30 AWS Security at Scale: From Development to Production Taiki Dang, Solutions Architect, AWS Session Details Completing a Large-Scale Migration and Modernisation with AWS\nThis session focused on valuable lessons from thousands of enterprises who have migrated and modernized their on-premises workloads with AWS. Topics included:\nProven mental models and technical best practices Modernization pathways that help organizations modernize while they migrate AWS migration accelerators and latest migration and modernization tools Case study showcasing how organizations established a robust foundation and strategic roadmap leveraging AWS cloud capabilities to achieve digital transformation goals Modernizing Applications with Generative AI-Powered Tools\nThis session explored how Amazon Q Developer transforms the software development lifecycle (SDLC) through its agentic capabilities across:\nAWS Console IDE CLI DevSecOps platforms Key capabilities demonstrated:\nQ\u0026rsquo;s agents accelerate code generation and improve code quality Seamless integration with existing workflows Automatic generation of comprehensive documentation and unit tests Improved code maintainability and reliability Understanding of complex codebases and optimization suggestions Automation of routine tasks across the development lifecycle Panel Discussion: Application Modernization - Accelerating Business Transformation\nPanelists:\nNguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight Transforming VMware with AI-driven Cloud Modernisation\nThis session showed how Vietnamese organizations are accelerating cloud adoption with VMware estates. Key topics:\nHow AWS Transform helps migrate fast, safely, and cost-effectively Step-by-step playbook and downtime-aware patterns Roadmap to modernize onto EKS, RDS, and serverless after landing Ideal for IT leaders, architects, and ops teams planning large-scale VMware-to-AWS moves AWS Security at Scale: From Development to Production\nThis session explored how to enhance cloud security posture across the entire development and production lifecycle. Topics covered:\nAWS comprehensive security approach: identification, prevention, detection, response, and remediation Security-by-design principles throughout the development process Advanced detection and response capabilities How generative AI enhances security analysis and automates operations Building resilient architectures that evolve with emerging threats Creating more secure, scalable cloud environments Key Takeaways Comprehensive understanding of AWS\u0026rsquo;s AI and cloud modernization strategy Practical insights into enterprise-scale AI adoption and implementation Best practices for data foundation, security, and application modernization Real-world case studies and lessons from industry leaders Hands-on knowledge of AWS services for GenAI, migration, and modernization "
},
{
	"uri": "http://localhost:1313/hugo_aws/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Lê Trọng Nhân\nPhone Number: 0397609967\nEmail: nhanle221199@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: SE190525\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/",
	"title": "Week 1 - Cloud Computing Fundamentals",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-08 to 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 1 Overview This week covered cloud computing fundamentals, AWS global infrastructure, and the core management tooling offered by AWS.\nKey Topics Introduction to Cloud Computing and its benefits AWS Global Infrastructure (Regions, AZs, Edge Locations) AWS Management Tools (Console, CLI, SDK) Cost Optimization approaches and Support Plans AWS Well-Architected Framework pillars Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/",
	"title": "Worklog - AWS Learning Journey",
	"tags": [],
	"description": "",
	"content": "Worklog Overview This worklog documents the AWS learning journey starting from September 8, 2025.\nStructure The worklog is organized by weeks, with each week consisting of five working days (Monday to Friday).\nMain Topics Cloud Computing Fundamentals\nAWS basics, global infrastructure, and management tools Cost optimization strategies and support plans The AWS Well-Architected Framework Networking\nVPC, subnets, security groups, and NACLs Load balancing (ALB, NLB, GWLB) VPC Peering and Transit Gateway VPN and Direct Connect Compute\nEC2, AMI, EBS, Instance Store Auto Scaling and pricing models Lightsail, EFS, FSx Storage\nS3, storage classes, Glacier Snow Family and Storage Gateway Disaster Recovery and AWS Backup Security \u0026amp; Identity\nIAM, Cognito, and AWS Organizations KMS and Security Hub Identity Center (SSO) Database\nRDS, Aurora, Redshift ElastiCache and DMS Database best practices Advanced Topics\nServerless (Lambda) Containers (ECS, EKS, ECR) Monitoring (CloudWatch, X-Ray, CloudTrail) "
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.2-blog2/",
	"title": "Announcing Amazon EC2 M4 and M4 Pro Mac Instances",
	"tags": [],
	"description": "",
	"content": "Announcing Amazon EC2 M4 and M4 Pro Mac Instances by Sébastien Stormacq — September 12, 2025 · Amazon EC2 Mac Instances · Launch · News · Permalink · Comments\nPermalink Comments Share · Voiced by Polly\nI have used macOS since 2001 and Amazon EC2 Mac instances since they launched four years ago. In that time I have helped many customers scale their continuous integration and continuous delivery (CI/CD) pipelines on AWS. Today I’m thrilled to share that the M4 and M4 Pro Mac instances are now generally available.\nTeams that build for Apple platforms need powerful compute to handle complex build pipelines and run multiple iOS simulators simultaneously. As projects grow in size and sophistication, they need more performance and memory to keep their development cycles fast.\nApple M4 Mac mini at the core EC2 M4 Mac instances (mac-m4.metal in the API) are based on the Apple M4 Mac mini and run on the AWS Nitro System. They include an Apple silicon M4 with a 10-core CPU (four performance cores and six efficiency cores), a 10-core GPU, a 16-core Neural Engine, and 24 GB of unified memory, delivering up to 20% faster iOS and macOS build performance than EC2 M2 Mac instances.\nEC2 M4 Pro Mac instances (mac-m4pro.metal) use the Apple silicon M4 Pro with a 14-core CPU, 20-core GPU, 16-core Neural Engine, and 48 GB of unified memory. They deliver up to 15% faster build times compared to EC2 M2 Pro Mac instances, and the additional memory and compute let you run more parallel test suites across multiple simulators.\nEach M4 and M4 Pro Mac instance now ships with 2 TB of local NVMe storage, providing low-latency storage to boost caching plus build-and-test performance.\nBoth instance types support macOS Sonoma 15.6 and later as Amazon Machine Images (AMIs). AWS Nitro provides up to 10 Gbps of Amazon VPC network bandwidth and up to 8 Gbps of Amazon EBS bandwidth via high-speed Thunderbolt connections.\nEC2 Mac instances integrate with the rest of AWS, so you can:\nBuild automated CI/CD pipelines with AWS CodeBuild and AWS CodePipeline. Store and manage build secrets—such as Apple developer certificates and keys—in AWS Secrets Manager. Manage your development infrastructure with AWS CloudFormation. Monitor instance performance by using Amazon CloudWatch. How to get started Launch an EC2 M4 or M4 Pro Mac instance with the AWS Management Console, the AWS Command Line Interface (AWS CLI), or an AWS SDK.\nIn this walkthrough I start an M4 Pro instance from the console. First I allocate a dedicated host. In the AWS Management Console, I open EC2, choose Dedicated Hosts, and select Allocate Dedicated Host.\nNext I add a tag, select the instance family mac-m4pro, and pick the mac-m4pro.metal instance type. I choose an Availability Zone and clear Host maintenance.\nThe console also shows the most recent macOS versions supported on this host family—in this case macOS 15.6.\nOn the Launch an instance page I enter a name, pick a macOS Sequoia AMI, make sure the architecture is 64-bit Arm, and select the mac-m4pro.metal instance type.\nThe remaining parameters are the usual EC2 settings for networking and storage. For development you should provision at least 200 GB; the default 100-GB volume is not enough to download and install Xcode.\nWhen I’m ready, I choose the orange Launch instance button. The instance quickly appears in the Running state, although it can take up to 15 minutes before SSH becomes available.\nYou can also use the CLI:\naws ec2 run-instances \\ --image-id \u0026#34;ami-000420887c24e4ac8\u0026#34; \\ # AMI ID varies by Region --instance-type \u0026#34;mac-m4pro.metal\u0026#34; \\ --key-name \u0026#34;my-ssh-key-name\u0026#34; \\ --network-interfaces \u0026#39;{\u0026#34;AssociatePublicIpAddress\u0026#34;:true,\u0026#34;DeviceIndex\u0026#34;:0,\u0026#34;Groups\u0026#34;:[\u0026#34;sg-0c2f1a3e01b84f3a3\u0026#34;]}\u0026#39; \\ # use your security group --tag-specifications \u0026#39;{\u0026#34;ResourceType\u0026#34;:\u0026#34;instance\u0026#34;,\u0026#34;Tags\u0026#34;:[{\u0026#34;Key\u0026#34;:\u0026#34;Name\u0026#34;,\u0026#34;Value\u0026#34;:\u0026#34;My Dev Server\u0026#34;}]}\u0026#39; \\ --placement \u0026#39;{\u0026#34;HostId\u0026#34;:\u0026#34;h-0e984064522b4b60b\u0026#34;,\u0026#34;Tenancy\u0026#34;:\u0026#34;host\u0026#34;}\u0026#39; \\ # supply your host ID --private-dns-name-options \u0026#39;{\u0026#34;HostnameType\u0026#34;:\u0026#34;ip-name\u0026#34;,\u0026#34;EnableResourceNameDnsARecord\u0026#34;:true,\u0026#34;EnableResourceNameDnsAAAARecord\u0026#34;:false}\u0026#39; \\ --count 1 Install Xcode from the terminal After the instance becomes reachable, connect via SSH and install your development tooling. I use xcodeinstall to download and install Xcode 16.4.\nFrom my laptop I start a session that has permission to read AWS Secrets Manager and authenticate with the Apple Developer portal:\nxcodeinstall authenticate -s eu-central-1 Retrieving Apple Developer Portal credentials...\rAuthenticating...\r🔐 Two-factor authentication is enabled, enter your 2FA code: 067785\r✅ Authenticated with MFA. Then I connect to the EC2 Mac instance I just launched and download/install Xcode:\nssh ec2-user@44.234.115.119 Warning: Permanently added \u0026#39;44.234.115.119\u0026#39; (ED25519) to the list of known hosts.\rLast login: Sat Aug 23 13:49:55 2025 from 81.49.207.77\r┌───┬──┐ __| __|_ )\r│ ╷╭╯╷ │ _| ( /\r│ └╮ │ ___|\\___|___|\r│ ╰─┼╯ │ Amazon EC2\r└───┴──┘ macOS Sequoia 15.6 brew tap sebsto/macos\rbrew install xcodeinstall\rxcodeinstall download -s eu-central-1 -f -n \u0026#34;Xcode 16.4.xip\u0026#34;\rxcodeinstall install -n \u0026#34;Xcode 16.4.xip\u0026#34;\rsudo xcodebuild -license accept Things to keep in mind Provision at least a 200-GB EBS volume for development (I usually pick 500 GB). If you resize the volume after launch, remember to resize the APFS filesystem. You can also install your tools on the 2-TB internal SSD available in the Mac mini for even lower latency. Note that the contents of this drive are tied to the instance lifecycle, not to the dedicated host, so it is wiped when you stop and start the instance. The mac-m4.metal and mac-m4pro.metal instance types support macOS Sequoia 15.6 and later. You can migrate existing EC2 Mac instances that already run macOS 15 (Sequoia). Create a custom AMI from the current instance and launch an M4 or M4 Pro Mac from that AMI. Finally, here are three guides I wrote to help you get started with EC2 Mac:\nStart an EC2 Mac instance Connect to an EC2 Mac instance (three different connection methods) Build iOS apps faster with a CI/CD pipeline on EC2 Mac Pricing and availability EC2 M4 and M4 Pro Mac instances are available in US East (N. Virginia) and US West (Oregon) today, with additional Regions to follow.\nYou purchase EC2 Mac instances as Dedicated Hosts with On-Demand pricing or Savings Plans. Billing is per second with a 24-hour allocation minimum, in accordance with Apple’s macOS Software License Agreement. After the 24-hour minimum you can release the host at any time with no further commitment.\nAs someone who works closely with Apple developers, I can’t wait to see how you use these new instances to accelerate your build cycles. The combination of higher performance, more memory, and deep AWS integration unlocks fresh possibilities for iOS, macOS, iPadOS, tvOS, watchOS, and visionOS teams. Beyond app development, the Apple Neural Engine makes these instances an efficient option for machine learning (ML) inference workloads. I will cover this topic in more detail at AWS re:Invent 2025, where I’ll share benchmarks and best practices for running ML inference on EC2 Mac.\nAbout the author Sébastien Stormacq\rSeb has been writing code since he first touched a Commodore 64 in the mid-eighties. He inspires builders to unlock the value of the AWS Cloud through a mix of passion, enthusiasm, customer advocacy, curiosity, and creativity. His interests include software architecture, developer tools, and mobile computing. If you want to sell him something, make sure it has an API. Follow @sebsto on Bluesky, X, Mastodon, and other networks.\r"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/",
	"title": "Day 02 - AWS Global Infrastructure",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-09 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Infrastructure Data Centers Each data center can host tens of thousands of servers. AWS builds and manages its own custom hardware for efficiency and reliability. Availability Zone (AZ) One or more physically separate data centers within a Region. Each AZ is designed for fault isolation. Connected via low-latency, high-throughput private links. AWS recommends deploying workloads across at least two AZs. Region A Region contains at least three Availability Zones. There are currently 25+ Regions worldwide. Regions are interconnected by the AWS backbone network. Most services are Region-scoped by default. Edge Locations Global network of edge sites designed to serve content with minimal latency. Used by services such as: Amazon CloudFront (CDN) AWS WAF (Web Application Firewall) Amazon Route 53 (DNS Service) Hands-On Labs Lab 01 – AWS Account \u0026amp; IAM Setup Create an AWS Account → 01-01 Configure Virtual MFA Device → 01-02 Create Admin Group and Admin User → 01-03 Account Authentication Support → 01-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/",
	"title": "Day 07 - VPC Routing &amp; Network Interfaces",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-16 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Routing \u0026amp; ENI Route Tables A route table defines how traffic is directed. Each VPC has a default route table containing only a local route allowing internal communication between subnets. Custom route tables can be created, but the local route cannot be deleted. Elastic Network Interface (ENI) An ENI is a virtual network card that can be moved between EC2 instances. It retains its private IP, Elastic IP address, and MAC address when reassigned. Elastic IP (EIP) is a static public IPv4 address that can be associated with an ENI. Unused EIPs incur charges. ENI Use Cases:\nManagement network separate from data network Network and security appliances Dual-homed instances with workloads on distinct subnets Low-budget, high-availability solution VPC Endpoints A VPC Endpoint enables private connectivity to supported AWS services via AWS PrivateLink without using the public Internet. Two types: Interface Endpoint: Uses an ENI with a private IP. Gateway Endpoint: Uses route tables (available for S3 and DynamoDB only). Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Create Internet Gateway (IGW) → 03-03.3 Create Route Table (Outbound via IGW) → 03-03.4 Create Security Groups → 03-03.5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/",
	"title": "Day 12 - EC2 Storage &amp; Backup",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-23 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Storage \u0026amp; Security Backup in EC2 AWS Backup provides centralized backup for AWS services including EC2. EBS Snapshots back up EBS volumes: Point-in-time backups Incremental (stores only changed blocks) Stored in S3 (not directly accessible) AMI Backup captures the full EC2 configuration as an image. Snapshot Best Practices:\nSchedule regular snapshots Copy snapshots to other regions for DR Tag snapshots for lifecycle management Use Amazon Data Lifecycle Manager (DLM) Key Pair Key Pairs are used for secure authentication when connecting to EC2: Public Key – stored on the instance Private Key – kept by the user for SSH (Linux) or RDP (Windows) Replaces passwords for better security. Important: If you lose your private key, AWS cannot recover it. Key Pair Management:\nCreate key pairs in AWS or import your own Store private keys securely Use different key pairs for different environments Rotate keys regularly Elastic Block Store (EBS) Amazon EBS provides persistent block storage for EC2 instances. Volume types: General Purpose SSD (gp2/gp3) – balance between performance \u0026amp; cost Provisioned IOPS SSD (io1/io2) – for high IOPS workloads Throughput Optimized HDD (st1) – for large, sequential data Cold HDD (sc1) – low-cost, infrequently accessed data Key Features\nAttach/detach volumes from instances Data persists when instances stop Create snapshots for backup or cross-region copy Automatically replicated within an AZ EBS Volume Comparison:\nType Use Case Max IOPS Max Throughput gp3 General purpose 16,000 1,000 MB/s io2 High performance 64,000 1,000 MB/s st1 Big data 500 500 MB/s sc1 Cold storage 250 250 MB/s "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/",
	"title": "Day 17 - S3 Advanced Features",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-30 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon S3 Static Website Hosting Host static websites (HTML, CSS, JS, images) directly from S3.\nKey Capabilities Simple setup: A few steps to enable static website hosting on a bucket. Low cost: Pay standard S3 storage and data transfer; no separate web server charges. Elastic scaling: Automatically handles traffic spikes. CDN integration: Easily front with Amazon CloudFront for global performance. Static Website Configuration:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS allows web resources (fonts, JavaScript, etc.) on one domain to request resources from another domain.\nConfiguring CORS on S3 Define policies: Specify which origins are permitted to access a bucket\u0026rsquo;s content. Control methods: Allow specific HTTP methods (GET, PUT, POST, etc.). Security posture: Prevent unauthorized cross-origin access. CORS Configuration Example:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Performance and Object Key Design Object key naming can significantly affect S3 performance:\nRandomized prefixes: Distribute keys across partitions for higher parallelism. Avoid sequential prefixes: Don\u0026rsquo;t use monotonically increasing prefixes (e.g., timestamps) for high-throughput workloads. Parallel access: Structure keys to enable concurrent reads/writes. Key Design Best Practices:\n❌ Bad: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg\r✅ Good: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier – Long-Term Archival S3 Glacier classes are optimized for ultra–low-cost long-term storage.\nRetrieval Options Expedited / Fast: Minutes; highest cost. Standard: 3–5 hours; balanced cost. Bulk: 5–12 hours; lowest cost for large restores. Glacier Deep Archive The lowest-cost class for multi-year retention, with ~12-hour retrieval times.\nHands-On Labs Lab 57 – Amazon S3 \u0026amp; CloudFront (Part 2) Configure Public Objects → 57-5 Test Website → 57-6 Block All Public Access → 57-7.1 Configure CloudFront → 57-7.2 Test CloudFront → 57-7.3 Bucket Versioning → 57-8 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/",
	"title": "Day 22 - IAM Policies &amp; Roles",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-07 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes IAM Policies JSON documents defining permissions. Types: Identity-based policies (attached to principals) Resource-based policies (attached to resources) Evaluation rule: explicit Deny overrides Allow across all policies. Pattern to constrain S3 administration:\nAllow all s3:* actions on a specific bucket. Explicitly Deny all non-S3 actions. Policy Structure:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Policy Evaluation Logic:\nBy default, all requests are denied Explicit allow overrides default deny Explicit deny overrides any allows Permissions boundaries limit maximum permissions IAM Roles Roles provide temporary permissions assumed by users, services, or external identities. Common use cases: Let an AWS service act on your behalf (e.g., EC2 → S3 writes) Cross-account access Federation from external IdPs Credentials for apps on EC2 without storing access keys Benefits\nNo long-term credentials, short-lived sessions, least privilege, and easier large-scale access management. Role Types:\nService Role: For AWS services (EC2, Lambda, etc.) Cross-Account Role: Access resources in another account Identity Provider Role: For federated users Instance Profile: Container for EC2 instance role Hands-On Labs Lab 48 – IAM Access Keys \u0026amp; Roles (Part 2) Use Access Key → 48-2.2 Create IAM Role → 48-3.1 Use IAM Role → 48-3.2 Clean Up Resources → 48-4 Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 1) Create IAM User → 28-2.1 Create IAM Policy → 28-3 Create IAM Role → 28-4 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/",
	"title": "Day 27 - Amazon RDS &amp; Aurora",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-14 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes RDBMS vs NoSQL RDBMS An RDBMS stores data in related tables (rows/columns), enforces integrity constraints, uses SQL, and provides ACID guarantees. Popular engines: Oracle, MySQL, SQL Server, PostgreSQL, IBM Db2. NoSQL Overview NoSQL systems target un/semistructured data with high scalability and performance. Types: Document (MongoDB, CouchDB) Key–Value (Redis, DynamoDB) Column-Family (Cassandra, HBase) Graph (Neo4j, Amazon Neptune) Traits: schema flexibility, horizontal scaling, big-data friendliness, CAP-oriented designs. RDBMS vs. NoSQL (high-level) OLTP vs. OLAP OLTP: many small, concurrent transactions; normalized data; short queries; index-heavy. OLAP: complex analytics over large historical datasets; star/snowflake schemas; read-heavy. Amazon RDS \u0026amp; Aurora Amazon Relational Database Service (RDS) Managed relational databases that simplify provisioning, patching, backups, and HA.\nSupported engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Amazon Aurora. Key features: automated backups/patching, easy scaling, Multi-AZ high availability, encryption \u0026amp; VPC/IAM/SSL security. Deployment options: Single-AZ Multi-AZ (synchronous standby in another AZ) Read Replicas for scaling reads RDS Features:\nAutomated Backups: Point-in-time recovery up to 35 days Manual Snapshots: User-initiated backups Multi-AZ: Automatic failover for high availability Read Replicas: Scale read workloads (up to 15 replicas) Parameter Groups: Database configuration management Option Groups: Additional features (e.g., Oracle Advanced Security) Amazon Aurora Cloud-native, MySQL/PostgreSQL-compatible relational database re-architected for AWS.\nHighlights: Up to ~5× MySQL / ~3× PostgreSQL performance (typical benchmarks) Storage auto-scales to 128 TB Six-way replication across three AZs; self-healing storage Aurora Serverless (on-demand capacity) Global Database for low-latency multi-region Aurora Features:\nAurora Replicas: Up to 15 read replicas with sub-10ms lag Aurora Serverless: Auto-scaling compute capacity Aurora Global Database: Cross-region replication \u0026lt; 1 second Aurora Backtrack: Rewind database to specific point in time Aurora Parallel Query: Faster analytics on current data Aurora Machine Learning: Native ML integration Aurora vs RDS:\nFeature Aurora RDS Performance 5x MySQL, 3x PostgreSQL Standard Storage Auto-scaling to 128 TB Manual scaling Replicas Up to 15 Up to 5 (MySQL) Failover \u0026lt; 30 seconds 1-2 minutes Backtrack Yes No Hands-On Labs Lab 05 – Amazon RDS \u0026amp; EC2 Integration (Part 2) Create EC2 Instance → 05-3 Create RDS Database Instance → 05-4 Application Deployment → 05-5 Backup and Restore → 05-6 Clean Up Resources → 05-7 Lab 43 – AWS Database Migration Service (DMS) (Part 1) EC2 Connect RDP Client → 43-01 EC2 Connect Fleet Manager → 43-02 SQL Server Source Config → 43-03 Oracle Connect Source DB → 43-04 Oracle Config Source DB → 43-05 Drop Constraint → 43-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/",
	"title": "Day 32 - Contract-First &amp; Mocking",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-21 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract-First Development 5-Step Workflow Write the OpenAPI spec to define the contract. Share the spec as the single source of truth for both frontend and backend. Let the frontend build UI against mock data derived from the contract. Implement the backend API to match the schema (status codes, payloads). Run contract testing to confirm the backend adheres to the spec. paths: /books/{id}: get: summary: Get book detail responses: \u0026#34;200\u0026#34;: $ref: \u0026#34;#/components/responses/BookDetail\u0026#34; Benefits Minimizes API mismatches because everyone references the same spec. Documentation, mock servers, and test scripts can be generated automatically. Makes it easy to review and version the API before real deployment. Insight Defining the contract before coding cuts ~80% of integration issues when teams work in parallel.\nMock APIs with Prism Prism reads OpenAPI specs to return mock responses so the frontend can test UI early. Multiple scenarios (200, 404, 500) are supported by attaching examples to the spec. Keeps delivery on track even when the backend is unfinished or in refactor mode. When to Use It The first sprint of a vertical slice. Need to showcase a flow without production data yet. Want automated UI tests based on the contract. Operational Notes Run Prism on localhost:4010 and point Next.js to the mock via NEXT_PUBLIC_API_URL. Mirror production CORS headers in the mock responses. Always commit the spec before mocking so everyone uses the same version. Hands-On Labs Define the OpenAPI spec for the /books/{id} endpoint. Launch the Prism mock server and verify the UI flow. Write a contract review checklist (status codes, schema, sample data). "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/1.8.2-day37-2025-10-28/",
	"title": "Day 37 - Voice Search &amp; Chatbot Architecture",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-28 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nVoice Search (How Siri Works) Voice search systems follow a pipeline from speech input to actionable response:\nPipeline Components: 1. Analog to Digital Conversion Speech (utterance) → Sound wave pattern → Spectrogram (frequency pattern) → Sequence of acoustic frames using Fast Fourier Transform (FFT)\n2. Automatic Speech Recognition (ASR) Feature analysis: Extract acoustic features Hidden Markov Model (HMM): Pattern recognition for speech-to-text Viterbi algorithm: Find most likely sequence of hidden states Phonetic dictionary: Map sounds to words Language model: Ensure grammatical correctness 3. NLP Annotation Tokenization POS tagging Named Entity Recognition (NER) 4. Pattern-Action Mappings Map recognized intents to appropriate actions\n5. Service Manager Internal \u0026amp; external APIs (email, SMS, maps, weather, stocks, etc.) Execute the requested action 6. Text-to-Speech (TTS) Convert response back to speech\n7. User Feedback System learns from corrections to improve accuracy\nVoicebot Architecture The voicebot processing pipeline consists of multiple linguistic levels:\nProcessing Layers: Speech Analysis (Phonology) Recognize and transcribe speech using Automatic Speech Recognition (ASR)\nMorphological and Lexical Analysis (Morphology) Analyze word structure and meaning using morphological rules and lexicon\nParsing (Syntax) Understand sentence structure using lexicon and grammar rules\nContextual Reasoning (Semantics) Understand meaning in context using discourse context\nApplication Reasoning and Execution (Reasoning) Use domain knowledge to decide actions\nUtterance Planning Plan what to say in response\nSyntactic Realization Generate grammatically correct sentences\nMorphological Realization Apply correct word forms\nPronunciation Model Generate proper pronunciation\nSpeech Synthesis Convert text back to speech\nChatbot Workflow Step-by-Step Process: 1. User → Chat Client User types: \u0026ldquo;I want to check my account balance.\u0026rdquo; Chat Client = interface where user types (web, app, messenger)\n2. Chat Client → Chatbot Message sent to chatbot system\n3. Chatbot → NLP Engine Chatbot sends message to NLP Engine for analysis\nNLP Engine performs two main tasks: (a) Intent Detection Determine what the user wants to do\nExample: check_balance (b) Entity Extraction Extract important data from the sentence\nExample: account = checking/savings? 4. NLP Engine → Business Logic / Data Services Based on intent, chatbot calls the appropriate service:\nQuery database Call API Execute business rules Process backend logic Example: Call API to get balance from banking system\n5. Data Services → Chatbot Backend returns result:\n\u0026ldquo;Your account balance is $12,500.00\u0026rdquo;\n6. Chatbot → Chat Client Chatbot packages information into natural language response\n7. Display to User User sees the response\nChatbot = Listening + Chatting Listening (NLP - Understanding) Intent recognition Entity extraction Context understanding Chatting (NLG - Generation) Natural language generation Response formulation Personalization Behind the Scenes: Knowledge-based data: Facts, rules, FAQs Machine learning: Learning from interactions Business logic: Application-specific rules Important Distinction: Keyword vs Entity Keywords = words that indicate topics or subjects Entities = specific data points with types and values\nExample: \u0026ldquo;Book a flight to Paris on Friday\u0026rdquo;\nKeywords: book, flight Entities: destination = \u0026ldquo;Paris\u0026rdquo; (LOCATION) date = \u0026ldquo;Friday\u0026rdquo; (DATE) Not all keywords are entities, but all entities are extracted from keywords!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/1.9.2-day42-2025-11-04/",
	"title": "Day 42 - Transformer Architecture Overview",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-04 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nTransformer Architecture: The Big Picture The transformer model introduced in the paper \u0026ldquo;Attention is All You Need\u0026rdquo; revolutionized NLP. Let\u0026rsquo;s understand its complete structure.\nHigh-Level Architecture INPUT SEQUENCE\r↓\r[Tokenization \u0026amp; Embedding]\r↓\r[Add Positional Encoding]\r↓\r┌─────────────────────────────────┐\r│ ENCODER (N layers) │\r│ ├─ Multi-Head Attention │\r│ ├─ Layer Normalization │\r│ ├─ Feed-Forward Network │\r│ └─ Residual Connections │\r└─────────────────────────────────┘\r↓\r[Context Vectors from Encoder]\r↓\r┌─────────────────────────────────┐\r│ DECODER (N layers) │\r│ ├─ Masked Multi-Head Attention │\r│ ├─ Encoder-Decoder Attention │\r│ ├─ Feed-Forward Network │\r│ └─ Layer Normalization │\r└─────────────────────────────────┘\r↓\r[Linear Layer + Softmax]\r↓\rOUTPUT PROBABILITIES Component 1: Word Embeddings Each word is converted to a dense vector (typically 512-1024 dimensions).\nExample:\nWord: \u0026#34;happy\u0026#34;\rEmbedding: [0.2, -0.5, 0.8, ..., 0.1] // 512 values Component 2: Positional Encoding Problem: Transformers don\u0026rsquo;t have sequential order built in (unlike RNNs). So we must add positional information explicitly.\nSolution: Add positional encoding vectors to embeddings.\nFormula:\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\rPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\rWhere:\r- pos = position in sequence (0, 1, 2, ...)\r- i = dimension index\r- d_model = embedding dimension (512, 1024, etc.) Intuition:\nPosition 0: \u0026ldquo;I\u0026rdquo; gets PE₀ Position 1: \u0026ldquo;am\u0026rdquo; gets PE₁ Position 2: \u0026ldquo;happy\u0026rdquo; gets PE₂ Example:\nEmbedding(\u0026#34;I\u0026#34;) = [0.2, -0.5, 0.8, ..., 0.1]\rPE(pos=0) = [0.0, 1.0, 0.0, 1.0, ..., 0.5]\rFinal = [0.2, 0.5, 0.8, 1.0, ..., 0.6] Component 3: Multi-Head Attention Instead of one attention mechanism, we have h different \u0026ldquo;heads\u0026rdquo; running in parallel.\nConcept:\nInput: Query (Q), Key (K), Value (V) matrices\rHead 1: ScaledDotProductAttention(Q₁, K₁, V₁)\rHead 2: ScaledDotProductAttention(Q₂, K₂, V₂)\r...\rHead h: ScaledDotProductAttention(Qₕ, Kₕ, Vₕ)\rOutput = Concatenate(Head₁, Head₂, ..., Headₕ) Why Multiple Heads?\nHead 1 might learn \u0026ldquo;subject-verb\u0026rdquo; relationships Head 2 might learn \u0026ldquo;adjective-noun\u0026rdquo; relationships Head 3 might learn \u0026ldquo;pronoun-reference\u0026rdquo; relationships Together: Rich contextual understanding Typical Configuration:\nNumber of heads: 8-16 Dimension per head: 64 (if total = 512, then 512/8 = 64) Component 4: Residual Connections \u0026amp; Layer Normalization Residual Connections Output = Input + Attention(Input) This helps with gradient flow during training and allows networks to go deeper.\nLayer Normalization Normalized = (x - mean) / sqrt(variance + epsilon) Stabilizes training and speeds up convergence.\nComponent 5: Feed-Forward Network After attention, there\u0026rsquo;s a simple 2-layer feed-forward network:\nOutput = ReLU(Linear₁(x)) → Linear₂ Typical dimensions:\nInput: [batch_size, seq_length, 512]\r↓ Linear₁ (512 → 2048)\r[batch_size, seq_length, 2048]\r↓ ReLU (non-linear)\r[batch_size, seq_length, 2048]\r↓ Linear₂ (2048 → 512)\r[batch_size, seq_length, 512] This expands then contracts, allowing for non-linear transformations.\nThe Encoder: Detailed View Single Encoder Layer:\nInput (x)\r↓\r[Multi-Head Self-Attention]\r↓\r[+ Residual Connection with input]\r↓\r[Layer Normalization]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual Connection]\r↓\r[Layer Normalization]\r↓\rOutput Key Point: In encoder, each word attends to ALL words (including itself) in the same sentence.\nEncoder gives: Contextual representation of each word, considering all other words.\nThe Decoder: Detailed View Decoder is similar but with masking:\nInput (shifted right by 1)\r↓\r[Masked Multi-Head Self-Attention] ← Can only attend to previous positions\r↓\r[+ Residual + LayerNorm]\r↓\r[Encoder-Decoder Attention] ← Attends to encoder output\r↓\r[+ Residual + LayerNorm]\r↓\r[Feed-Forward Network]\r↓\r[+ Residual + LayerNorm]\r↓\rOutput Three Attention Mechanisms in Decoder:\nMasked Self-Attention:\nQueries, Keys, Values from decoder Each position can only attend to previous positions Prevents information leakage (decoder doesn\u0026rsquo;t see future words) Encoder-Decoder Attention:\nQueries from decoder Keys, Values from encoder Decoder can attend to any encoder position Feed-Forward:\nSame 2-layer network as encoder Putting It Together: Full Transformer Training Phase Input: \u0026#34;Je suis heureux\u0026#34; (French)\rTarget: \u0026#34;I am happy\u0026#34; (English)\rEncoder Input:\r- Tokenize: [Je, suis, heureux]\r- Embed each token\r- Add positional encoding\r- Process through N encoder layers\r→ Output: C (context vectors)\rDecoder Input:\r- Target shifted right: [\u0026lt;START\u0026gt;, I, am]\r- Embed each token\r- Add positional encoding\r- Process through N decoder layers\r- Using masked self-attention\r- Using encoder-decoder attention on C\r→ Output logits for each position\rLoss: Compare predicted \u0026#34;am happy\u0026#34; with actual \u0026#34;am happy\u0026#34;\rBackprop: Update all weights Inference Phase Encoder Input: [Je, suis, heureux]\r→ Output: C (context vectors)\rDecoder:\rStep 1: Start with [\u0026lt;START\u0026gt;]\rPredict next word: \u0026#34;I\u0026#34;\rStep 2: [\u0026lt;START\u0026gt;, I] Predict next word: \u0026#34;am\u0026#34;\rStep 3: [\u0026lt;START\u0026gt;, I, am]\rPredict next word: \u0026#34;happy\u0026#34;\rStep 4: [\u0026lt;START\u0026gt;, I, am, happy]\rPredict next word: \u0026lt;END\u0026gt;\rOutput: \u0026#34;I am happy\u0026#34; Summary: Why This Architecture Works Feature Benefit No RNN Fully parallelizable - train on GPUs efficiently Self-Attention in Encoder Each word gets context from ALL other words Masked Attention in Decoder Can\u0026rsquo;t see future - trains with autoregressive generation Positional Encoding Preserves word order without RNN sequential processing Multi-Head Attention Learn multiple types of relationships simultaneously Residual Connections Gradient flow - enables training deep networks Layer Normalization Stability - faster convergence Key Innovations Parallelization: O(1) depth instead of O(n) for RNNs Long-range Dependencies: Attention can directly connect any two positions Scalability: Can increase model size with predictable improvements Transfer Learning: Pre-trained transformers (BERT, GPT) work across tasks Next: We dive into the mathematical details of attention mechanisms!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/1.10.2-day47-2025-11-11/",
	"title": "Day 47 - BERT Architecture",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-11 (Tuesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nBERT: Bidirectional Encoder Representations from Transformers BERT was the breakthrough that changed NLP forever. Published by Google in 2018, it showed that bidirectional context matters more than anyone thought.\nWhat\u0026rsquo;s Different About BERT? The Evolution Word2Vec (2013)\r├─ Embedding: \u0026#34;cat\u0026#34; → [0.2, -0.5, 0.8, ...]\r├─ Problem: Same embedding everywhere, no context\r└─ Approach: Simple, static\r↓\rELMo (2015)\r├─ BiLSTM: Process left AND right\r├─ Problem: Still sequential, slower\r└─ Approach: Bidirectional but not parallel\r↓\rGPT (2018)\r├─ Transformer: Parallel attention!\r├─ Problem: Only looks LEFT (left-to-right)\r│ \u0026#34;The bank was robbed\u0026#34; → Can\u0026#39;t use \u0026#34;robbed\u0026#34; to understand \u0026#34;bank\u0026#34;\r└─ Approach: Unidirectional, limited\r↓\rBERT (2018) ⭐\r├─ Transformer: Parallel AND bidirectional!\r├─ Solution: Sees BOTH sides of each word\r│ \u0026#34;The bank was robbed\u0026#34; → Uses both \u0026#34;The\u0026#34; and \u0026#34;was robbed\u0026#34; to understand \u0026#34;bank\u0026#34;\r└─ Approach: Full bidirectional context BERT Architecture Overview Model Specs BERT-base:\r├─ Transformer layers: 12\r├─ Hidden size: 768\r├─ Attention heads: 12 (768/12 = 64 per head)\r├─ Feed-forward size: 3072\r├─ Total parameters: 110 Million\r├─ Training time: 4 days on 16 TPUs\r└─ Training data: 3.3 billion word pieces from Wikipedia + BookCorpus\rBERT-large:\r├─ Transformer layers: 24\r├─ Hidden size: 1024\r├─ Total parameters: 340 Million\r└─ Much slower but higher performance Encoder-Only Architecture BERT vs Transformer:\rTransformer (Full):\rInput → Encoder (6 layers) → Decoder (6 layers) → Output\r(Can see all) (Masked, can\u0026#39;t see future)\rBERT (Encoder Only):\rInput → Encoder (12 layers) → Output\r(Full bidirectional attention!)\rKey Difference:\r├─ GPT = Encoder + Decoder but decoder is unidirectional\r├─ BERT = Encoder only, fully bidirectional\r└─ T5 = Encoder + Decoder, both bidirectional The Self-Attention Mechanism (Review) Remember from Day 43:\nFor each word, create three vectors:\r├─ Query (Q): \u0026#34;What information do I need?\u0026#34;\r├─ Key (K): \u0026#34;What information do I provide?\u0026#34;\r└─ Value (V): \u0026#34;What\u0026#39;s my actual representation?\u0026#34;\rAttention Score = softmax(Q·K^T / √d) · V\rExample: Processing \u0026#34;bank\u0026#34; in \u0026#34;The bank was robbed\u0026#34;\rQuery for \u0026#34;bank\u0026#34;: [0.1, 0.2, 0.3, ...]\rKeys for all words:\r├─ \u0026#34;The\u0026#34; key: [0.2, 0.1, 0.1, ...] → Score: 0.7\r├─ \u0026#34;bank\u0026#34; key: [0.1, 0.2, 0.3, ...] → Score: 0.95 (high! self-attention)\r├─ \u0026#34;was\u0026#34; key: [0.5, 0.2, 0.1, ...] → Score: 0.8\r└─ \u0026#34;robbed\u0026#34; key: [0.3, 0.3, 0.1, ...] → Score: 0.85\rNotice: \u0026#34;robbed\u0026#34; helps understand \u0026#34;bank\u0026#34;!\rThis is why BERT is bidirectional. BERT vs GPT: The Key Difference Attention Masking GPT (Left-to-Right, Decoder-style) Processing: \u0026#34;The bank was robbed\u0026#34;\rPosition 1 (\u0026#34;The\u0026#34;):\r├─ Can attend to: \u0026#34;The\u0026#34; ✓\r├─ Can attend to: \u0026#34;bank\u0026#34; ✗ (masked, future)\r├─ Can attend to: \u0026#34;was\u0026#34; ✗ (masked, future)\r└─ Can attend to: \u0026#34;robbed\u0026#34; ✗ (masked, future)\rPosition 4 (\u0026#34;robbed\u0026#34;):\r├─ Can attend to: \u0026#34;The\u0026#34; ✓\r├─ Can attend to: \u0026#34;bank\u0026#34; ✓\r├─ Can attend to: \u0026#34;was\u0026#34; ✓\r└─ Can attend to: \u0026#34;robbed\u0026#34; ✓\rProblem: \u0026#34;The\u0026#34; can\u0026#39;t use \u0026#34;robbed\u0026#34; to understand its meaning! BERT (Bidirectional, Encoder-style) Processing: \u0026#34;The bank was robbed\u0026#34;\rPosition 1 (\u0026#34;The\u0026#34;):\r├─ Can attend to: \u0026#34;The\u0026#34; ✓\r├─ Can attend to: \u0026#34;bank\u0026#34; ✓\r├─ Can attend to: \u0026#34;was\u0026#34; ✓\r└─ Can attend to: \u0026#34;robbed\u0026#34; ✓\rPosition 4 (\u0026#34;robbed\u0026#34;):\r├─ Can attend to: \u0026#34;The\u0026#34; ✓\r├─ Can attend to: \u0026#34;bank\u0026#34; ✓\r├─ Can attend to: \u0026#34;was\u0026#34; ✓\r└─ Can attend to: \u0026#34;robbed\u0026#34; ✓\rBenefit: \u0026#34;The\u0026#34; CAN use \u0026#34;robbed\u0026#34; to understand context!\rBetter representations for ALL words! Why Bidirectional Is Better Language Understanding Example Sentence: \u0026#34;I went to the bank to deposit money\u0026#34;\rWord: \u0026#34;bank\u0026#34; (financial institution)\rGPT Processing (Left-to-Right):\r├─ Can see: \u0026#34;I went to the\u0026#34;\r├─ Cannot see: \u0026#34;to deposit money\u0026#34; (future!)\r├─ Context understanding: 30% (incomplete)\rBERT Processing (Bidirectional):\r├─ Can see: \u0026#34;I went to the\u0026#34; + \u0026#34;to deposit money\u0026#34;\r├─ Full context: 100%\r├─ Context understanding: 90% (much better!) Word Sense Disambiguation \u0026#34;I went to the bank to deposit money\u0026#34; → Financial institution\r\u0026#34;The river bank was very scenic\u0026#34; → Land along river\r\u0026#34;The bank was robbed yesterday\u0026#34; → Company/institution\rGPT: Sees left context only\r├─ \u0026#34;I went to the\u0026#34; (incomplete)\r├─ Risk: Wrong sense!\rBERT: Sees full context\r├─ \u0026#34;I went to the _____ to deposit money\u0026#34;\r├─ Automatically disambiguates! ✓ BERT\u0026rsquo;s Input Representation Token Embeddings BERT uses WordPiece tokenization:\rInput: \u0026#34;The bank was robbed\u0026#34;\rTokenization:\r├─ \u0026#34;The\u0026#34; → [101] (CLS token adds to beginning)\r├─ \u0026#34;bank\u0026#34; → [1998]\r├─ \u0026#34;was\u0026#34; → [2003]\r├─ \u0026#34;robbed\u0026#34; → [6861]\r├─ \u0026#34;[SEP]\u0026#34; → [102] (separator token adds to end)\rRaw tokens: [101, 1998, 2003, 6861, 102] Three Types of Embeddings BERT combines THREE embedding types:\rToken Embedding: \u0026#34;bank\u0026#34; → [0.2, -0.5, 0.8, ...]\r├─ Specific word representation\r├─ Learned during pre-training\rSegment Embedding: Sentence A vs Sentence B → [0.1, 0.2, ...]\r├─ Marks which sentence each token belongs to\r├─ For sentence pair tasks (Next Sentence Prediction)\rPositional Embedding: Position 2 → [0.5, 0.1, ...]\r├─ Encodes position in sequence\r├─ 0, 1, 2, 3, 4...\rFinal Embedding = Token + Segment + Positional\r= [0.2, -0.5, 0.8, ...] + [0.1, 0.2, ...] + [0.5, 0.1, ...]\r= [0.8, -0.2, 1.3, ...] Comparison: BERT vs GPT vs Original Transformer BERT GPT Transformer\r──── ─── ─────────────\rEncoder-Decoder Encoder only Encoder+Decoder Encoder+Decoder\rAttention Type Bidirectional Unidirectional Mixed\rPre-train Task MLM + NSP Language Model None (N/A)\rDirection Both ways Left to right Decoder masked\rBest For Understanding Generation Translation\rPerformance Best (90%) Good (85%) Varies (varies%)\rSpeed Medium Fast Slowest\rParameters 110M/340M 117M/345M ~100M BERT\u0026rsquo;s Performance on NLP Tasks GLUE Benchmark Results Before BERT (State-of-the-art 2017):\r├─ Sentiment: 92.1% accuracy\r├─ Textual Similarity: 80.5% Spearman\r├─ Named Entity: 91.6% F1\r└─ Question Answering: 78.2% F1\rBERT-base:\r├─ Sentiment: 94.9% accuracy (+2.8%)\r├─ Textual Similarity: 85.8% Spearman (+5.3%)\r├─ Named Entity: 96.4% F1 (+4.8%)\r└─ Question Answering: 84.2% F1 (+6.0%)\rImprovement: 4-6% across the board!\rFor a deployed system, that\u0026#39;s huge! Why BERT Became So Popular Simple Fine-tuning: Add one classification head, fine-tune all layers Strong Performance: Beats all previous methods by large margins Pretrained Weights: Can download weights, use immediately Multilingual: BERT trained on 104 languages (mBERT) Transferable: Works for almost any NLP task BERT\u0026rsquo;s Limitations ⚠️ Bidirectional = Can\u0026rsquo;t generate text directly\nProblem: You can\u0026#39;t do \u0026#34;given \u0026#39;The bank\u0026#39;, generate the rest\u0026#34;\rReason: Each token can see the future!\rWould collapse the generation task.\rSolution: Fine-tune for classification, not generation ⚠️ Fixed context window: 512 tokens maximum\nLong documents don\u0026#39;t fit!\rSolution: Hierarchical BERT or RoBERTa (4096 tokens) ⚠️ Computationally expensive\n110M parameters = 440MB just for weights\rInference slower than smaller models\rSolution: Distillation (DistilBERT, 40% parameters, 60% speed) Variants of BERT BERT\r├─ BERT-base: Original, 110M params\r├─ BERT-large: Bigger, 340M params\r├─ mBERT: 104 languages, multilingual\r├─ DistilBERT: Distilled, 40M params, faster\r└─ RoBERTa: Improved training, 125M params\rAll based on same encoder-only architecture Key Takeaways ✅ Bidirectional context matters: BERT\u0026rsquo;s breakthrough insight ✅ Encoder-only architecture: Perfect for understanding ✅ Pre-trained weights are gold: Don\u0026rsquo;t train from scratch! ✅ Self-attention with full context: Better representations ✅ Simple fine-tuning: Add head + train 1-3 epochs\nWhat\u0026rsquo;s Coming Next Day 48: How BERT learned this understanding (MLM + NSP) Day 49: T5 - Scaling up to encoder-decoder Day 50: Fine-tuning BERT for real tasks The beauty of BERT: Bidirectional transformers showed that seeing everything at once is the key to understanding language.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/4.2-event2/",
	"title": "Event 2 - AWS GenAI Builder Club: AI-Driven Development Life Cycle",
	"tags": [],
	"description": "",
	"content": "AWS GenAI Builder Club: AI-Driven Development Life Cycle - Reimagining Software Engineering Date \u0026amp; Time: Friday, October 3, 2025 | 14:00 (2:00 PM)\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nInstructors: Toan Huynh \u0026amp; My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nEvent Overview This AWS GenAI Builder Club session explored the AI-Driven Development Lifecycle (AI-DLC), a transformative approach to software engineering that integrates AI as a central collaborator throughout the entire development process. The session featured hands-on demonstrations of Amazon Q Developer and Kiro, showcasing practical applications of AI in modern software development.\nAgenda Time Session Instructor 14:00 - 14:15 Welcoming - 14:15 - 15:30 AI-Driven Development Life Cycle Overview \u0026amp; Amazon Q Developer Demonstration Toan Huynh 15:30 - 15:45 Break - 15:45 - 16:30 Kiro Demonstration My Nguyen Key Concepts \u0026amp; Learnings 1. AI-Driven Development Lifecycle (AI-DLC) Overview Core Philosophy The AI-Driven Development Lifecycle represents a fundamental shift in how software is built. Rather than treating AI as an afterthought or simple code completion tool, AI-DLC embeds AI as an intelligent partner throughout the entire development process.\nKey Principles:\nYou Are in Control - AI is your assistant, not your manager. You must maintain decision-making authority over the project direction and implementation details.\nAI as Collaborator, Not Replacement - AI should ask critical questions about your project requirements, architecture, and goals. The collaboration should be bidirectional, with you guiding the AI\u0026rsquo;s suggestions.\nPlan Before Implementation - Always create a comprehensive plan before diving into code. AI can help generate this plan, but you must review, validate, and refine it.\nThe Development Workflow Step 1: Create a Project Plan\nDefine clear project requirements and scope Ask AI to generate a plan based on your specifications Review the plan critically and request modifications Ensure the plan is detailed and unambiguous Step 2: Break Down into User Stories\nConvert the plan into user stories with clear acceptance criteria Divide large scope into smaller, manageable units Each unit becomes a mini-project that can be assigned to team members Estimate timelines for each unit (though be cautious of over-estimation) Step 3: Define Technology Stack\nClearly specify the technologies, frameworks, and tools to be used Instead of telling AI \u0026ldquo;don\u0026rsquo;t implement this,\u0026rdquo; tell it \u0026ldquo;implement this way\u0026rdquo; Positive direction yields higher success rates than negative constraints Step 4: Detailed Requirements \u0026amp; Design\nWrite requirements with precision and clarity Collaborate with AI to create detailed specifications Define data models, API contracts, and system architecture Create design documents before implementation begins Step 5: Implementation \u0026amp; Verification\nImplement features according to the plan Use mob development approach (team works together on code) Verify all output code as a team Conduct code reviews and quality checks Step 6: Testing \u0026amp; Deployment\nMove through environments: Development (Dev) → Testing (QA) → User Acceptance Testing (UAT) → Production (Prod) Ensure quality gates at each stage Validate functionality before production release Critical Success Factors Create a Plan First - Don\u0026rsquo;t expect AI to handle everything. Always start with a clear plan. Review Regularly - Continuously review AI suggestions and outputs. High error rates are possible. You Are the Manager - Your value lies in code validation and project management, not in writing every line of code. Ask Clarifying Questions - Ensure AI understands your project context by asking it critical questions about requirements, architecture, and goals. Use Prompt Templates - Create structured prompts that include user context, user stories, and specific requirements to get clearer AI responses. Export Plans to Files - Have AI generate plans as files you can save, review, and modify. This creates a living document for future reference. Be Polite to AI - Maintain respectful communication with AI tools. Good rapport may help in future interactions (and it\u0026rsquo;s just good practice!). 2. Amazon Q Developer Demonstration What is Amazon Q Developer? Amazon Q Developer is an AI-powered assistant that transforms the software development lifecycle (SDLC) through agentic capabilities across multiple platforms:\nAWS Console - Helps with infrastructure and service configuration IDE (Integrated Development Environment) - Provides code generation and optimization suggestions CLI (Command Line Interface) - Assists with command generation and automation DevSecOps Platforms - Integrates security practices into the development workflow Key Capabilities Code Generation \u0026amp; Quality\nAccelerates code generation with AI-powered suggestions Improves code quality through intelligent recommendations Maintains seamless integration with existing workflows Understands complex codebases and suggests optimizations Documentation \u0026amp; Testing\nAutomatically generates comprehensive documentation Creates unit tests with minimal manual effort Significantly improves code maintainability and reliability Reduces boilerplate and repetitive coding tasks Intelligent Collaboration\nActs as an intelligent collaborator leveraging large language models Combines deep AWS service knowledge with coding expertise Helps developers accelerate development cycles Enhances code quality and strengthens security posture Automation Across Development Lifecycle\nAutomates routine tasks across the entire development lifecycle Reduces manual, repetitive work Allows developers to focus on higher-value, creative tasks Improves overall productivity and efficiency Best Practices for Using Amazon Q Developer Provide Clear Context - Give Q detailed information about your project, architecture, and requirements Use Specific Prompts - Instead of vague requests, provide specific, detailed prompts with examples Review Suggestions - Always review Q\u0026rsquo;s suggestions before implementing them Iterate and Refine - If the first suggestion isn\u0026rsquo;t perfect, refine your prompt and try again Leverage AWS Knowledge - Take advantage of Q\u0026rsquo;s deep understanding of AWS services and best practices 3. Kiro Demonstration What is Kiro? Kiro is an agentic IDE (Integrated Development Environment) developed by Amazon Web Services that bridges the gap between rapid AI-powered prototyping and production-ready software development. It\u0026rsquo;s currently in public preview.\nCore Philosophy Kiro embodies the principle that AI should enhance developer productivity while maintaining professional standards, clear structure, comprehensive testing, documentation, and long-term maintainability.\nKey Features Spec-Driven Development\nWhen you submit a requirement (e.g., \u0026ldquo;add a product rating system\u0026rdquo;), Kiro converts it into: User stories with clear acceptance criteria Design documentation Task lists and implementation plans Structured specifications before code generation Agent Hooks \u0026amp; Automation\nAutomatically triggers tasks based on events: File saves trigger documentation updates Commits trigger test generation Specific actions trigger performance optimization Reduces manual, repetitive work Steering \u0026amp; Project Context\nCreate steering files (markdown) to describe: Project structure and organization Coding standards and conventions Desired architecture patterns Team guidelines and best practices Helps Kiro understand your project context deeply Multi-File Analysis \u0026amp; Intent Understanding\nAnalyzes multiple files simultaneously Understands functional goals across the codebase Makes changes aligned with overall project objectives Goes beyond simple code completion VS Code Integration\nBuilt on VS Code\u0026rsquo;s open-source foundation Import settings, themes, and extensions from VS Code Familiar interface for existing VS Code users Seamless transition for developers Flexible AI Model Selection\nCurrently uses Claude Sonnet 4 as default \u0026ldquo;Auto\u0026rdquo; mode combines multiple models based on context Balance between quality and cost Flexibility to choose different models for different tasks Advantages of Using Kiro Increased Transparency \u0026amp; Control\nStart with specifications before code generation Review and validate specs before implementation Reduce hallucinated code or misaligned implementations Maintain clear traceability from requirements to code Reduced Boilerplate \u0026amp; Repetitive Tasks\nAgent hooks automate documentation generation Automatic unit test creation Automatic information updates Frees developers for higher-value work Security \u0026amp; Privacy\nMost code operations happen locally Data only sent externally with explicit permission Maintains control over sensitive information Extensibility \u0026amp; Flexibility\nIntegrates external tools via MCP (Model Context Protocol) Supports multiple AI models Not locked into a single AI environment Adaptable to different team workflows Limitations \u0026amp; Considerations Preview Status - Still in public preview; stability and features may change Complex Projects - May struggle with deep contextual understanding in highly complex projects Supervision Required - Users still need to oversee and validate AI decisions Future Pricing - Expected pricing tiers: Free: ~50 tasks/month Pro: ~1,000 tasks/month Pro+: ~3,000 tasks/month When to Use Kiro You want an AI + programming workflow that maintains professionalism and clear structure Building rapid prototypes but concerned about production sustainability Exploring how AI can become a true programming colleague, not just a code suggestion tool You need spec-driven development with automated documentation and testing Common Pitfalls When Using AI in Development 1. Expecting AI to Handle Everything Problem: Many developers expect AI to complete entire projects autonomously.\nSolution: Always create a plan first and review regularly. AI is a tool to enhance productivity, not replace developer judgment.\n2. High Error Rates Problem: AI can make mistakes, especially in complex scenarios.\nSolution: Implement regular review cycles. Validate all AI-generated code before deployment.\n3. Lack of Clear Requirements Problem: Vague or unclear requirements lead to vague AI outputs.\nSolution: Write requirements with precision. Collaborate with AI to create detailed specifications before implementation.\n4. Negative Constraints Instead of Positive Direction Problem: Telling AI \u0026ldquo;don\u0026rsquo;t do this\u0026rdquo; is less effective than \u0026ldquo;do this.\u0026rdquo;\nSolution: Use positive, specific instructions. Higher success rates come from clear positive direction.\n5. Insufficient Project Context Problem: AI doesn\u0026rsquo;t understand your project\u0026rsquo;s unique requirements and constraints.\nSolution: Create steering files, provide detailed context, and ask AI critical questions about your project.\n6. Treating AI as a Manager Problem: Letting AI make all decisions about project direction and architecture.\nSolution: Remember: You are the manager. Your value lies in code validation and project oversight, not in writing every line of code.\nKey Takeaways AI is Your Assistant - Maintain control over project decisions and implementation direction\nPlan First, Code Second - Always create a comprehensive plan before implementation\nCollaboration Over Automation - AI should ask questions and collaborate, not just execute commands\nClear Requirements Matter - Precision in requirements leads to better AI outputs\nRegular Review is Essential - Don\u0026rsquo;t expect AI to be perfect; review and validate continuously\nYou Are the Code Manager - Your value is in validation and oversight, not in writing every line\nUse Structured Prompts - Templates with context, user stories, and requirements yield better results\nExport Plans to Files - Create living documents you can reference and modify\nPositive Direction Works Better - Tell AI what to do, not what to avoid\nExperience Matters - Use these tools hands-on to understand their capabilities and limitations\nRecommended Tools \u0026amp; Resources Amazon Q Developer - AI-powered development assistant integrated with AWS services Kiro IDE - Spec-driven development environment with AI collaboration AWS CodeWhisperer - Code generation and optimization tool MCP (Model Context Protocol) - Framework for integrating external tools and services Conclusion The AI-Driven Development Lifecycle represents a new paradigm in software engineering where AI and humans collaborate as equals. Success requires clear planning, regular review, precise requirements, and maintaining developer control over project direction. Tools like Amazon Q Developer and Kiro are enabling this new workflow, but they work best when developers understand their capabilities and limitations, and maintain their role as project managers and code validators.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/hugo_aws/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Online Library – A Serverless Content Platform for Small Groups 1. Executive Summary The Online Library project aims to build a low-cost serverless platform for storing and distributing content (PDF/ePub) for a small user group (initially ~100 users, primarily students/labs needing controlled internal research-material sharing). The solution prioritizes security, content moderation (Admin Approval), and transparent, linear operating costs as it scales.\nThe architecture uses a fully AWS Serverless stack (Amplify, Cognito, API Gateway, Lambda, S3, CloudFront, DynamoDB).\nEstimated cost for the MVP (excluding Free Tier) is ≈ $9.80/month, with predictable scaling to 5,000–50,000 users.\n2. Problem Statement What’s the Problem? Documents and books are scattered; there is no secure content delivery system with access control; the process of adding or moderating user-generated content (UGC) is slow and has high friction.\nThe Solution: Build a serverless pipeline on AWS:\nUsers upload files via Presigned PUT URL to temporary S3; Admin approves → Lambda moves the file to a protected public folder; Readers access content via Signed GET URL (from CloudFront/CDN) to ensure speed and controlled access.\nBenefits and Return on Investment Business value: Centralized content; quality control through moderation; fast deployment with CI/CD. Technical benefits: Very low operating cost (≈ $9.80/month for MVP); scalable serverless architecture; secured content access. 3. Solution Architecture A. High level B. Request flow AWS Services Used Service Primary Role Specific Tasks Amplify Hosting CI/CD + FE Hosting Build \u0026amp; Deploy Next.js, domain management Cognito Authentication Sign-up/Login, JWT issuance, refresh tokens API Gateway API Entry Point Receive requests, validate JWT, route to Lambda Lambda Business Logic Handle upload/approval, generate signed URLs, write metadata S3 Object Storage Store original and approved files, served via CloudFront Signed URL CloudFront CDN Fast content delivery, blocks direct S3 access via OAC DynamoDB Database Store metadata (title, uploader, approval status) Route 53 DNS Domain mapping to Amplify, API Gateway, CloudFront CloudWatch Monitoring Lambda logs, anomaly alerts Search Simple search fields (titles, author) using DynamoDB GSIs.\nComponent Design User Upload: Presigned PUT to S3 uploads/. Admin Approval: Lambda copies file from uploads/ → public/books/ upon approval. Reader Security: CloudFront OAC prevents direct S3 access; reading occurs only through Signed URL generated by Lambda. Search Architecture Simple Search: Design GSI for title and author (example: GSI1: PK=TITLE#{normalizedTitle}, SK=BOOK#{bookId}; GSI2: PK=AUTHOR#{normalizedAuthor}, SK=BOOK#{bookId}). Add endpoint GET /search?title=...\u0026amp;author=... to query GSI instead of Scan. Admin Authorization Use Cognito User Groups with an Admins group. Admin JWT contains cognito:groups: [\u0026quot;Admins\u0026quot;]. Admin-specific Lambdas (exampleapproveBook, takedownBook) check this claim and return 403 Forbidden otherwise. JWT Authorizer (API Gateway HTTP API) handles authentication, while authorization logic is inside Lambda. 4. Technical Implementation Implementation Phases Design \u0026amp; IaC: Use CDK to define all stacks (Cognito, DDB, S3, Amplify, Lambda, API). Upload \u0026amp; Approval Flow: Implement Presigned PUT, metadata (status= PENDING), Admin approval logic. Reading Flow: Implement Signed GET, FE reader stream via CloudFront. Ops: CloudWatch logs, budget alerts, IAM hardening. Search: Add GSI for title, author, implement GET /search. Technical Requirements Entire infrastructure defined using CDK. API Gateway uses HTTP API for cost savings. Lambda (Python) handles business logic \u0026amp; DynamoDB/S3. S3 Bucket Policy must deny public access and allow only CloudFront OAC. 5. Timeline \u0026amp; Milestones Project Timeline Platform \u0026amp; Authentication (Week 1–2) Objective: Set up infrastructure and allow user login.\nBackend Tasks (CDK/DevOps): CDK/IaC stack for Cognito. CDK stack for DynamoDB (main table, no GSI yet). CDK stack for S3 (uploads, public, logs) + OAC. Deploy API Gateway (HTTP API) + a test Lambda. Frontend Tasks (Amplify): Configure Amplify Hosting + GitHub CI/CD. Integrate Amplify UI / Cognito SDK for: Sign-up, Email verification, Login, Forgot password. Milestone: git push automatically deploys FE. User can sign-up/login and obtain JWT. Upload \u0026amp; Approval Flow (Week 2–3) Objective: Allow authenticated users to upload files and Admins to approve them.\nBackend (Lambda/CDK): Implement createUploadUrl Lambda: Validate JWT. Create Presigned PUT URL to uploads/. Write metadata (status=PENDING). Implement approveBook: Validate Admin role. Copy S3 file uploads/ → public/books/. Update DynamoDB status (APPROVED). Frontend: Upload form (drag \u0026amp; drop). Upload via Presigned PUT. Admin dashboard with list of PENDING, button “Approve”. Reading \u0026amp; Search (Week 3–4) Objective: Allow reading \u0026amp; searching approved books.\nBackend: Implement getReadUrl: generate Signed GET URL (short TTL). Add GSI for title, author. Implement searchBooks. Frontend: Homepage: book list. Search bar → API searchBooks. Reader screen using the Signed URL (e.g., via react-pdf). Ops \u0026amp; Security (Week 5–6) Backend: S3 Event Notification for new uploads. Lambda validateMimeType: read magic bytes to verify PDF/ePub. Lambda takedownBook (Admin), deleteUpload (auto cleanup after 72h). DevOps: AWS Budget Alerts, CloudWatch Alarms. IAM least-privilege + CORS tightening. 6. Budget Estimation Budget comes from AWS Pricing Calculator.\nMonthly cost (strict, no Free Tier, ~100 users): ≈ $9.80/month.\n# AWS Service Region Monthly (USD) Notes 0 Amazon CloudFront Asia Pacific (Singapore) 0.86 10 GB data egress + 10 000 HTTPS requests 1 AWS Amplify Asia Pacific (Singapore) 1.31 100 build min + 0.5 GB storage + 2 GB served 2 Amazon API Gateway Asia Pacific (Singapore) 0.01 ~10 000 HTTP API calls/tháng 3 AWS Lambda Asia Pacific (Singapore) 0.00 128 MB RAM × 100 ms × 10 000 invokes 4 Amazon S3 (Standard) Asia Pacific (Singapore) 0.05 2 GB object storage for books/images 5 Data Transfer Asia Pacific (Singapore) 0.00 Included in CloudFront cost 6 DynamoDB (On-Demand) Asia Pacific (Singapore) 0.03 Light metadata table (0.1 GB, few reads/writes) 7 Amazon Cognito Asia Pacific (Singapore) 5.00 100 MAU, Advanced Security enabled 8 Amazon CloudWatch Asia Pacific (Singapore) 1.64 5 metrics + 0.1 GB logs/tháng 9 Amazon Route 53 Asia Pacific (Singapore) 0.90 1 Hosted Zone + DNS queries ≈ 9.80 USD / month No Free Tier applied Infrastructure Costs This cost model demonstrates the efficiency of serverless architecture: costs are primarily centered on the value delivered to the user (Cognito MAU), rather than paying for \u0026lsquo;idle servers\u0026rsquo;.\n7. Risk Assessment Risk Matrix Risk Impact Mitigation Cost spike due to sudden user growth High Limit MAU, cache metadata via CloudFront Abuse of uploads Medium Limit ≤ 50MB; auto-delete after 72h Fake/malicious file types Medium S3 Event → Lambda MIME validation Monitoring overload Low CloudWatch alerts, 14-day retention Mitigation Strategies cost: Set AWS Budget Alerts for CloudFront and Cognito. Be aware that Signed URLs have a short TTL and should not be cached publicly long-term; instead, cache metadata/API responses (book lists, details) on CloudFront for 3–5 minutes to reduce API load. Only generate Signed URLs when the user actually clicks to read (on-demand), do not pre-generate for the entire list. Upload: Limit file size to ≤ 50MB for MVP. (Can be increased to 200MB if needed, use multipart upload on the FE to avoid timeouts.) Apply Rate Limit/Throttling on API Gateway for endpoints that create Presigned URLs. Set up an S3 Lifecycle Policy to automatically delete unapproved files in uploads/ after 72h. Add Server-side Validation: S3 Event Notifications $\\to$ Lambda reads magic bytes (e.g., file-type library) to verify correct PDF/ePub; if incorrect, automatically delete and write REJECTED_INVALID_TYPE status to DynamoDB. Copyright (DMCA): Store Audit Log in DynamoDB: uploaderID, uploadTimestamp, adminApproverID, approvalTimestamp for traceability. Build a Takedown API (Admin only): update status to TAKEDOWN; optionally move the object from public/books/ to quarantine/books/ (do not delete completely) to preserve traces. Contingency Plans If costs exceed budget, enable Invite-Only mode to cap Cognito MAU and reduce load.\n8. Expected Outcomes Technical Improvements Fast and secure content delivery (CDN + Signed URL). Standard AWS Serverless architecture capable of scaling to 50,000 users without redesign. Fully automated CI/CD for both frontend \u0026amp; backend. Long-term Value A centralized content platform for structured book data. Continuous documentation of an end-to-end Serverless implementation. Room for future analytics (QuickSight) or AI/ML features. This system proves the ability to build a platform that securely, cost-effectively, and scalably easy by AWS Serverless services - that suitable to apply for small groups or communities.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/",
	"title": "Week 2 - AWS Networking Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-15 to 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 2 Overview This week focused on AWS networking services, spanning foundational VPC concepts through enterprise-grade connectivity patterns.\nKey Topics Amazon VPC and subnet design Security Groups and NACLs Internet Gateway and NAT Gateway VPC Peering and Transit Gateway Elastic Load Balancing (ALB, NLB, GWLB) Hands-on Labs Lab 03: Amazon VPC \u0026amp; Networking Basics Lab 10: Hybrid DNS (Route 53 Resolver) Lab 19: VPC Peering Lab 20: AWS Transit Gateway "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/",
	"title": "Day 03 - AWS Management Tools",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-10 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Management Tools AWS Management Console Log in as Root User or IAM User (requires 12-digit Account ID). Search and access individual service dashboards. Support Center allows you to open support cases directly. AWS Command Line Interface (CLI) Open-source command-line tool for interacting with AWS services. Provides functionality equivalent to the Console. Key Features:\nCross-platform support (Windows, macOS, Linux) Scriptable and automatable Direct access to AWS service APIs Supports profiles for multiple accounts AWS SDK (Software Development Kit) Simplifies integration of AWS services within applications. Handles authentication, retries, and data serialization/deserialization automatically. Supported Languages:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go, and more "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/",
	"title": "Day 08 - VPC Security &amp; Flow Logs",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-17 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Security Security Group (SG) A stateful virtual firewall that controls inbound and outbound traffic to AWS resources. Rules are based on protocol, port, source, or another security group. Only allow rules are supported. Applied to Elastic Network Interfaces (ENIs). Security Group Characteristics:\nStateful: return traffic automatically allowed Supports allow rules only Evaluates all rules before deciding Applies to instance level (ENI) Network Access Control List (NACL) A stateless virtual firewall that operates at the subnet level. Rules control inbound and outbound traffic by protocol, port, and source. Default NACL allows all traffic. NACL Characteristics:\nStateless: must explicitly allow return traffic Supports both allow and deny rules Rules processed in number order Applies to subnet level VPC Flow Logs Capture metadata about IP traffic to and from network interfaces in your VPC. Logs can be delivered to Amazon CloudWatch Logs or S3. Flow Logs do not record packet payloads. Flow Log Use Cases:\nTroubleshoot connectivity issues Monitor traffic patterns Security analysis Compliance requirements Hands-On Labs Lab 03 – Amazon VPC \u0026amp; Networking (continued) Launch EC2 Instances in Subnets → 04-1 Test Connection Between Instances → 04-2 Create NAT Gateway (Private ↔ Internet) → 04-3 EC2 Instance Connect Endpoint (no bastion) → 04-5 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/",
	"title": "Day 13 - Instance Store &amp; User Data",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-24 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Advanced Features Instance Store Instance Store provides temporary block-level storage physically attached to the EC2 host. Characteristics\nVery high I/O and throughput Data lost when instance stops or terminates Cannot be detached or snapshotted Use Cases\nCaching or temporary data processing Applications with their own redundancy or replication Instance Store vs EBS:\nFeature Instance Store EBS Persistence Temporary Persistent Performance Very high High Snapshot No Yes Detachable No Yes Cost Included Additional User Data User Data scripts run automatically at instance launch (once per AMI provision). Linux – bash scripts Windows – PowerShell scripts User Data Examples:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata provides details about the running instance such as private/public IP, hostname, and security groups. Often used in user data scripts for dynamic configuration. Accessing Metadata:\n# Get instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Get public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Get IAM role credentials curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/",
	"title": "Day 18 - AWS Snow Family &amp; Hybrid Storage",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-01 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Snow Family Purpose-built devices and services to move large datasets into and out of AWS when networks are limited or data volumes are massive.\nAWS Snowcone: Small, rugged device (~8 TB). Suited for edge and remote sites. AWS Snowball: Snowball Edge Storage Optimized: Up to ~80 TB usable storage. Snowball Edge Compute Optimized: Adds powerful compute with ~42 TB storage. AWS Snowmobile: Exabyte-scale data transfer (up to 100 PB) in a secure containerized data center. Snow Family Comparison:\nDevice Storage Compute Use Case Snowcone 8 TB 2 vCPUs Edge, IoT Snowball Storage 80 TB 40 vCPUs Data migration Snowball Compute 42 TB 52 vCPUs Edge computing Snowmobile 100 PB N/A Datacenter migration When to Use Snow Family:\nLimited or expensive bandwidth Large data volumes (TB to PB) Remote or disconnected locations Edge computing requirements Regulatory data residency AWS Storage Gateway Hybrid cloud storage service that connects on-premises applications with cloud-backed storage.\nGateway Types File Gateway\nNFS/SMB file shares backed by S3 objects. Use cases: user shares, application backups, archives. Volume Gateway\niSCSI block storage backed by S3 with EBS snapshots. Modes: Cached volumes: Primary data in S3; local cache on-prem. Stored volumes: Primary data on-prem; async copy to S3. Use cases: on-prem block workloads with cloud backup/DR. Tape Gateway\nVirtual Tape Library (VTL) for existing backup apps (e.g., NetBackup, Veeam). Writes appear as tape but land in S3/Glacier. Use cases: tape replacement and archival modernization. Hands-On Labs Lab 24 – AWS Storage Gateway (On-Premises Integration) Create Storage Gateway → 24-2.1 Create File Shares → 24-2.2 Mount File Shares On-Prem → 24-2.3 Clean Up Resources → 24-3 Lab 14 – AWS VM Import/Export (Part 1) VMware Workstation → 14-01 Export Virtual Machine from On-Premises → 14-02.1 Upload Virtual Machine to AWS → 14-02.2 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/",
	"title": "Day 23 - Amazon Cognito &amp; Organizations",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-08 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Cognito Managed authentication/authorization and user management for web \u0026amp; mobile apps. Components: User Pools: Sign-up/sign-in user directories. Identity Pools: Federated identities for temporary AWS credentials to access services. Cognito User Pools Features:\nSign-up and sign-in Social identity providers (Google, Facebook, Amazon) SAML identity providers Multi-factor authentication (MFA) Email and phone verification Custom authentication flows Lambda triggers for customization Cognito Identity Pools Features:\nTemporary AWS credentials Authenticated and unauthenticated access Role-based access control Integration with User Pools Support for external identity providers AWS Organizations Centrally manage multiple AWS accounts in a single organization. Benefits\nCentralized account management Consolidated Billing Hierarchies with Organizational Units (OUs) Guardrails with Service Control Policies (SCPs) Organizational Units (OUs) Group accounts by department, project, or environment; nest OUs for hierarchical policies. Example OU Structure:\nRoot\r├── Production OU\r│ ├── Web App Account\r│ └── Database Account\r├── Development OU\r│ ├── Dev Account\r│ └── Test Account\r└── Security OU\r└── Audit Account Consolidated Billing One invoice for all accounts; volume pricing benefits; no extra cost. Benefits:\nVolume discounts across accounts Easier tracking and reporting Simplified payment method Reserved Instance sharing Hands-On Labs Lab 28 – IAM Cross-Region Role \u0026amp; Policy (Part 2) Switch Roles → 28-5.1 Access EC2 Console – Tokyo → 28-5.2.1 Access EC2 Console – N. Virginia → 28-5.2.2 Create EC2 (No Qualified Tags) → 28-5.2.3 Edit EC2 Resource Tag → 28-5.2.4 Policy Check → 28-5.2.5 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 1) Create EC2 Instance with Tag → 27-2.1.1 Manage Tags in AWS Resources → 27-2.1.2 Filter Resources by Tag → 27-2.1.3 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/",
	"title": "Day 28 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-15 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Redshift Fully managed cloud data warehouse optimized for large-scale analytics (OLAP).\nColumnar storage, compression, MPP execution; scales from hundreds of GB to PB. Integrations: S3, Kinesis, DynamoDB, BI tools; strong security features. Concurrency Scaling adds capacity automatically during spikes. Architecture: cluster (leader node + compute nodes), each compute node has slices. Deployment options:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (query S3 directly) Use cases: enterprise BI, data lake analytics, dashboards, trend analysis, forecasting.\nRedshift Features:\nColumnar Storage: Optimized for analytics queries Massively Parallel Processing (MPP): Distributes queries across nodes Result Caching: Speeds up repeated queries Automatic Compression: Reduces storage costs Workload Management (WLM): Query prioritization Concurrency Scaling: Handle burst workloads Redshift vs Traditional Data Warehouse:\nFeature Redshift Traditional DW Setup Minutes Weeks/Months Scaling Elastic Fixed capacity Cost Pay-as-you-go Large upfront Maintenance Managed Self-managed Redshift Spectrum:\nQuery data directly in S3 without loading Separate compute and storage Support for various file formats (Parquet, ORC, JSON) Cost-effective for infrequently accessed data Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 2) MSSQL → Aurora MySQL Target Config → 43-07 MSSQL → Aurora MySQL Create Project → 43-08 MSSQL → Aurora MySQL Schema Conversion → 43-09 Oracle → MySQL Schema Conversion (1) → 43-10 Create Migration Task \u0026amp; Endpoints → 43-11 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/",
	"title": "Day 33 - Next.js App Router",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-22 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Next.js 16 App Router Leverage Server Components to fetch data directly on the server and avoid bloated bundles. The /app/books/[id]/page.tsx route handles data fetching and returns a pre-rendered UI. generateMetadata supplies SEO meta tags based on the book payload. // app/books/[id]/page.tsx import { getBook } from \u0026#34;@/lib/api\u0026#34;; export default async function BookDetail({ params }) { const book = await getBook(params.id); if (!book) return notFound(); return \u0026lt;BookPage book={book} /\u0026gt;; } Error \u0026amp; Not Found Handling Only not-found.tsx is needed for missing books—treat it as an expected branch. Skip error.tsx to avoid double handling; log unexpected issues on the backend. Keep UX consistent with a CTA back to the listing page plus a support hotline. Environment \u0026amp; Config Use explicit environment variables: NEXT_PUBLIC_API_URL for the frontend and API_URL for route handlers. Keep .env.example in sync whenever a new variable is introduced. Centralize URL construction in lib/api.ts to prevent duplication. Insight Server Components noticeably reduce latency when rendering detail pages. The App Router enforces a clear folder structure, making it easier to split new slices. When pointing to the mock API, fetch directly from Prism by swapping the base URL. Hands-On Labs Create a custom not-found.tsx with navigation CTAs. Implement a reusable getBook(id) helper for server components and tests. Run npm run lint \u0026amp;\u0026amp; npm run build to ensure the Next.js configuration stays clean. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/1.8.3-day38-2025-10-29/",
	"title": "Day 38 - Seq2seq Models &amp; LSTM Deep Dive",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-29 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nSeq2seq Model Sequence-to-Sequence (Seq2seq) models introduce an encoder-decoder architecture effective for tasks like machine translation and text summarization.\nKey Features: Maps variable-length sequences to fixed-length memory Input and outputs can have different lengths Uses LSTMs and GRUs to avoid vanishing and exploding gradients Encoder takes word tokens as input → hidden state vectors → decoder generates output sequence LSTM Architecture: Deep Dive What is LSTM? LSTM (Long Short-Term Memory) is like a mini version of the human brain when processing memory.\nLSTM Structure = 3 Gates + 1 Cell State 1. Forget Gate – Deciding What to Forget Decides what information to discard from the old state.\nFormula:\nf_t = σ(W_f · [h_{t-1}, x_t] + b_f) Brain analogy:\nUseless messages from someone who ghosted you → forget Formulas you use daily → keep 2. Input Gate – Deciding What to Remember Decides what new information to add to memory.\nFormulas:\ni_t = σ(W_i · [h_{t-1}, x_t] + b_i)\rĈ_t = tanh(W_C · [h_{t-1}, x_t] + b_C) Brain analogy:\nValuable information → store in long-term memory Irrelevant noise → discard immediately 3. Cell State Update – Long-term Memory Updates long-term memory by combining forget and input gates.\nFormula:\nC_t = f_t ⊙ C_{t-1} + i_t ⊙ Ĉ_t Where:\nf_t ⊙ C_{t-1} = what to keep from old memory i_t ⊙ Ĉ_t = what to add from new input 4. Output Gate – Deciding What to Output Decides which memory to use for current output.\nFormulas:\no_t = σ(W_o · [h_{t-1}, x_t] + b_o)\rh_t = o_t ⊙ tanh(C_t) Brain analogy:\nWhen taking an NLP exam → recall LSTM formulas When talking to someone → recall conversation context When doing DevOps → recall AWS specs LSTM vs Human Brain Human Brain LSTM Long-term memory Cell State Filter out unnecessary information Forget Gate Accept new valuable information Input Gate Retrieve appropriate memory to respond Output Gate Learn from sequential experiences RNN backbone Don\u0026rsquo;t forget quickly Long-term dependencies What is a Gate? Gate = cognitive filter\nEach gate = a mechanism that decides \u0026ldquo;keep or discard\u0026rdquo;\nExample: When You Study NLP Forget Gate: \u0026ldquo;Do I still need to remember this outdated method?\u0026rdquo; → Discard if no Input Gate: \u0026ldquo;Is this new concept valuable?\u0026rdquo; → Store if yes Output Gate: \u0026ldquo;What knowledge do I need right now?\u0026rdquo; → Retrieve relevant parts Hidden State Limitations Hidden state doesn\u0026rsquo;t have a token limit, but has a capacity limit for effective memory.\nMathematical Perspective: Hidden state = fixed-size vector (e.g., 128, 256, 512 dimensions) Can process 10 tokens or 10,000 tokens → won\u0026rsquo;t crash Problem: can\u0026rsquo;t remember everything Why? Even with cell state, gradients weaken over many time steps Long-term dependencies get lost Tokens far from the start have weak influence on final output Solution: This is why we need Attention mechanism!\nThrottling in NLP Two Meanings of Throttling: 1. System-Level Throttling (API) Limiting request rate or token processing to:\nProtect GPU resources Distribute resources fairly Avoid server overload Control costs Examples:\nOpenAI GPT: 10 requests/second, 90k tokens/min Anthropic Claude: 20 requests/second HuggingFace: timeout if generation takes too long 2. Model-Level Throttling (Architecture) LSTM, Transformer, and Attention all have mechanisms to limit information processing at any given time:\n(A) LSTM Throttling → Forget Gate When sequence is too long:\nForget gate automatically \u0026ldquo;throttles\u0026rdquo; old information Only allows part of meaning to pass through Like network throttling: \u0026ldquo;overload → reduce bandwidth → drop packets\u0026rdquo; (B) Transformer Throttling → Context Window Limit\nBERT: 512 tokens GPT-3: 2048-4096 tokens GPT-4: 128k-1M tokens Claude 3.5 Sonnet: 200k-1M tokens When input exceeds limit:\nModel cuts data Or refuses to process Or downgrades attention quality (C) Attention Throttling → Sparse Attention In long-context models (Longformer, BigBird, Mistral):\nCan\u0026rsquo;t compute full n² attention Only attend to important regions (local attention) Or global tokens Or sliding window (D) Token Generation Throttling Some decoders will:\nSlow down token generation Limit sampling Apply temperature control Cut beam search When input is noisy or uncertain, this acts like a brake: \u0026ldquo;Not sure → slow down generation → increase quality\u0026rdquo;\nSummary LSTM is not just a model — it\u0026rsquo;s a computational mimicry of how human memory works. Understanding gates helps you understand why certain information persists while other information fades away.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/1.9.3-day43-2025-11-05/",
	"title": "Day 43 - Scaled Dot-Product Attention Deep Dive",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-05 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nScaled Dot-Product Attention: The Core Mechanism This is the heart and soul of transformers. Understanding this deeply is critical.\nThe Attention Formula (Q × K^T)\rAttention(Q, K, V) = softmax(─────────) × V\r√(d_k) Where:\nQ = Query matrix (what are we looking for?) K = Key matrix (what can we attend to?) V = Value matrix (what information do we get?) d_k = dimension of keys (usually 64) Step-by-Step Computation Let\u0026rsquo;s compute attention for a simple example:\nInput Sentence: \u0026ldquo;I am happy\u0026rdquo;\nSetup Phase Step 1: Create Word Embeddings\nI: [0.1, 0.2, 0.3]\ram: [0.4, 0.5, 0.6]\rhappy: [0.7, 0.8, 0.9] Step 2: Convert to Q, K, V In practice, we learn linear projections:\nQ = Embedding × W_q\rK = Embedding × W_k\rV = Embedding × W_v For simplicity, let\u0026rsquo;s say:\nQ = [0.1, 0.2, 0.3] K = [0.1, 0.2, 0.3] V = [0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6] [0.4, 0.5, 0.6] [0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] [0.7, 0.8, 0.9] [0.7, 0.8, 0.9] (In reality, Q, K, V would be different projections, but this shows the concept)\nComputation Phase Step 3: Compute Q × K^T (dot products)\nQ × K^T = [0.1, 0.2, 0.3] [0.1, 0.4, 0.7]\r[0.4, 0.5, 0.6] × [0.2, 0.5, 0.8]\r[0.7, 0.8, 0.9] [0.3, 0.6, 0.9]\rQ₁·K₁ = 0.1×0.1 + 0.2×0.2 + 0.3×0.3 = 0.01 + 0.04 + 0.09 = 0.14\rQ₁·K₂ = 0.1×0.4 + 0.2×0.5 + 0.3×0.6 = 0.04 + 0.10 + 0.18 = 0.32\rQ₁·K₃ = 0.1×0.7 + 0.2×0.8 + 0.3×0.9 = 0.07 + 0.16 + 0.27 = 0.50\rResult matrix:\r[0.14, 0.32, 0.50]\r[0.32, 0.77, 1.22]\r[0.50, 1.22, 1.94] Interpretation:\nQ₁ (query for \u0026ldquo;I\u0026rdquo;) has similarity scores: [0.14, 0.32, 0.50] 0.14 with \u0026ldquo;I\u0026rdquo; itself 0.32 with \u0026ldquo;am\u0026rdquo; 0.50 with \u0026ldquo;happy\u0026rdquo; Step 4: Scale by √d_k\nd_k = 3 (embedding dimension), so √d_k = √3 ≈ 1.73\nScaled matrix = Q×K^T / √3:\r[0.14/1.73, 0.32/1.73, 0.50/1.73] [0.08, 0.18, 0.29]\r[0.32/1.73, 0.77/1.73, 1.22/1.73] = [0.18, 0.44, 0.70]\r[0.50/1.73, 1.22/1.73, 1.94/1.73] [0.29, 0.70, 1.12] Why scale?\nWhen d_k is large (e.g., 64), dot products get very large Large numbers cause softmax to have very small gradients (saturation) Scaling by √d_k keeps numbers in reasonable range for training Step 5: Apply Softmax\nSoftmax converts scores to probabilities (sum to 1):\nsoftmax(x) = exp(x) / sum(exp(x))\rFor first row [0.08, 0.18, 0.29]:\rexp(0.08) ≈ 1.083\rexp(0.18) ≈ 1.197\rexp(0.29) ≈ 1.336\rSum ≈ 3.616\rProbabilities:\r[1.083/3.616, 1.197/3.616, 1.336/3.616] ≈ [0.30, 0.33, 0.37] All three rows:\nSoftmax weights (attention matrix):\r[0.30, 0.33, 0.37]\r[0.26, 0.37, 0.37]\r[0.25, 0.36, 0.39] Interpretation:\nWord \u0026ldquo;I\u0026rdquo; pays 30% attention to itself, 33% to \u0026ldquo;am\u0026rdquo;, 37% to \u0026ldquo;happy\u0026rdquo; Word \u0026ldquo;am\u0026rdquo; pays 26% attention to \u0026ldquo;I\u0026rdquo;, 37% to itself, 37% to \u0026ldquo;happy\u0026rdquo; Word \u0026ldquo;happy\u0026rdquo; pays 25% to \u0026ldquo;I\u0026rdquo;, 36% to \u0026ldquo;am\u0026rdquo;, 39% to itself Step 6: Multiply by Value Matrix (V)\nContext = Softmax_weights × V\rContext(for \u0026#34;I\u0026#34;): 0.30×[0.1,0.2,0.3] + 0.33×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.03,0.06,0.09] + [0.13,0.17,0.20] + [0.26,0.30,0.33]\r= [0.42, 0.53, 0.62]\rContext(for \u0026#34;am\u0026#34;): 0.26×[0.1,0.2,0.3] + 0.37×[0.4,0.5,0.6] + 0.37×[0.7,0.8,0.9]\r= [0.026,0.052,0.078] + [0.148,0.185,0.222] + [0.259,0.296,0.333]\r= [0.433, 0.533, 0.633]\rContext(for \u0026#34;happy\u0026#34;): 0.25×[0.1,0.2,0.3] + 0.36×[0.4,0.5,0.6] + 0.39×[0.7,0.8,0.9]\r= [0.025,0.05,0.075] + [0.144,0.18,0.216] + [0.273,0.312,0.351]\r= [0.442, 0.542, 0.642] Output Context Matrix:\n[0.42, 0.53, 0.62]\r[0.433, 0.533, 0.633]\r[0.442, 0.542, 0.642] Each word now has a context vector that combines information from all words weighted by attention scores!\nWhy Scaled Dot-Product Attention? Aspect Reason Dot Product Efficient similarity measure (just matrix multiplication) Scaling Prevents softmax saturation (keeps gradients healthy) Softmax Converts similarities to normalized weights [0,1] Multiply by V Gets actual information (weighted combination) Multi-Head Attention: Multiple Perspectives Instead of one attention head, we use h = 8 (or more) attention heads:\nMultiHeadAttention(Q, K, V) = Concat(Head₁, ..., Head₈) × W_o\rWhere:\rHead_i = Attention(Q × W_q^i, K × W_k^i, V × W_v^i) Different heads learn different relationships:\nHead 1: Subject-verb relationships Head 2: Adjective-noun relationships Head 3: Pronoun-antecedent relationships Head 4-8: Other semantic patterns Example:\nSentence: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;\rHead 1 (subject-verb):\r- \u0026#34;fox\u0026#34; → \u0026#34;jumps\u0026#34;: 0.9\r- \u0026#34;dog\u0026#34; → has_property: 0.7\rHead 2 (adjective-noun):\r- \u0026#34;quick\u0026#34; → \u0026#34;fox\u0026#34;: 0.85\r- \u0026#34;brown\u0026#34; → \u0026#34;fox\u0026#34;: 0.8\r- \u0026#34;lazy\u0026#34; → \u0026#34;dog\u0026#34;: 0.9\rHead 3 (spatial):\r- \u0026#34;over\u0026#34; → connects \u0026#34;fox\u0026#34; and \u0026#34;dog\u0026#34;: 0.8 All these different perspectives combined give rich contextual understanding.\nComputational Efficiency Why is scaled dot-product attention so efficient?\nMatrix Operations: Just multiplication and softmax (GPU-optimized) Parallelizable: Can process entire sequences at once Memory Efficient: O(n²) memory for n-length sequence (acceptable) Fast Training: Modern GPUs can do billions of dot products/second Comparison:\nRNN: O(n) sequential steps → slow Attention: O(1) depth, O(n²) memory → fast! Key Insights ✅ Attention is learned: W_q, W_k, W_v are trainable parameters ✅ Position-free: No sequential dependencies - can attend across any distance ✅ Interpretable: Can visualize which words attend to which ✅ Efficient: Uses only matrix operations (GPU-friendly)\nNext Steps Now that we understand scaled dot-product attention, we\u0026rsquo;ll explore:\nSelf-attention (query=key=value) Masked attention (decoder only sees past) Encoder-decoder attention (cross-lingual connections) Multi-head attention in detail (learning multiple patterns) "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/1.10.3-day48-2025-11-12/",
	"title": "Day 48 - BERT Pre-training Objectives",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-12 (Wednesday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nHow BERT Actually Learned BERT wasn\u0026rsquo;t just a lucky guess. Google scientists designed two specific pre-training tasks that made it learn bidirectional understanding. These tasks are brilliantly simple.\nPre-training Task 1: Masked Language Modeling (MLM) The Core Idea Hide random words and predict them.\nOriginal: \u0026#34;The quick brown fox jumps over the lazy dog\u0026#34;\rMasked: \u0026#34;The quick [MASK] fox jumps over the lazy dog\u0026#34;\rTask: Predict what [MASK] is!\rAnswer: \u0026#34;brown\u0026#34;\rWhy this works:\r├─ Model must understand context from BOTH sides\r├─ To predict \u0026#34;brown\u0026#34;, need to know:\r│ ├─ Left context: \u0026#34;The quick\u0026#34;\r│ └─ Right context: \u0026#34;fox jumps over the lazy dog\u0026#34;\r└─ Forces bidirectional learning! Masking Strategy Not all tokens are masked the same way:\nWhen we select a token to mask:\r80% of the time: Replace with [MASK] token\r├─ \u0026#34;The quick [MASK] fox\u0026#34;\r├─ Model must learn the word\r└─ Normal case\r10% of the time: Replace with random word\r├─ \u0026#34;The quick apple fox\u0026#34; (instead of \u0026#34;brown\u0026#34;)\r├─ Model learns to correct noisy input\r└─ Robustness training\r10% of the time: Keep the original word\r├─ \u0026#34;The quick brown fox\u0026#34;\r├─ Model learns the representation anyway\r└─ Helps with fine-tuning stability The Mathematics For each token position i in a 15% masked vocabulary:\rP(mask token at position i) = 0.15\rLoss_MLM = -log(P(correct_word | context))\rFor masked position with word \u0026#34;brown\u0026#34;:\r├─ Model outputs: [0.02, 0.05, 0.88, 0.03, ...] (probabilities for all words)\r│ ↑ position of \u0026#34;brown\u0026#34;\r├─ Correct word: \u0026#34;brown\u0026#34; (position 3)\r├─ Loss = -log(0.88) = 0.128\r└─ This pushes probability higher!\rAggregate: Average loss over all 15% masked tokens Example Walkthrough Sentence: \u0026#34;The bank was robbed by masked men\u0026#34;\rTokens: [The, bank, was, robbed, by, masked, men]\rMask 15%: [The, bank, [MASK], robbed, by, masked, men]\rStep 1: Tokenize and embed all tokens\r├─ Input IDs: [101, 1996, 2924, 103, 2001, 10122, 2039, 2095, 2273, 102]\r├─ Token embeddings: 7 x 768 dimensional vectors\r└─ Add positional embeddings: position 0, 1, 2, 3, ...\rStep 2: Process through 12 transformer layers\r├─ Layer 1: Self-attention across all positions\r├─ Layer 2: Self-attention with updated representations\r├─ ...\r└─ Layer 12: Final contextual representations\rStep 3: Predict masked token\r├─ Output for position 2 (was): [0.02, 0.05, 0.88, 0.03, ...]\r├─ argmax = 0.88 → Word ID 3 → \u0026#34;was\u0026#34;\r├─ Correct! ✓\r└─ Loss: small\rStep 4: Backprop gradient through all 12 layers\r└─ Updates all parameters to make this prediction likely Pre-training Task 2: Next Sentence Prediction (NSP) The Core Idea Given two sentences, predict if they\u0026rsquo;re consecutive.\nSentence A: \u0026#34;The bank was robbed yesterday.\u0026#34;\rSentence B: \u0026#34;The police are investigating.\u0026#34;\rQuestion: Are these consecutive sentences?\rAnswer: Yes (IsNext) ✓\rSentence A: \u0026#34;The cat sat on the mat.\u0026#34;\rSentence B: \u0026#34;I enjoy pizza for lunch.\u0026#34;\rQuestion: Are these consecutive sentences?\rAnswer: No (NotNext) ✗\rWhy this helps:\r├─ Model learns to understand relationships between sentences\r├─ Important for tasks like question answering\r├─ Helps with semantic understanding\r└─ But: Recent research shows this is actually less important than MLM! Input Format BERT\u0026#39;s special tokens for sentence pairs:\r[CLS] Sentence A [SEP] Sentence B [SEP]\r↑ ↑ ↑\rToken for Separator End of\rclassification between sequence\r(used for NSP) sentences\rSegment IDs:\r[0 0 0 0 0 0 1 1 1 1 1 1]\rCLS The bank was robbed SEP police are investigating\rToken A = 0 (first segment)\rToken B = 1 (second segment)\rCLS token\u0026#39;s representation used for NSP classification! The Task NSP is a binary classification problem:\rInput: [CLS] ... [SEP] ... [SEP]\r↓\rModel outputs: [0.1, 0.9]\r↑ ↑\rNotNext IsNext\rIf label is IsNext: Loss = -log(0.9) = 0.105\rIf label is NotNext: Loss = -log(0.1) = 2.303\rModel learns to distinguish consecutive sentences! Combined Loss Function BERT trains on both objectives simultaneously:\nLoss_total = Loss_MLM + Loss_NSP\rExample iteration:\r├─ Batch contains 32 sentence pairs\r├─ For each pair:\r│ ├─ 15% of tokens are masked → MLM loss\r│ ├─ Sentence order labeled → NSP loss\r│ └─ Both losses computed\r├─ Average losses: 2.15 (MLM) + 1.45 (NSP)\r├─ Total loss: 3.60\r└─ Backprop updates all parameters\rWhy both?\r├─ MLM forces bidirectional understanding\r├─ NSP forces sentence-level semantics\r└─ Together: Rich linguistic knowledge!\rNote: Modern variants (RoBERTa) drop NSP\r(Found that MLM alone is sufficient!) BERT\u0026rsquo;s Input Embeddings (Deep Dive) Three Embeddings Combined BERT\u0026#39;s embedding = Token + Segment + Position\rExample: \u0026#34;The quick brown fox\u0026#34;\rPosition: 0 1 2 3\rToken Embedding:\r├─ \u0026#34;The\u0026#34; (ID=1996): [0.2, -0.5, 0.1, ...]\r├─ \u0026#34;quick\u0026#34; (ID=2522): [0.1, 0.3, -0.2, ...]\r├─ \u0026#34;brown\u0026#34; (ID=2829): [-0.1, 0.2, 0.4, ...]\r└─ \u0026#34;fox\u0026#34; (ID=4397): [0.3, 0.1, -0.3, ...]\rSegment Embedding (same sentence):\r├─ Position 0: [0.1, 0.2, 0.3, ...] (Segment A)\r├─ Position 1: [0.1, 0.2, 0.3, ...] (Segment A)\r├─ Position 2: [0.1, 0.2, 0.3, ...] (Segment A)\r└─ Position 3: [0.1, 0.2, 0.3, ...] (Segment A)\rPositional Embedding:\r├─ Position 0: [0.0, 0.5, -0.1, ...]\r├─ Position 1: [1.0, 0.3, 0.2, ...]\r├─ Position 2: [0.5, -0.1, 0.5, ...]\r└─ Position 3: [-0.3, 0.6, 0.1, ...]\rFinal Embedding (sum all three):\r├─ Position 0: [0.3, 0.7, 0.3, ...]\r├─ Position 1: [1.1, 0.8, 0.0, ...]\r├─ Position 2: [0.5, 0.3, 0.9, ...]\r└─ Position 3: [0.0, 0.7, -0.2, ...] Positional Encoding Details BERT uses sinusoidal positional encoding:\rPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\rPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\rWhere:\r├─ pos = position in sequence (0, 1, 2, ...)\r├─ i = dimension index (0, 1, 2, ...)\r├─ d_model = 768 (hidden size)\rFor position 0:\r├─ PE(0, 0) = sin(0) = 0.0\r├─ PE(0, 1) = cos(0) = 1.0\rFor position 1:\r├─ PE(1, 0) = sin(1 / 10000^0) = sin(1) = 0.841\r├─ PE(1, 1) = cos(1 / 10000^0) = cos(1) = 0.540\rPattern: Different position → Different encoding!\rCan\u0026#39;t change two positions and keep encoding same! Pre-training Data and Scale Corpus BERT Pre-training Corpus:\rWikipedia: 13 GB\r├─ 2,500M words\r├─ High quality, diverse topics\r└─ But relatively small!\rBookCorpus: Not publicly released but ~12 GB\r├─ 800M words\r├─ Books written by people → Good grammar\r└─ Long documents (good for learning long-range dependencies)\rTotal: ~25 GB (3.3 billion word pieces)\r├─ This is the secret sauce!\r├─ Modern models use 100-1000x more data\r└─ More data = Better performance (scaling laws) Training Details BERT Pre-training:\rDuration: 4 days (BERT-base)\rHardware: 16 TPUv2 devices\rBatch size: 256 sequences (512 tokens each) = 131,072 tokens per batch\rLearning rate: 1e-4 with warmup\rOptimizer: Adam\rSteps: ~1,000,000 steps\rComputational cost:\r├─ 16 TPUs × 4 days = 64 TPU-days\r├─ ~$100,000+ in compute\r└─ That\u0026#39;s why we use pre-trained models now!\rAfter training:\r├─ Model can be used for fine-tuning\r├─ Just add classification head!\r└─ No need to train from scratch ever again Why These Tasks Work So Well MLM Advantages ✅ Bidirectional learning: Must use both sides ✅ Natural task: Similar to human reading ✅ Flexible: Can mask any proportion ✅ Hard enough: Non-trivial prediction\nNSP Advantages ✅ Discourse understanding: Learns sentence relationships ✅ Pair tasks: Helps with QA and NLI ✅ Classification format: Easy to extract feature for classification\nCombined ✅ Multi-task learning: Rich supervision signal ✅ Complementary: MLM + NSP teach different things ✅ Efficient: One training run, two learning signals\nComparison: Different Pre-training Objectives Word2Vec:\r├─ Task: Predict neighboring words\r├─ Result: Static embeddings\r└─ Problem: Same embedding everywhere\rELMo:\r├─ Task: Bidirectional language modeling\r├─ Result: Context-dependent embeddings\r└─ Problem: Still sequential processing\rGPT:\r├─ Task: Next-word prediction (left-to-right)\r├─ Result: Good for generation\r└─ Problem: Can\u0026#39;t see future context\rBERT (MLM + NSP):\r├─ Task: Masked prediction + sentence order\r├─ Result: Deep bidirectional understanding\r└─ Advantage: Best of both worlds!\rT5:\r├─ Task: Text-to-text with multiple objectives\r├─ Result: Unified framework for all tasks\r└─ Advantage: Simpler than two separate losses Key Takeaways ✅ MLM is the star: Most important for performance ✅ 15% masking rate: Empirically determined to be optimal ✅ Masking strategy: 80-10-10 makes model robust ✅ NSP helps but is secondary: Mainly for paired tasks ✅ Pre-training is expensive: But reusable! ✅ Simple ideas, powerful results: BERT\u0026rsquo;s genius\nWhat\u0026rsquo;s Next Day 49: How T5 scaled this idea (and dropped MLM+NSP) Day 50: Fine-tuning these pre-trained models The pre-training objectives are the foundation of modern NLP. Everything downstream depends on these two simple tasks!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section lists and introduces the blogs you have translated:\nBlog 1 – Accelerate Your Data and AI Flow by Connecting Amazon SageMaker Unified Studio to Visual Studio Code This article explains how to link a local Visual Studio Code environment with Amazon SageMaker Unified Studio so you can optimize data and AI development workflows. It walks through personalizing the IDE, accessing AWS Analytics and AI/ML services inside one unified environment, and configuring the remote connection prerequisites to build end-to-end data and AI pipelines.\nBlog 2 – Announcing Amazon EC2 M4 and M4 Pro Mac Instances This post introduces the newly released Amazon EC2 M4 and M4 Pro Mac instances that are built on the Apple M4 Mac mini and the AWS Nitro System. It covers the hardware configuration (Apple silicon M4/M4 Pro chips, multi-core CPU, GPU, Neural Engine), the 15–20% build-performance improvement, the new 2-TB local storage, and how to launch the instances via the AWS Management Console or CLI while integrating with other AWS services to build automated CI/CD pipelines.\nBlog 3 – Tuning Guide for AMD-based Amazon EC2 Instances This blog provides detailed guidance on optimizing Amazon EC2 instances that use AMD EPYC processors. It shows how to choose the right instance type based on workload characteristics (compute-intensive, big data and analytics, databases, web servers, AI/ML), explains 3rd- and 4th-generation AMD EPYC features, and outlines practical tuning methods—CPU, memory, and system parameters—to deliver the best price-to-performance ratio.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/3-blogstranslated/3.3-blog3/",
	"title": "Tuning Guide for AMD-based Amazon EC2 Instances",
	"tags": [],
	"description": "",
	"content": "Tuning Guide for AMD-based Amazon EC2 Instances By Suyash Nadkarni and Dylan Souvage — September 12, 2025 · Amazon EC2 · Best Practices · Expert (400) · Technical How-to\nAs organizations move more mission-critical workloads to the cloud, optimizing price-performance becomes essential. Amazon EC2 instances powered by AMD EPYC processors offer high core density, large memory bandwidth, and hardware-enabled security features, making them a strong fit for compute-, memory-, or I/O-intensive applications. This post explains how to choose the right AMD-based Amazon EC2 instance family and describes tuning techniques that improve workload efficiency—whether you run simulations, large-scale analytics, or inference jobs.\nAmazon EC2 offers AMD options across multiple EPYC generations. We focus on optimization strategies for 3rd- and 4th-generation processors, which are designed for compute- and memory-heavy workloads.\nGeneration 3 (M6a, R6a, C6a, Hpc6a): Balanced compute, memory, and storage—ideal for analytics, web servers, and HPC. Generation 4 (M7a, R7a, C7a, Hpc7a): Up to 50% more performance than prior AMD generations, with AVX-512 support, DDR5 memory, and Simultaneous Multithreading (SMT) disabled so each vCPU maps to a physical core for better isolation. Choose the right AMD EPYC instance family Selecting the correct AMD EPYC instance begins with understanding how your application consumes compute, memory, storage, and network resources. Each family is tailored to a specific profile.\nCompute-intensive workloads — large-scale numerical calculations, simulations, or encoding that require high CPU throughput and advanced instruction support.\nRecommended: C7a, Hpc7a, C6a, Hpc6a Use cases: scientific computing, financial modeling, media transcoding, encryption, ML inference Big Data \u0026amp; Analytics — large data processing and analytics that benefit from high memory bandwidth and balanced compute-to-memory ratios.\nRecommended: R7a, M7a, R6a, M6a Use cases: stream processing, real-time analytics, BI tools, distributed caching Database workloads — relational, NoSQL, or in-memory databases that need consistent memory performance and high I/O throughput.\nRecommended: R7a, M7a, R6a, M6a Use cases: MySQL, PostgreSQL, MongoDB, Cassandra, Redis Web and application servers — variable request workloads that need balanced compute, memory, and networking.\nRecommended: C7a, M7a, C6a, M6a Use cases: web servers, CMS platforms, e-commerce sites, API endpoints AI/ML on CPU — ML tasks that do not require GPUs (such as inference or preprocessing).\nRecommended: C7a, Hpc7a, M7a Use cases: fluid dynamics, genomics, seismic analysis, engineering simulations Matching the instance to the workload provides predictable performance and cost efficiency. Services like Amazon EC2 Auto Scaling and AWS Compute Optimizer can help with sizing and continuous scaling decisions.\nOptimize AMD EPYC-based Amazon EC2 instances 4th-generation AMD EPYC processors use a modular “chiplet” architecture. Each CPU is composed of multiple Core Complex Dies (CCD), and each CCD contains one or more core complexes (CCX). A CCX bundles up to eight physical cores; each core includes 1 MB of private L2 cache, and the eight cores share 32 MB of L3 cache. The CCDs connect to a central I/O die that manages memory and inter-chip links.\n(Die diagram: Zen 4 CPU with eight cores per die)\nThis modular design lets instances such as m7a.24xlarge and m7a.48xlarge expose very high core counts—up to 96 physical cores per socket. For example:\nm7a.24xlarge delivers 96 physical cores from a single socket. m7a.48xlarge spans two sockets for 192 physical cores. Understanding how EC2 instance sizes map to the underlying processor layout helps you optimize cache locality. Workloads that rely on shared-memory access or thread synchronization—such as HPC or in-memory databases—benefit from choosing sizes that minimize cross-socket communication and maximize L3-cache locality.\n(BLOCK diagram: EPYC chiplet layout)\n4th-generation AMD EPYC instances run with SMT disabled, so each vCPU maps directly to a physical core. This eliminates resource sharing between sibling threads (execution units, caches, etc.) and can reduce intra-core interference. The result is lower jitter and more consistent throughput for HPC, ML inference, and transactional database workloads.\nCPU optimizations Tools like htop reveal CPU usage patterns, system load averages, and per-process resource consumption. Evaluate CPU utilization in the context of workload requirements. Sustained utilization near 100% may indicate the workload is CPU-bound. Before resizing instances, enabling Auto Scaling, or switching families, analyze tuning opportunities that improve performance without infrastructure changes. Load averages that frequently exceed the vCPU count also signal saturation.\nUse the L3 cache effectively L3 cache is a fast shared cache accessible by a group of cores. On AMD-based EC2 instances, cores are grouped into L3 cache slices shared by a subset of cores on the same socket. Threads scheduled within the same slice access shared data more efficiently, reducing memory latency. On 4th-generation instances such as m7a.2xlarge or r7a.2xlarge, all vCPUs often map to cores within the same L3 slice. For larger sizes (m7a.8xlarge and above), thread pinning—assigning threads to specific physical cores—helps maintain locality and lowers performance variability.\ntaskset -c 0-3 ./your_application Use lscpu or lstopo to inspect the CPU topology and group related threads onto cores that share L3 cache.\nOptimize Docker containers By default, container runtimes such as Docker allow the OS scheduler to move containers across any CPU core. That flexibility can introduce variability when containers bounce between cores that do not share cache. Pin containers to specific cores with --cpuset-cpus to improve cache efficiency and reduce jitter:\ndocker run --cpuset-cpus=\u0026#34;1,3\u0026#34; my-container Choose cores based on the CPU topology so that containers stay on cores sharing the same L3 slice.\nSet the CPU frequency governor Some operating systems dynamically scale CPU frequency to save power via the CPU frequency governor. For latency-sensitive or compute-bound workloads, switch to performance mode so the CPU runs at max frequency under load:\nsudo cpupower frequency-set -g performance Benchmark with other governors (such as ondemand or schedutil) to confirm the performance mode delivers measurable gains without excessive power usage.\nUse architecture-specific compiler flags When compiling C/C++ applications, architecture flags such as -march=znverX enable AMD EPYC–specific optimizations (vectorization, floating-point throughput, etc.). Ensure that the compiler and target instance generation match the flag; binaries built with -march=znver4 will raise SIGILL on older instances like M5a.\nAMD EPYC generation -march flag Minimum GCC version Minimum LLVM/Clang version Generation 4 (M7a) znver4 GCC 12 Clang 15 Generation 3 (M6a) znver3 GCC 11 Clang 13 Generation 2 (M5a) znver2 GCC 9 Clang 11 Supported flags (GCC 11+ / Clang 13+):\nGeneration 4 (M7a, R7a, C7a, Hpc7a): -march=znver4 Generation 3 (M6a, R6a, C6a): -march=znver3 Generation 2 (M5a, R5a, C5a): -march=znver2 When to enable AVX-512 and VNNI 4th-generation AMD EPYC instances support SIMD instruction sets such as AVX2, AVX-512, and VNNI. These can boost throughput for vector-heavy workloads (ML inference, image processing, scientific simulations). Only enable them on generations that support the instructions to avoid SIGILL errors on older hardware.\ngcc -mavx2 -mavx512f -O2 your_program.c -o your_program\r# Investigate vectorization:\r-ftree-vectorizer-verbose=2 -fopt-info-vec-missed AMD Optimizing CPU Libraries (AOCL) AMD Optimizing CPU Libraries (AOCL) provide tuned math routines—vector, scalar, RNG, FFT, BLAS, LAPACK, and more—built specifically for AMD EPYC processors. Link your application against AOCL to leverage hardware optimizations without rewriting code.\nConfigure AOCL export AOCL_ROOT=/path/to/aocl\rgcc -I$AOCL_ROOT/include -L$AOCL_ROOT/lib -lamdlibm -lm your_program.c -o your_program\r# Vector math\rgcc -lamdlibm -fveclib=AMDLIBM -lm your_program.c -o your_program\r# Faster scalar math\rgcc -lamdlibm -fsclrlib=AMDLIBM -lamdlibmfast -lm your_program.c -o your_program\rexport AOCL_PROFILE=1\r./your_program Profiling generates aocl_profile_report.txt, which lists call counts, execution time, and thread usage so you can focus optimization on the hottest routines.\nConclusion We showed how to match AMD-based Amazon EC2 instance families to workload characteristics and how to apply tuning techniques focused on CPU utilization, thread placement, cache efficiency, and math libraries. These practices are especially valuable for CPU-bound or latency-sensitive workloads where consistent performance is critical.\nReady to get started? Sign in to the AWS Management Console and launch AMD EPYC–powered Amazon EC2 instances to begin optimizing your workloads today.\nTAGS: AMD\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/",
	"title": "Week 3 - AWS Compute Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-22 to 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 3 Overview This week highlighted AWS compute services, especially Amazon EC2 and supporting capabilities for scaling, storage, and pricing.\nKey Topics Amazon EC2 and instance families AMIs and backup strategies EBS volumes vs. Instance Store EC2 Auto Scaling patterns EC2 pricing models Amazon Lightsail, EFS, and FSx Hands-on Labs Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/",
	"title": "Day 04 - Cost Optimization on AWS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-11 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Cost Optimization on AWS Cost Optimization Strategies Choose the right resource types and Regions. Use pricing models such as Reserved Instances, Savings Plans, and Spot Instances. Remove or schedule idle resources. Leverage serverless architectures. Continuously review and improve cost efficiency with AWS Budgets and Cost Explorer. Tag resources with Cost Allocation Tags for department-level tracking. AWS Pricing Calculator calculator.aws\nCreate and share cost estimates for common services. Pricing varies by Region. Key Features:\nEstimate costs before deployment Compare pricing across regions Export and share estimates Template-based estimation AWS Support Plans Four tiers: Basic, Developer, Business, and Enterprise. Plans can be upgraded temporarily during critical incidents. Support Plan Comparison Feature Basic Developer Business Enterprise Cost Free $29/month $100/month $15,000/month Response Time N/A 12-24 hours 1 hour (urgent) 15 min (critical) Technical Support Forums only Business hours 24/7 24/7 + TAM Hands-On Labs Lab 07 – AWS Budgets \u0026amp; Cost Management Create Budget by Template → 07-01 Create Cost Budget Tutorial → 07-02 Create Usage Budget → 07-03 Create Reserved Instance (RI) Budget → 07-04 Create Savings Plans Budget → 07-05 Clean Up Budgets → 07-06 Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/",
	"title": "Day 09 - VPC Connectivity &amp; Load Balancing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-18 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Peering \u0026amp; Transit Gateway VPC Peering Enables direct, private connectivity between two VPCs without traversing the Internet. Does not support transitive routing or overlapping CIDRs. VPC Peering Limitations:\nNo transitive peering No overlapping CIDR blocks Limited to 125 peering connections per VPC Cross-region peering supported AWS Transit Gateway (TGW) Acts as a hub to connect multiple VPCs and on-prem networks, simplifying complex mesh topologies. TGW Attachments associate subnets in specific AZs with a TGW. All subnets within the same AZ can reach the TGW once attached. Transit Gateway Benefits:\nCentralized connectivity hub Simplified network architecture Scalable to thousands of VPCs Supports inter-region peering VPN \u0026amp; Direct Connect Site-to-Site VPN Establishes a secure IPSec connection between an on-premises data center and AWS VPC. Consists of: Virtual Private Gateway (VGW): AWS-managed, multi-AZ endpoints. Customer Gateway (CGW): Customer-managed device or software appliance. AWS Direct Connect Provides a dedicated private network connection between an on-prem data center and AWS. Typical latency: 20–30 ms. In Vietnam, available through Hosted Connections (via partners). Bandwidth is adjustable. Hands-On Labs Lab 10 – Hybrid DNS (Route 53 Resolver) Generate Key Pair → 10-02.1 Initialize CloudFormation Template → 10-02.2 Configure Security Group → 10-02.3 Set up DNS System → 10-05 Create Route 53 Outbound Endpoint → 10-05.1 Create Resolver Rules → 10-05.2 Create Inbound Endpoints → 10-05.3 Lab 19 – VPC Peering Initialize CloudFormation Templates → 19-02.1 Create Security Group → 19-02.2 Create EC2 Instance (Test Peering) → 19-02.3 Create Peering Connection → 19-04 Configure Route Tables (Cross-VPC) → 19-05 Enable Cross-Peer DNS → 19-06 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/",
	"title": "Day 14 - EC2 Auto Scaling",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-25 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon EC2 Auto Scaling EC2 Auto Scaling automatically adjusts the number of EC2 instances based on demand. Benefits\nElastic capacity adjustment Increased application availability Cost optimization Components\nAuto Scaling Group (ASG) – logical group of EC2 instances Launch Template / Configuration – defines instance parameters Scaling Policies – rules for adding/removing instances Scaling Policies Simple / Step Scaling – add/remove instances when thresholds are met Target Tracking – maintain a metric (e.g., CPU = 50%) Scheduled Scaling – scale on a predefined schedule Predictive Scaling – uses ML to forecast and scale proactively Scaling Policy Examples:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Integration with Load Balancer ASGs often pair with Elastic Load Balancers (ELB). New instances automatically register; terminated instances deregister automatically. Auto Scaling Best Practices:\nUse multiple AZs for high availability Set appropriate cooldown periods Monitor CloudWatch metrics Use lifecycle hooks for custom actions Test scaling policies before production EC2 Pricing Options On-Demand: Pay per hour/second. Most expensive but flexible. Reserved Instances: 1- or 3-year commitment for discount; tied to specific instance type/family. Savings Plans: 1- or 3-year commitment; flexible across instance families. Spot Instances: Use spare capacity at up to 90% discount; can be terminated with 2-minute notice. Combine multiple pricing models within an Auto Scaling Group for cost optimization.\nPricing Comparison:\nModel Discount Flexibility Commitment On-Demand 0% High None Reserved 40-60% Low 1-3 years Savings Plans 40-60% Medium 1-3 years Spot 50-90% Low None Hands-On Labs Lab 09 – AWS Support Plans AWS Support Packages → 09-01 Types of Support Requests → 09-02 Change Support Package → 09-03 Manage Support Requests → 09-04 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/",
	"title": "Day 19 - Disaster Recovery on AWS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-02 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Disaster Recovery (DR) on AWS Disaster Recovery is about restoring IT services after major incidents (outages, disasters, hardware failures, cyberattacks).\nRTO (Recovery Time Objective): How quickly to restore service. RPO (Recovery Point Objective): How much data loss (time window) is acceptable. DR Strategies (ordered by complexity \u0026amp; cost) Backup \u0026amp; Restore\nMaintain backups only (EBS/RDS snapshots, S3/Glacier). Restore to new infrastructure during incidents. RTO: hours–days. RPO: depends on backup frequency. Cost: lowest. Pilot Light\nMinimal core services always running on AWS. Scale out to full production during DR. RTO: hours. RPO: minutes. Cost: moderate. Warm Standby\nFull system running at reduced scale on AWS. Scale up on failover. RTO: minutes–hours. RPO: seconds–minutes. Cost: higher. Multi-Site (Active/Active or Active/Passive)\nProduction running across on-prem and AWS, or multi-Region AWS. Traffic can be shifted instantly (Route 53, Global Accelerator). RTO/RPO: near zero. Cost: highest. DR Strategy Comparison:\nStrategy RTO RPO Cost Complexity Backup \u0026amp; Restore Hours-Days Hours $ Low Pilot Light Hours Minutes $$ Medium Warm Standby Minutes Seconds $$$ Medium-High Multi-Site Seconds Near-zero $$$$ High DR Best Practices Planning Define RTO and RPO requirements Document recovery procedures Identify critical systems and dependencies Establish communication plans Implementation Automate recovery processes Use multiple AZs and Regions Implement data replication Regular backup testing Testing Conduct DR drills regularly Test recovery procedures Measure actual RTO/RPO Update documentation Hands-On Labs Lab 14 – AWS VM Import/Export (Part 2) Import Virtual Machine to AWS → 14-02.3 Deploy Instance from AMI → 14-02.4 Set Up S3 Bucket ACL → 14-03.1 Export Virtual Machine from Instance → 14-03.2 Resource Cleanup on AWS → 14-05 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/",
	"title": "Day 24 - SCPs, Identity Center &amp; KMS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-09 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Service Control Policies (SCPs) Define maximum permissions for accounts; they limit but do not grant permissions. Apply to accounts or OUs; affect all users/roles, including root; Deny overrides Allow. Example SCP (deny bucket deletion)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } SCP Use Cases:\nPrevent accounts from leaving the organization Restrict regions where resources can be created Enforce encryption requirements Prevent disabling security services Require specific tags on resources SCP Best Practices:\nStart with least privilege Test in non-production first Use explicit denies for critical controls Document SCP purposes Regular review and updates AWS Identity Center (formerly AWS SSO) Centralizes access to AWS accounts and external applications. Identity sources: built-in, AWS Managed Microsoft AD, on-prem AD (trust/AD Connector), or external IdPs. Permission Sets define what users/groups can do in target accounts (materialized as IAM roles). Multiple permission sets per user are supported. Identity Center Features:\nSingle sign-on to multiple AWS accounts Integration with Microsoft Active Directory SAML 2.0 support Multi-factor authentication Centralized permission management Audit logging with CloudTrail AWS Key Management Service (KMS) Managed keys for data protection with deep service integration and full auditability. Highlights\nCreate/manage keys without operating your own HSM infrastructure. Fine-grained access via IAM \u0026amp; key policies; usage logged in CloudTrail. Key categories\nCustomer-managed keys, AWS-managed keys, and AWS-owned keys. KMS Key Types:\nSymmetric: Single encryption key (AES-256) Asymmetric: Public/private key pair (RSA, ECC) KMS Features:\nAutomatic key rotation Key policies and grants Envelope encryption Integration with AWS services CloudTrail logging Multi-region keys Hands-On Labs Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 1) Create Policy and Role → 33-2.1 Create Group and User → 33-2.2 Create KMS Key → 33-3 Create S3 Bucket → 33-4.1 Upload Data to S3 → 33-4.2 Lab 30 – IAM Restriction Policy Create Restriction Policy → 30-3 Create IAM Limited User → 30-4 Test IAM User Limits → 30-5 Clean Up Resources → 30-6 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/",
	"title": "Day 29 - Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-16 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon ElastiCache Managed in-memory caching service for Redis and Memcached to reduce latency and offload databases.\nMicrosecond reads, Multi-AZ with failover, simple scaling, encryption/auth, automated ops. Redis: rich data structures, backups, replication, cluster mode. Memcached: simple, horizontally scalable cache with auto-discovery. Common uses: web/mobile acceleration, DB query caching, session stores, leaderboards, pub/sub, queues.\nElastiCache for Redis Features:\nData Structures: Strings, lists, sets, sorted sets, hashes, bitmaps, hyperloglogs Persistence: Snapshots and AOF (Append-Only File) Replication: Primary-replica with automatic failover Cluster Mode: Partition data across multiple shards Pub/Sub: Real-time messaging Lua Scripting: Server-side scripting Geospatial: Location-based queries ElastiCache for Memcached Features:\nMulti-threaded: Utilize multiple cores Auto Discovery: Automatic node discovery Horizontal Scaling: Add/remove nodes easily Simple: Easy to use, no persistence Redis vs Memcached:\nFeature Redis Memcached Data Structures Rich (lists, sets, etc.) Simple (key-value) Persistence Yes No Replication Yes No Multi-AZ Yes No Backup/Restore Yes No Pub/Sub Yes No Multi-threaded No Yes Caching Strategies Cache-Aside (Lazy Loading) Application checks cache first On miss, load from database and populate cache Pros: Only requested data is cached Cons: Cache miss penalty, stale data possible Write-Through Write to cache and database simultaneously Pros: Data always fresh, no cache misses on reads Cons: Write penalty, unused data may be cached Write-Behind (Write-Back) Write to cache immediately, async write to database Pros: Fast writes, reduced database load Cons: Risk of data loss, complexity Use Cases Session Store:\n# Store user session in Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Retrieve session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Add score to sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Get top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Increment counter with expiry pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Hands-On Labs Lab 43 – AWS Database Migration Service (DMS) (Part 3) Inspect S3 → 43-12 Create Serverless Migration → 43-13 Create Event Notification → 43-14 Logs → 43-15 Troubleshoot: Memory Pressure → 43-16 Troubleshoot: Table Error → 43-17 "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/1.7.4-day34-2025-10-23/",
	"title": "Day 34 - FastAPI Clean Architecture",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-23 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Clean Architecture Overview Separate config, models, routes, and core logic to simplify scaling. Keep main.py lightweight—only boot the app, load config, and mount routers. Use Pydantic models to standardize request/response shapes so the contract matches OpenAPI. backend/\r├── main.py\r├── core/\r│ └── config.py\r├── models/\r│ └── book.py\r├── routes/\r│ └── books.py\r└── services/\r└── books.py Configuration \u0026amp; Dependencies core/config.py reads environment variables and centralizes CORS, API prefixes, and debug flags. Use FastAPI’s dependency injection to pass the service layer into routers. Enables swapping data sources (in-memory → PostgreSQL) without changing function contracts. CORS \u0026amp; API Stability Restrict CORS to required origins (http://localhost:3000 during development). Allow only GET for the first slice to shrink the attack surface. Keep /openapi.json accessible so contract-testing tools can pull the spec. Start Simple, Refactor Later Begin with an in-memory repository for fast demos, then add a real database. Document TODOs clearly to avoid losing them between sprints. Keep logging minimal and focus on critical faults (timeouts, data mismatches). Hands-On Labs Refactor main.py so it only handles app bootstrapping and router registration. Build the get_book_detail(id) service layer using spec-driven fake data. Configure CORSMiddleware to match the frontend mock and production URLs. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/1.8.4-day39-2025-10-30/",
	"title": "Day 39 - NMT &amp; Text Summarization",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-30 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nNeural Machine Translation (NMT) Architecture Overview The input sentence is converted to a numerical representation and encoded into a deep representation by a six-layer encoder, which is subsequently decoded by a six-layer decoder into the translation in the target language.\nEncoder and Decoder Layers Layers consist of:\nSelf-attention: Helps model focus on different parts of input Feed-forward layers: Process information Encoder-decoder attention layer (decoder only): Uses deep representation from last encoder layer Attention Mechanism Example Translation Task: \u0026ldquo;The woman took the empty magazine out of her gun\u0026rdquo;\nTarget Language: Czech\nVisualization of Self-Attention When translating \u0026ldquo;magazine\u0026rdquo;, the attention mechanism:\nCreates strong attention link between \u0026lsquo;magazine\u0026rsquo; and \u0026lsquo;gun\u0026rsquo; This helps correctly translate \u0026ldquo;magazine\u0026rdquo; as \u0026ldquo;zásobník\u0026rdquo; (gun magazine) Instead of \u0026ldquo;časopis\u0026rdquo; (news magazine) Why Attention Matters Attention = mechanism that helps model focus on the most important parts of input when generating output\nIn other words: Attention = selective information processing instead of consuming everything at once\nIn NLP, attention allows the model to decide which words most strongly influence understanding another word in the sentence.\nNMT Implementation Details Model Architecture Components: Inputs: Input tokens (source language) Target tokens (target language) Step 1: Make Copies Create two copies each of input and target tokens (needed in different places of model)\nStep 2: Encoder One copy of input tokens → encoder Transform into key and value vectors Go through embedding layer → LSTM Step 3: Pre-attention Decoder One copy of target tokens → pre-attention decoder Shift sequence right + add start-of-sentence token (teacher forcing) Go through embedding layer → LSTM Output becomes query vectors Note: Encoder and pre-attention decoder can run in parallel (no dependencies)\nStep 4: Prepare for Attention Get query, key, value vectors Create padding mask to identify padding tokens Use copy of input tokens for this step Step 5: Attention Layer Pass queries, keys, values, and mask to attention layer\nOutputs context vectors and mask Step 6: Post-attention Decoder Drop mask, pass context vectors through:\nLSTM Dense layer LogSoftmax Step 7: Output Model returns:\nLog probabilities Copy of target tokens (for loss computation) Text Summarization Summarization = condensing content while preserving main ideas\nTwo Types: 1. Extractive Summarization Concept: Select the most important sentences from original text\nCharacteristics:\nDoesn\u0026rsquo;t rewrite text Preserves original wording Like \u0026ldquo;highlighting key sentences\u0026rdquo; Process (Classical TextRank):\nSplit into sentences Convert sentences to embeddings Calculate similarity (cosine) Create graph (sentences as nodes) Rank using TextRank Select top-ranked sentences Result: Subset of original text\n2. Abstractive Summarization Concept: Rewrite main ideas in new sentences\nCharacteristics:\nCreates sentences that never appeared in original Understands content → paraphrases Requires strong models (seq2seq, Transformer) Example: Original article discusses prosecutor\u0026rsquo;s investigation process\u0026hellip;\nGenerated summary:\n\u0026ldquo;Prosecutor: So far no videos were used in the crash investigation.\u0026rdquo;\nThis sentence doesn\u0026rsquo;t exist in original but captures the main idea.\nExtractive vs Abstractive Summary Feature Extractive Abstractive Approach Select existing sentences Generate new sentences Creativity Low High Complexity Simpler More complex Accuracy More faithful to source May introduce errors Model TextRank, graph-based Seq2seq, Transformer TextRank Pipeline Step-by-step extractive summarization:\nCombine articles → full text Split sentences Convert sentences → vectors (embeddings) Create similarity matrix Build graph (sentences = nodes, edges = similarity) Rank nodes using TextRank algorithm Select top-ranked sentences → Summary This is the classical algorithm that dominated before deep learning!\nSyntax and Semantics Review Syntax – Sentence Structure Syntax examines how words combine to form grammatically correct sentences.\nIncludes: Word order: English uses S–V–O (Subject–Verb–Object) Phrase structure: NP (Noun Phrase), VP (Verb Phrase), PP (Prepositional Phrase) Dependency relations: How words relate to each other NLP Relevance: POS tagging Parsing Named Entity Recognition Machine translation Question answering Semantics – Meaning of Words and Sentences Semantics focuses on meaning independent of external context.\nIncludes: Lexical semantics: Word meaning Compositional semantics: Sentence meaning Synonymy / antonymy: Similar/opposite meanings Hypernymy / hyponymy: General/specific relationships NLP Relevance: Word embeddings Similarity measures Semantic search Text classification Pragmatics – Intended Meaning in Context Pragmatics studies meaning from context, speaker intention, and real-world knowledge.\nCovers: Implicature: Hidden meaning Deixis: Context-dependent references (this/that/here/you) Speech acts: Promises, requests, apologies Politeness, formality, sarcasm: Tone and intention NLP Relevance: Dialogue systems Chatbots Sentiment and irony detection Contextual language models (BERT, GPT) "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/1.9.4-day44-2025-11-06/",
	"title": "Day 44 - Attention Types: Self, Masked, and Encoder-Decoder",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-06 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nThree Types of Attention in Transformers The transformer uses attention in three different ways. Understanding each is crucial.\nType 1: Self-Attention (Encoder) Definition: Each position attends to all positions in the same sequence.\nUse Case: In the encoder, we want each word to understand its context by looking at all other words.\nExample:\nSentence: \u0026#34;The cat sat on the mat\u0026#34;\rFor word \u0026#34;cat\u0026#34;:\r- Attend to \u0026#34;The\u0026#34;: 0.15 (article)\r- Attend to \u0026#34;cat\u0026#34;: 0.40 (itself)\r- Attend to \u0026#34;sat\u0026#34;: 0.20 (verb)\r- Attend to \u0026#34;on\u0026#34;: 0.10\r- Attend to \u0026#34;the\u0026#34;: 0.08\r- Attend to \u0026#34;mat\u0026#34;: 0.07\rResult: \u0026#34;cat\u0026#34; context = weighted combination of all 6 words Why useful:\nCaptures full sentence context Can identify relationships (subject-verb, adjective-noun, etc.) Each word gets information from entire sentence Implementation:\nQ = K = V = Input (same source!)\rattention(Q, K, V) = softmax(Q×K^T / √d_k) × V Since Q, K, V come from the same place, it\u0026rsquo;s called \u0026ldquo;self-attention\u0026rdquo;.\nType 2: Masked Self-Attention (Decoder) Problem: During training, if the decoder can \u0026ldquo;see\u0026rdquo; future words, it cheats!\nExample - The Problem:\nTask: Translate \u0026#34;Je suis heureux\u0026#34; → \u0026#34;I am happy\u0026#34;\rTraining:\rStep 1: Predict \u0026#34;am\u0026#34; using... \u0026#34;am\u0026#34; (it can see the answer!)\rStep 2: Predict \u0026#34;happy\u0026#34; using \u0026#34;I am happy\u0026#34; (knows the answer!)\rStep 3: Predict \u0026#34;happy\u0026#34; is done (cheating!)\rResult: Model trains perfectly but fails at test time! Solution: Mask (hide) future positions during self-attention.\nMasked Self-Attention:\nInstead of: We do:\r[0.30, 0.33, 0.37] [0.30, -∞, -∞]\r[0.26, 0.37, 0.37] → [0.26, 0.37, -∞]\r[0.25, 0.36, 0.39] [0.25, 0.36, 0.39]\rAfter softmax:\r[1.00, 0.00, 0.00]\r[0.30, 0.70, 0.00]\r[0.25, 0.36, 0.39]\r(normalized) Mask Matrix:\nMask = [1, 0, 0]\r[1, 1, 0]\r[1, 1, 1]\rOr: -∞ for masked positions Effect:\nPosition 0: Attends to position 0 only\rPosition 1: Attends to positions 0, 1 only\rPosition 2: Attends to positions 0, 1, 2\rDecoder can only use past information! Why this works:\nDuring training, can use autoregressive generation During inference, generates word-by-word naturally Prevents the model from \u0026ldquo;seeing the answer\u0026rdquo; Type 3: Encoder-Decoder Attention Purpose: Decoder attends to encoder output.\nExample:\nEncoder processes: \u0026#34;Je suis heureux\u0026#34; (French)\rProduces: Context vectors C\rDecoder processes: \u0026#34;\u0026#34; (empty start)\rFor generating first word:\r- Query: from decoder (what should I translate?)\r- Key, Value: from encoder (what French words should I look at?)\rResult: Decoder attends to French words to generate English Key Difference from Self-Attention:\nSelf-Attention: Encoder-Decoder:\rQ, K, V all from input Q from decoder\rSame sequence K, V from encoder\rAttends within self Attends to other sequence Use Cases:\nDecoder looking back at encoder output Allows translation: French → English Allows summarization: Document → Summary Generally useful for seq2seq tasks Comparison: All Three Types Type Q Source K, V Source Purpose Self-Attention Input Input Understand context within same sequence Masked Self-Attention Input Input (masked future) Autoregressive generation, prevent cheating Encoder-Decoder Decoder Encoder Cross-sequence understanding Masked Attention in Detail The Math Before masking:\nAttention = softmax(Q×K^T / √d_k) × V With masking:\nScores = Q×K^T / √d_k\rMask matrix M:\rM[i,j] = 0 if j \u0026lt;= i (allowed)\rM[i,j] = -∞ if j \u0026gt; i (mask future)\rMasked_scores = Scores + M\rAttention = softmax(Masked_scores) × V Example with Real Numbers Original attention scores (3×3):\n[0.1, 0.2, 0.3]\r[0.4, 0.5, 0.6]\r[0.7, 0.8, 0.9] Mask matrix:\n[0, -∞, -∞]\r[0, 0, -∞]\r[0, 0, 0] After adding mask:\n[0.1, -∞, -∞]\r[0.4, 0.5, -∞]\r[0.7, 0.8, 0.9] After softmax (applying exp and normalize):\nexp(0.1) / exp(0.1) = 1.0, softmax([0.1]) = [1.0]\rSo:\rRow 0: [1.0, 0, 0]\rexp(0.4) ≈ 1.49, exp(0.5) ≈ 1.65\rRow 1: [1.49/(1.49+1.65), 1.65/(1.49+1.65), 0] ≈ [0.47, 0.53, 0]\rRow 2: softmax([0.7, 0.8, 0.9]) (all allowed) Final attention weights:\n[1.0, 0.0, 0.0]\r[0.47, 0.53, 0.0]\r[0.25, 0.33, 0.42] Key insight: Position 2 can only use information from positions 0, 1, 2 (not future)\nComplete Transformer Attention Flow INPUT: \u0026#34;Je suis heureux\u0026#34;\r↓\rENCODER LAYERS (repeat 6 times):\r├─ Self-Attention: Each French word attends to all French words\r├─ Feed-Forward\r→ Output: C (French context vectors)\rDECODER LAYERS (repeat 6 times):\r├─ Masked Self-Attention: Each generated word attends to previous words\r├─ Encoder-Decoder Attention: Generated word attends to French context\r├─ Feed-Forward\r→ Output: Logits for next word prediction\rOUTPUT: \u0026#34;I am happy\u0026#34; Key Insights ✅ Self-Attention: Bidirectional understanding (encoder) ✅ Masked Attention: Unidirectional generation (decoder) ✅ Encoder-Decoder: Cross-sequence transfer ✅ Masking prevents cheating: Model can\u0026rsquo;t use future information\nWhy Not Always Use All Three? BERT (Encoder-only): Uses only self-attention (bidirectional, good for classification) GPT (Decoder-only): Uses only masked self-attention (autoregressive, good for generation) T5 (Full): Uses all three (balanced, good for seq2seq tasks) Next: Implementation Now that we understand the three attention types, we\u0026rsquo;ll see how to implement them in code!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/1.10.4-day49-2025-11-13/",
	"title": "Day 49 - T5: Text-to-Text Transfer Transformer",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-13 (Thursday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nT5: The Ultimate Transfer Learning Model Published by Google in 2019, T5 (Text-to-Text Transfer Transformer) unified ALL NLP tasks into one framework.\nThe key insight: Every NLP task can be phrased as text-to-text.\nThe T5 Revolution Before T5: Task-Specific Models Sentiment Analysis:\rInput: \u0026#34;I love this movie!\u0026#34;\rOutput: Positive (classification)\rModel: bert-sentiment-specific\rMachine Translation:\rInput: \u0026#34;I love this movie!\u0026#34;\rOutput: \u0026#34;J\u0026#39;adore ce film!\u0026#34;\rModel: marianmt-en-fr\rSummarization:\rInput: \u0026#34;The movie was great... [1000 words]\u0026#34;\rOutput: \u0026#34;Great movie.\u0026#34;\rModel: bart-summarization\rQuestion Answering:\rInput: \u0026#34;What is the capital?\u0026#34; + Wikipedia passage\rOutput: \u0026#34;Paris\u0026#34; (span extraction)\rModel: bert-qa-specific\rProblem: Different input/output types, different models! After T5: Unified Framework Sentiment Analysis:\rInput: \u0026#34;sentiment: I love this movie!\u0026#34;\rOutput: \u0026#34;positive\u0026#34;\rMachine Translation:\rInput: \u0026#34;translate English to French: I love this movie!\u0026#34;\rOutput: \u0026#34;J\u0026#39;adore ce film!\u0026#34;\rSummarization:\rInput: \u0026#34;summarize: The movie was great... [1000 words]\u0026#34;\rOutput: \u0026#34;Great movie.\u0026#34;\rQuestion Answering:\rInput: \u0026#34;question: What is the capital? context: Paris is...\u0026#34;\rOutput: \u0026#34;Paris\u0026#34;\rSolution: ONE model, ONE architecture, ONE training procedure!\rJust change the task prefix! T5 Architecture Encoder-Decoder Transformer T5 (Both Fully Bidirectional):\rInput: \u0026#34;translate English to French: Hello world\u0026#34;\r↓\rEncoder (12 layers of bidirectional transformers)\r├─ Layer 1: Self-attention on all tokens\r├─ Layer 2: Self-attention on all tokens\r├─ ...\r├─ Layer 12: Self-attention on all tokens\r└─ Output: Contextual representations\r↓\rDecoder (12 layers, can attend to encoder)\r├─ Layer 1: Self-attention (masked), Encoder-attention\r├─ Layer 2: Self-attention (masked), Encoder-attention\r├─ ...\r├─ Layer 12: Self-attention (masked), Encoder-attention\r└─ Output: \u0026#34;Bonjour le monde\u0026#34;\rKey Difference from BERT:\r├─ BERT: Encoder only (understands)\r├─ T5: Encoder + Decoder (understands and generates)\r└─ GPT: Decoder only (generates) Model Sizes T5-small: 60 million parameters\r├─ Encoder: 6 layers, hidden size 512\r├─ Decoder: 6 layers, hidden size 512\r└─ Fast, small memory footprint\rT5-base: 220 million parameters\r├─ Encoder: 12 layers, hidden size 768\r├─ Decoder: 12 layers, hidden size 768\r└─ Good balance\rT5-large: 770 million parameters\r├─ Encoder: 24 layers, hidden size 1024\r├─ Decoder: 24 layers, hidden size 1024\r└─ High performance\rT5-3B and T5-11B: Even larger\r├─ Used for very challenging tasks\r└─ Impressive performance on everything The Task Prefix System The genius of T5: Simple task prefixes guide the model\nTask Prefix Examples:\r1. Classification (Sentiment):\rInput: \u0026#34;sentiment: I loved this movie!\u0026#34;\rOutput: \u0026#34;positive\u0026#34;\r2. Translation:\rInput: \u0026#34;translate English to French: Hello\u0026#34;\rOutput: \u0026#34;Bonjour\u0026#34;\r3. Summarization:\rInput: \u0026#34;summarize: Long document... [500 words] ...end\u0026#34;\rOutput: \u0026#34;Summary of key points\u0026#34;\r4. Question Answering:\rInput: \u0026#34;question: Who wrote Hamlet? context: Shakespeare...\u0026#34;\rOutput: \u0026#34;Shakespeare\u0026#34;\r5. Paraphrasing:\rInput: \u0026#34;paraphrase: The cat is on the mat.\u0026#34;\rOutput: \u0026#34;A feline rests on the floor mat.\u0026#34;\r6. Entailment:\rInput: \u0026#34;nli: A person is riding a bike. A man cycles.\u0026#34;\rOutput: \u0026#34;entailment\u0026#34; or \u0026#34;neutral\u0026#34; or \u0026#34;contradiction\u0026#34;\r7. Zero-shot Classification:\rInput: \u0026#34;znli: I liked it. premise: The speaker liked something.\u0026#34;\rOutput: \u0026#34;entailment\u0026#34;\r8. Multiple Choice:\rInput: \u0026#34;multiple_choice: Paris is the capital of?\r(A) France (B) Germany (C) Italy\u0026#34;\rOutput: \u0026#34;A\u0026#34;\rWhy this works:\r├─ Same model learns all these patterns\r├─ Task prefix acts as instruction\r├─ Scalable to any text-in/text-out task\r└─ Elegant! T5 Pre-training: SPAN CORRUPTION T5 introduced a new pre-training objective: Span Corruption\nHow It Works Original: \u0026#34;The quick brown fox jumps over the lazy dog.\u0026#34;\rSpan Corruption Process:\rStep 1: Randomly select spans to corrupt\r├─ Select 15% of tokens\r├─ Group into spans\r└─ Example: [quick brown] and [lazy]\rStep 2: Replace spans with sentinel tokens\r├─ \u0026#34;The [X] fox jumps over the [Y] dog.\u0026#34;\r├─ [X] is a unique placeholder for first span\r├─ [Y] is a unique placeholder for second span\rStep 3: Predict the missing spans\rInput: \u0026#34;The [X] fox jumps over the [Y] dog.\u0026#34;\rOutput: \u0026#34;[X] quick brown [Y] lazy [Z]\u0026#34;\r(including start token [Z])\rTask: Reconstruct corrupted spans! Why Span Corruption \u0026gt; MLM BERT\u0026#39;s MLM:\r├─ Mask individual tokens: \u0026#34;The [MASK] brown [MASK] jumps\u0026#34;\r├─ Predict each independently\r├─ Problem: Easier than real text corruption\r└─ Tokens are not correlated\rT5\u0026#39;s Span Corruption:\r├─ Corrupt word-level spans: \u0026#34;The [X] [Y] fox jumps\u0026#34;\r├─ Generate entire spans\r├─ Advantage: Harder, more realistic\r└─ Spans are correlated (important for generation!)\rResult: T5 is better at generation tasks! Mathematical Formulation Span corruption loss:\rFor each corrupted span s with length L:\rLoss = -∑(log P(y_i | y_\u0026lt;i, x))\rWhere:\r├─ y_i = token i of the corrupted span\r├─ y_\u0026lt;i = previously generated tokens\r├─ x = input with corrupted spans\r└─ Sum over all tokens in span\rThis is essentially language modeling loss\rBut on spans instead of individual tokens! T5 Pre-training Data The C4 Corpus T5 was trained on C4: Colossal Clean Crawled Corpus\nC4 Statistics:\r├─ Source: 750 billion web documents\r├─ Size: 800 GB of text\r├─ 200 billion tokens\r├─ Processed: Deduplicated, filtered, cleaned\r├─ Languages: Primarily English (but multilingual versions exist)\rHow it was created:\r├─ Take Common Crawl (web snapshots)\r├─ Filter: Remove bad content\r├─ Clean: Fix HTML, remove boilerplate\r├─ Deduplicate: Remove near-duplicates\r├─ Result: High-quality 800GB corpus!\rComparison:\r├─ BERT pre-training: 3.3 billion word pieces (~13GB)\r├─ T5 pre-training: 200 billion tokens (~800GB)\r└─ T5 trained on 60x more data! Multi-task Pre-training T5 introduced pre-training on multiple tasks simultaneously\nDuring pre-training, T5 sees diverse objectives:\r50% Span Corruption (main task):\r├─ \u0026#34;The [X] fox jumps over [Y] dog\u0026#34;\r├─ Predict: \u0026#34;[X] quick brown [Y] lazy\u0026#34;\r50% Task-specific objectives:\r├─ Translate (15%): \u0026#34;translate en to fr: Hello\u0026#34;\r├─ Summarize (15%): \u0026#34;summarize: Article... \u0026#34;\r├─ QA (15%): \u0026#34;question: What... context: ...\u0026#34;\r├─ Other tasks (5%): Various NLP tasks\rWhy?\r├─ Exposes model to task diversity early\r├─ Better transfer to downstream tasks\r├─ Model learns that same weights work for multiple things\r└─ Results in more generalizable representations T5 Comparison with BERT T5 BERT\r── ────\rArchitecture Enc-Dec Enc only\rMask Type Span Token\rPre-train Data 800GB (C4) 13GB (Wiki+Books)\rScale 220M-11B 110M-340M\rGeneration Excellent Can\u0026#39;t generate\rUnderstanding Good Excellent\rTraining Time ~31 days (TPU) ~4 days (TPU)\rBest For Generation Understanding\rFlexibility Text-to-text Task-specific fine-tuning\rWhich to use?\r├─ Translation: T5 (handles sequences better)\r├─ Classification: BERT (faster, simpler)\r├─ Summarization: T5 (generation task)\r├─ Sentiment: BERT (classification task)\r├─ QA: Both work, but T5 is more flexible T5 Performance GLUE Benchmark (Understanding) Task T5-base Best Previous\rSentiment (SST-2) 94.9% 92.1%\rSimilarity (STS-B) 89.2% 87.1%\rInference (RTE) 93.5% 91.0%\rQuestion Answering (QNLI) 95.1% 93.2%\rParaphrase (MRPC) 89.2% 87.1%\rAverage improvement: +2-3% BLEU Score (Generation) Task T5-base Best Previous\rEnglish to German 28.4 23.1\rEnglish to French 41.0 35.2\rEnglish to Romanian 34.2 29.1\rT5 is much better at generation! T5 Variants T5\r├─ T5-small (60M)\r├─ T5-base (220M)\r├─ T5-large (770M)\r├─ T5-3B\r├─ T5-11B\r└─ mT5 (multilingual, 13 variants)\rEach scales to different trade-offs:\r├─ Smaller = Faster, less memory\r└─ Larger = Better performance, more parameters Why T5 Matters ✅ Unification: One model, one architecture, many tasks ✅ Text-to-text simplicity: Don\u0026rsquo;t design task-specific architectures ✅ Span corruption: Better pre-training objective ✅ Large data: 800GB corpus shows scaling benefits ✅ Flexible: Can tackle any generation task ✅ Strong baselines: Best performance on many benchmarks\nKey Takeaways Task Prefixes: Simple but powerful way to guide models Span Corruption: Better than token masking for generation Large Pre-training Data: More data = better transfer Encoder-Decoder: Perfect for seq2seq tasks Unified Framework: Simplifies NLP systems Evolution of Transfer Learning Word2Vec (2013) → ELMo (2015) → BERT (2018) → T5 (2019) → GPT-3 (2020) → Present\rScale: Simple → Medium → Large → Very Large → Huge → Massive\rArchitecture: Embeddings → BiLSTM → BiTransformer → Enc-Dec → Dec-only\rData: Millions → Billions → 13GB → 800GB → 570GB → Trillions?\rPerformance: Basic → Good → Excellent → Better → Amazing → Superhuman? Next: Fine-tuning Day 50: How to take T5 (or BERT) and fine-tune for specific tasks T5 is the ultimate generalist model. With just a task prefix and 100-1000 examples, you can build systems that work better than specialist models from 2 years ago!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Throughout my internship, I had the opportunity to participate in several impactful events that enriched my professional journey with valuable insights and memorable experiences.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 – 17:00 VNT, Thursday, September 18, 2025\nLocation: 36th Floor, 2 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Vietnam Cloud Day 2025 was a comprehensive AWS event featuring keynote addresses from government speakers, AWS executives, and industry leaders. The event included two main tracks: a live telecast track with keynotes and panel discussions on GenAI revolution and executive leadership, and breakout tracks covering topics such as unified data foundations for AI/analytics, GenAI adoption roadmaps, AI-driven development lifecycle, securing GenAI applications, and AI agents for productivity. The event showcased AWS\u0026rsquo;s latest services and strategic initiatives for AI and cloud modernization.\nOutcomes: Gained insights into enterprise-scale AI adoption strategies, learned about AWS services for data foundation and GenAI implementation, and understood best practices for securing AI applications and modernizing legacy systems.\nEvent 2 Event Name: AWS GenAI Builder Club - AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 (2:00 PM), Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nDescription: This AWS GenAI Builder Club session focused on the AI-Driven Development Lifecycle (AI-DLC), exploring how generative AI transforms software development from architecture through deployment and maintenance. The session featured demonstrations of Amazon Q Developer and Kiro, showcasing how AI can automate undifferentiated heavy lifting tasks and enable developers to focus on higher-value, creative work. The agenda included an overview of AI-DLC concepts, Amazon Q Developer capabilities, and hands-on Kiro demonstrations.\nOutcomes: Learned practical applications of AI in the software development lifecycle, gained hands-on experience with Amazon Q Developer and Kiro tools, and understood how to integrate AI as a central collaborator in development processes to increase productivity and code quality.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/",
	"title": "Week 4 - AWS Storage Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-29 to 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 4 Overview This week deep-dived into AWS storage services, ranging from S3 object storage to hybrid storage integrations.\nKey Topics Amazon S3 and storage classes S3 static website hosting S3 Glacier for archival workloads AWS Snow Family AWS Storage Gateway Disaster Recovery strategies AWS Backup Hands-on Labs Lab 13: AWS Backup Lab 14: AWS VM Import/Export Lab 24: AWS Storage Gateway Lab 25: Amazon FSx Lab 57: Amazon S3 \u0026amp; CloudFront "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/",
	"title": "Day 05 - AWS Well-Architected Framework",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-12 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nExploration AWS Well-Architected Framework A set of design principles and best practices for building reliable, secure, efficient, and cost-effective cloud architectures. The Well-Architected Tool in the Console provides self-assessments and improvement guidance. Six Pillars of Well-Architected Framework 1. Operational Excellence Focus on running and monitoring systems Continuous improvement of processes Automation of changes Response to events 2. Security Protect information and systems Identity and access management Detective controls Infrastructure protection Data protection 3. Reliability Recover from failures automatically Scale horizontally for resilience Test recovery procedures Manage change through automation 4. Performance Efficiency Use computing resources efficiently Select the right resource types Monitor performance Make informed decisions 5. Cost Optimization Avoid unnecessary costs Understand spending patterns Select appropriate services Optimize over time 6. Sustainability Minimize environmental impact Understand your impact Maximize utilization Use managed services Best Practices Review Design Principles Stop guessing capacity needs: Use auto-scaling Test at production scale: Clone environments easily Automate architecture experimentation: Use IaC Allow for evolutionary architectures: Design for change Drive architectures using data: Monitor and measure Improve through game days: Practice failure scenarios Week 1 Summary Tuần này đã hoàn thành các kiến thức nền tảng về AWS:\n✅ Hiểu về Cloud Computing và lợi ích\n✅ Nắm được AWS Global Infrastructure\n✅ Biết cách sử dụng AWS Management Tools\n✅ Học về Cost Optimization strategies\n✅ Tìm hiểu AWS Well-Architected Framework\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/",
	"title": "Day 10 - Elastic Load Balancing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-19 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Elastic Load Balancing (ELB) Overview A fully managed service distributing traffic across multiple targets (EC2, containers, etc.). Supports HTTP, HTTPS, TCP, TLS. Can be deployed in public or private subnets. Provides DNS names; only NLB supports static IPs. Includes health checks and access logs (to S3). Supports sticky sessions (session affinity). Types: Application, Network, Classic, and Gateway Load Balancer. Application Load Balancer (ALB) Operates at Layer 7 (HTTP/HTTPS). Supports path-based routing (e.g., /mobile vs /desktop). Targets: EC2, Lambda, IP addresses, containers (ECS/EKS). ALB Features:\nHost-based routing Path-based routing HTTP header-based routing Query string parameter-based routing WebSocket support HTTP/2 support Network Load Balancer (NLB) Operates at Layer 4 (TCP/TLS). Supports static IPs and handles millions of requests per second. Targets: EC2, IP addresses, containers (ECS/EKS). NLB Features:\nUltra-low latency Static IP addresses Preserve source IP Long-lived TCP connections TLS termination Gateway Load Balancer (GWLB) Operates at Layer 3 (IP packets). Uses the GENEVE protocol on port 6081. Routes traffic to virtual appliances such as firewalls or monitoring tools. Partner list: aws.amazon.com/elasticloadbalancing/partners Exploration AWS Advanced Networking – Specialty Study Guide Official study guide covering exam topics, AWS network design principles, and real-world architecture scenarios. Hands-On Labs Lab 20 – AWS Transit Gateway Preparation Steps → 20-02 Create Transit Gateway → 20-03 Create TGW Attachments → 20-04 Create TGW Route Tables → 20-05 Add TGW Routes to VPC Route Tables → 20-06 Week 2 Summary Tuần này đã hoàn thành kiến thức về AWS Networking:\n✅ Amazon VPC và Subnets\n✅ Security Groups và NACLs\n✅ VPC Peering và Transit Gateway\n✅ VPN và Direct Connect\n✅ Elastic Load Balancing (ALB, NLB, GWLB)\nLabs completed: 4 labs (VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/",
	"title": "Day 15 - Lightsail, EFS &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-26 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Lightsail Simplified compute service with predictable monthly pricing (starting ~$3.5/month). Includes bundled data transfer at lower rates than EC2. Ideal for small workloads, development, or testing environments. Supports snapshots for backups. Runs inside a managed VPC and can connect to standard VPCs via one-click peering. Lightsail Use Cases:\nSimple web applications WordPress sites Development/test environments Small business applications Learning and experimentation Lightsail vs EC2:\nFeature Lightsail EC2 Pricing Fixed monthly Pay-as-you-go Complexity Simple Advanced Scalability Limited Unlimited Target Small projects Enterprise Amazon EFS (Elastic File System) Fully managed NFSv4 file system mountable by multiple EC2 instances simultaneously. Scales automatically to petabytes. Pay only for the storage used (unlike EBS provisioned size). Can be mounted from on-prem via VPN or Direct Connect. EFS Features:\nConcurrent access from multiple instances Automatic scaling Regional service (multi-AZ) Lifecycle management Encryption at rest and in transit EFS Storage Classes:\nStandard: Frequently accessed files Infrequent Access (IA): Lower cost for rarely accessed files One Zone: Single AZ for cost savings Amazon FSx Managed, scalable file systems for Windows, Lustre, and NetApp ONTAP. AWS handles setup, scaling, and backups. Accessible from EC2, on-prem servers, or users via SMB or NFS protocols. FSx Variants:\nFSx for Windows File Server Native Windows file system SMB protocol support Active Directory integration DFS namespaces FSx for Lustre High-performance computing Machine learning workloads Sub-millisecond latencies S3 integration FSx for NetApp ONTAP Multi-protocol (NFS, SMB, iSCSI) Data deduplication and compression Snapshots and replication AWS Application Migration Service (MGN) Used for migrating or replicating physical/virtual servers to AWS for DR or modernization. Continuously replicates source machines to lightweight staging instances on EC2. During cut-over, MGN launches fully functional EC2 instances from the replicated data. Migration Phases:\nInstall agent on source servers Continuous replication to AWS Testing with non-disruptive test instances Cutover to production Exploration Microsoft Workloads on AWS A curated series covering deployment, optimization, and best practices for running Microsoft workloads on AWS. Week 3 Summary Tuần này đã hoàn thành kiến thức về AWS Compute:\n✅ Amazon EC2 và Instance Types\n✅ AMI, EBS, Instance Store\n✅ EC2 Auto Scaling\n✅ EC2 Pricing Options\n✅ Lightsail, EFS, FSx\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/",
	"title": "Day 20 - AWS Backup &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-03 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Backup Centralized backup service for automating and governing data protection at scale.\nKey Capabilities Central management: Define and apply backup policies across services. Multi-service support: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3, and more. Scheduling \u0026amp; lifecycle: Automate backups and retention. Compliance: Support for governance and audit requirements. Benefits Operational simplicity: No custom scripts or disparate tools. Time savings: Automated, policy-driven protection. Reporting \u0026amp; audit: Visibility into backup status and compliance. Backup Vault Lock Immutability controls to prevent modifications or deletions of protected backups for strict compliance. AWS Backup Features:\nCross-region backup copy Cross-account backup Backup policies (plans) Lifecycle management Encryption at rest Tag-based backup policies Backup Plan Example:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Exploration AWS Skill Builder Curated learning plans and deep-dive content for storage specialists: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 – AWS Backup Create S3 Bucket → 13-02.1 Deploy Infrastructure → 13-02.2 Create Backup Plan → 13-03 Set Up Notifications → 13-04 Test Restore → 13-05 Clean Up Resources → 13-06 Lab 25 – Amazon FSx (File Systems) Create SSD Multi-AZ File System → 25-2.2 Create HDD Multi-AZ File System → 25-2.3 Create New File Shares → 25-3 Test Performance → 25-4 Monitor Performance → 25-5 Enable Data Deduplication → 25-6 Enable Shadow Copies → 25-7 Manage User Sessions and Open Files → 25-8 Enable User Storage Quotas → 25-9 Scale Throughput Capacity → 25-11 Scale Storage Capacity → 25-12 Delete Environment → 25-13 Week 4 Summary Tuần này đã hoàn thành kiến thức về AWS Storage:\n✅ Amazon S3 và Storage Classes\n✅ S3 Static Website và CORS\n✅ AWS Snow Family\n✅ AWS Storage Gateway\n✅ Disaster Recovery Strategies\n✅ AWS Backup\nLabs completed: 5 labs (Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/",
	"title": "Day 25 - AWS Security Hub &amp; Automation",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-10 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Security Hub Aggregates and prioritizes security findings and posture across accounts/services. Capabilities\nAutomated checks, normalized findings, prioritized remediation workflows. Compliance standards: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Integrations\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer, plus partner tools. Outcomes\nLess time aggregating, more time fixing; unified visibility and improved security hygiene. Security Hub Features:\nContinuous security posture monitoring Automated compliance checks Centralized findings across accounts Integration with 50+ AWS and partner services Custom insights and dashboards Automated remediation with EventBridge Security Standards:\nAWS Foundational Security Best Practices: 50+ controls CIS AWS Foundations Benchmark: Industry best practices PCI DSS: Payment card industry standards NIST: National Institute of Standards framework Security Automation AWS Services for Automation:\nAWS Config: Track resource configuration changes Amazon EventBridge: Event-driven automation AWS Lambda: Serverless remediation functions AWS Systems Manager: Automated patching and compliance Common Automation Patterns:\nAuto-remediate non-compliant resources Automated incident response Security group rule validation Encryption enforcement Tag compliance Exploration AWS Certified Security – Specialty: All-in-One Exam Guide (SCS-C01) Comprehensive preparation material for the Security Specialty certification. Hands-On Labs Lab 18 – AWS Security Hub Enable Security Hub → 18-02 Score for Each Set of Criteria → 18-03 Clean Up Resources → 18-04 Lab 22 – AWS Lambda Automation with Slack Create VPC → 22-2.1 Create Security Group → 22-2.2 Create EC2 Instance → 22-2.3 Incoming Webhooks (Slack) → 22-2.4 Create Tag for Instance → 22-3 Create Role for Lambda → 22-4 Function: Stop Instance → 22-5.1 Function: Start Instance → 22-5.2 Check Result → 22-6 Clean Up Resources → 22-7 Lab 27 – AWS Resource Groups \u0026amp; Tagging (Part 2) Use Tags with CLI → 27-2.2 Create a Resource Group → 27-3 Clean Up Resources → 27-4 Lab 33 – AWS KMS \u0026amp; CloudTrail Integration (Part 2) Create CloudTrail → 33-5.1 Log to CloudTrail → 33-5.2 Create Amazon Athena → 33-5.3 Query with Athena → 33-5.4 Test \u0026amp; Share Encrypted S3 Data → 33-6 Resource Cleanup → 33-7 Lab 44 – IAM Advanced Role Control Create IAM Group → 44-2 Create IAM Users → 44-3.1 Check Permissions → 44-3.2 Create Admin IAM Role → 44-4.1 Configure Switch Role → 44-4.2 Restrict Switch Role by IP → 44-4.3.1 Restrict Switch Role by Time → 44-4.3.2 Clean Up Resources → 44-5 Week 5 Summary Tuần này đã hoàn thành kiến thức về AWS Security:\n✅ Shared Responsibility Model\n✅ AWS IAM (Users, Groups, Roles, Policies)\n✅ Amazon Cognito\n✅ AWS Organizations \u0026amp; SCPs\n✅ AWS Identity Center\n✅ AWS KMS\n✅ AWS Security Hub\nLabs completed: 8 labs (Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/",
	"title": "Day 30 - Database Migration &amp; Best Practices",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-17 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Database Migration Service (DMS) AWS DMS helps migrate databases to AWS quickly and securely with minimal downtime.\nKey Features:\nHomogeneous Migrations: Same database engine (e.g., Oracle to Oracle) Heterogeneous Migrations: Different engines (e.g., Oracle to Aurora) Continuous Replication: Keep source and target in sync Schema Conversion: AWS Schema Conversion Tool (SCT) Migration Types:\nFull Load: One-time migration of existing data Full Load + CDC: Initial load plus ongoing changes CDC Only: Replicate only ongoing changes Supported Sources:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Supported Targets:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Database Best Practices Performance Optimization RDS/Aurora:\nUse appropriate instance types Enable Enhanced Monitoring Optimize queries and indexes Use Read Replicas for read-heavy workloads Enable Performance Insights Redshift:\nChoose appropriate distribution keys Use sort keys for frequently filtered columns Vacuum and analyze tables regularly Use columnar compression Implement workload management (WLM) ElastiCache:\nChoose appropriate node types Use cluster mode for Redis scalability Implement proper cache eviction policies Monitor cache hit rates Use connection pooling Security Best Practices Encryption at Rest: Enable for all databases Encryption in Transit: Use SSL/TLS connections Network Isolation: Deploy in private subnets IAM Authentication: Use for RDS/Aurora when possible Secrets Manager: Store database credentials securely Security Groups: Restrict access to minimum required Audit Logging: Enable CloudWatch Logs and CloudTrail High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nEnable Multi-AZ for production workloads Configure automated backups Test restore procedures regularly Use Aurora Global Database for multi-region DR Implement read replicas in different regions Redshift:\nEnable automated snapshots Copy snapshots to other regions Use Redshift Spectrum for data lake integration Implement cross-region snapshot copy ElastiCache:\nEnable Multi-AZ with automatic failover (Redis) Configure backup and restore (Redis) Use cluster mode for Redis scalability Implement application-level retry logic Cost Optimization Right-sizing: Choose appropriate instance types Reserved Instances: Commit for 1-3 years for discounts Aurora Serverless: For variable workloads Redshift Serverless: For intermittent analytics Storage Optimization: Use appropriate storage types Lifecycle Policies: Archive old data to S3/Glacier Monitor Usage: Use Cost Explorer and Budgets Exploration The Data Warehouse Toolkit Canonical reference for dimensional modeling and DW design patterns. Week 6 Summary Tuần này đã hoàn thành kiến thức về AWS Database Services:\n✅ Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP)\n✅ Amazon RDS \u0026amp; Aurora\n✅ Amazon Redshift\n✅ Amazon ElastiCache\n✅ AWS Database Migration Service\nLabs completed: 2 labs (RDS \u0026amp; EC2 Integration, Database Migration Service)\nTổng kết 6 tuần đầu (8/9 - 17/10/2025) 30 ngày làm việc đã hoàn thành:\nWeek 1: Cloud Computing Fundamentals AWS basics, infrastructure, management tools, cost optimization Week 2: AWS Networking Services VPC, subnets, security groups, load balancing, hybrid connectivity Week 3: AWS Compute Services EC2, AMI, storage, auto scaling, pricing models Week 4: AWS Storage Services S3, Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR Week 5: AWS Security \u0026amp; Identity IAM, Cognito, Organizations, KMS, Security Hub Week 6: AWS Database Services RDS, Aurora, Redshift, ElastiCache, DMS Tổng số labs hoàn thành: 25+ labs\nTiếp theo: Tuần 7-8 sẽ bắt đầu từ ngày 20/10/2025 (Monday)\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/1.7.5-day35-2025-10-24/",
	"title": "Day 35 - Contract Testing &amp; Retrospective",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-24 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Contract Testing with Schemathesis Run schemathesis run --checks all --workers 4 --url http://127.0.0.1:8000/openapi.yaml to auto-generate tests. Schemathesis explores random scenarios (happy paths, edge cases, missing fields). Ensures the backend doesn’t break the schema when the frontend switches to the real API. Benefits Eliminates the need to handcraft complex test cases. Reduces mismatch risk after refactors. Acts as a quality gate inside the CI pipeline. Mistakes \u0026amp; Fixes Mistake Root Cause Fix Added both error.tsx and not-found.tsx Redundant handling Keep only not-found.tsx Used --base-url in Schemathesis Wrong CLI option Use the correct --url flag Timeout on /openapi.json CORS or slow resp Point Schemathesis to the YAML file Over-engineered backend Too many files early Start simple and refactor later Validated Workflow 1. Define Contract (OpenAPI)\r2. Mock API (Prism)\r3. Build Frontend with mock data\r4. Implement Backend according to the spec\r5. Switch to the real API\r6. Contract Testing (Schemathesis) Enables frontend and backend teams to work in parallel. Reduces conflicts, accelerates demos, and keeps quality stable. Key Insights Contract-first development keeps specs aligned and shrinks integration bugs. Vertical slices make it possible to release in increments and gather early feedback. Automation (Prism, Schemathesis) cuts manual testing effort. Start simple and refactor gradually as scope grows. Hands-On Labs Run Schemathesis with the latest spec and capture the results. Refresh the README workflow so the whole team can follow it. Prepare the backlog for the next vertical slice based on demo feedback. "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/1.8.5-day40-2025-10-31/",
	"title": "Day 40 - MT Evaluation &amp; Decoding Strategies",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-31 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nBLEU Score – Precision-Based Evaluation BLEU (Bilingual Evaluation Understudy) is an algorithm designed to evaluate machine translation quality.\nHow BLEU Works Core Concept: Compare candidate translation to one or more reference translations (often human translations)\nScore Range: 0 to 1\nCloser to 1 = better translation Closer to 0 = worse translation Calculating BLEU Score Vanilla BLEU (Problematic) Example:\nCandidate: \u0026ldquo;I I am I\u0026rdquo; Reference 1: \u0026ldquo;Eunice said I\u0026rsquo;m hungry\u0026rdquo; Reference 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; Process:\nCount how many candidate words appear in any reference Divide by total candidate words Result: 4/4 = 1.0 (perfect score!)\nProblem: This translation is terrible but gets perfect score! A model that outputs common words will score well.\nModified BLEU (Better) Key Change: After matching a word, exhaust it from references\nSame Example:\n\u0026ldquo;I\u0026rdquo; (first) → matches → exhaust \u0026ldquo;I\u0026rdquo; from references → count = 1 \u0026ldquo;I\u0026rdquo; (second) → no match left → count = 1 \u0026ldquo;am\u0026rdquo; → matches → exhaust \u0026ldquo;am\u0026rdquo; → count = 2 \u0026ldquo;I\u0026rdquo; (third) → no match left → count = 2 Result: 2/4 = 0.5 (more realistic!)\nBLEU Limitations ❌ Doesn\u0026rsquo;t consider semantic meaning\nOnly checks word matches ❌ Doesn\u0026rsquo;t consider sentence structure\n\u0026ldquo;Ate I was hungry because\u0026rdquo; vs \u0026ldquo;I ate because I was hungry\u0026rdquo; Both get same score! ✅ Still most widely adopted metric despite limitations\nROUGE Score – Recall-Based Evaluation ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nBLEU vs ROUGE Metric Focus Calculation BLEU Precision How many candidate words in reference? ROUGE Recall How many reference words in candidate? Calculating ROUGE-N Score Example:\nCandidate: \u0026ldquo;I I am I\u0026rdquo; Reference 1: \u0026ldquo;Younes said I am hungry\u0026rdquo; (5 words) Reference 2: \u0026ldquo;He said I\u0026rsquo;m hungry\u0026rdquo; (5 words) Process for Reference 1:\n\u0026ldquo;Younes\u0026rdquo; → no match → count = 0 \u0026ldquo;said\u0026rdquo; → no match → count = 0 \u0026ldquo;I\u0026rdquo; → match → count = 1 \u0026ldquo;am\u0026rdquo; → match → count = 2 \u0026ldquo;hungry\u0026rdquo; → no match → count = 2 ROUGE score for Ref 1: 2/5 = 0.4\nIf multiple references: Calculate for each, take maximum\nF1 Score – Combining BLEU and ROUGE Since BLEU = precision and ROUGE = recall, we can calculate F1 score:\nFormula:\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\rF1 = 2 × (BLEU × ROUGE) / (BLEU + ROUGE) Example:\nBLEU = 0.5 ROUGE = 0.4 F1 = 2 × (0.5 × 0.4) / (0.5 + 0.4) = 4/9 ≈ 0.44 Beam Search Decoding Problem: Taking the highest probability word at each step doesn\u0026rsquo;t guarantee best overall sequence\nSolution: Beam search finds most likely sequences over a fixed window\nHow Beam Search Works Beam Width (B): Number of sequences to keep at each step\nProcess: Step 1: Start with SOS token Get probabilities for first word:\nI: 0.5 am: 0.4 hungry: 0.1 Keep top B=2: \u0026ldquo;I\u0026rdquo; and \u0026ldquo;am\u0026rdquo;\nStep 2: Calculate Conditional Probabilities For \u0026ldquo;I\u0026rdquo;:\nI am: 0.5 × 0.5 = 0.25 I I: 0.5 × 0.1 = 0.05 For \u0026ldquo;am\u0026rdquo;:\nam I: 0.4 × 0.7 = 0.28 am hungry: 0.4 × 0.2 = 0.08 Keep top B=2: \u0026ldquo;am I\u0026rdquo; (0.28) and \u0026ldquo;I am\u0026rdquo; (0.25)\nStep 3: Repeat Continue until all B sequences reach EOS token\nStep 4: Select Best Choose sequence with highest overall probability\nBeam Search Characteristics Advantages:\nBetter than greedy decoding (B=1) Finds globally better sequences Widely used in production Disadvantages:\nMemory intensive (store B sequences) Computationally expensive (run model B times per step) Penalizes long sequences (product of many probabilities) Solution for Long Sequences: Normalize by length: divide probability by number of words\nMinimum Bayes Risk (MBR) Decoding Concept: Generate multiple samples and find consensus\nMBR Process: Step 1: Generate Multiple Samples Create ~30 random samples from the model\nStep 2: Compare All Pairs For each sample, compare against all others using similarity metric (e.g., ROUGE)\nStep 3: Calculate Average Similarity For each candidate, compute average similarity with all other candidates\nStep 4: Select Best Choose the sample with highest average similarity (lowest risk)\nMBR Formula E* = argmax_E [ average ROUGE(E, E\u0026#39;) for all E\u0026#39; ] Where:\nE = candidate translation E\u0026rsquo; = all other candidates Goal: Find E that maximizes average ROUGE with every E' MBR Example (4 Candidates) Step 1: Calculate pairwise ROUGE scores\nROUGE(C1, C2), ROUGE(C1, C3), ROUGE(C1, C4) Average = R1 Step 2: Repeat for C2, C3, C4\nGet R2, R3, R4 Step 3: Select highest\nChoose candidate with max(R1, R2, R3, R4) MBR Characteristics Advantages:\nMore contextually accurate than random sampling Finds consensus translation Can outperform beam search Disadvantages:\nRequires generating many samples (expensive) Requires O(n²) comparisons When to Use:\nWhen you need high-quality translation When computational cost is acceptable When beam search outputs are inconsistent Summary: Decoding Strategies Method Description Pros Cons Greedy Pick highest prob at each step Fast, simple Suboptimal sequences Beam Search Keep top-B sequences Better quality Memory + compute cost Random Sampling Sample from distribution Diverse outputs Inconsistent quality MBR Consensus from samples High quality Very expensive Evaluation Metrics Summary Metric Type Focus Best For BLEU Precision Candidate → Reference General MT ROUGE Recall Reference → Candidate Summarization F1 Harmonic Mean Both precision \u0026amp; recall Balanced view Critical Note: All these metrics:\n❌ Don\u0026rsquo;t consider semantics ❌ Don\u0026rsquo;t consider sentence structure ✅ Only count n-gram matches Modern Alternative: Use neural metrics or human evaluation for critical applications!\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/1.9.5-day45-2025-11-07/",
	"title": "Day 45 - Transformer Decoder &amp; GPT2 Implementation",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-07 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nBuilding a Transformer Decoder: GPT2 Architecture Now let\u0026rsquo;s see how all these pieces come together in actual code!\nTransformer Decoder Structure (GPT2-style) Input: Tokenized sentence [1, 2, 3, 4, 5]\r↓\rEmbedding Layer: Convert tokens to vectors\r↓\rAdd Positional Encoding: Add position info\r↓\r┌──────────────────────────────────┐\r│ Decoder Block (N times) │\r│ ├─ Masked Self-Attention │\r│ ├─ Residual + LayerNorm │\r│ ├─ Feed-Forward │\r│ └─ Residual + LayerNorm │\r└──────────────────────────────────┘\r↓\rLinear Layer: Project to vocab size\r↓\rSoftmax: Convert to probabilities\r↓\rOutput: Probabilities for next word PyTorch Implementation Step 1: Word Embedding + Positional Encoding import torch import torch.nn as nn class TransformerDecoder(nn.Module): def __init__(self, vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1): super().__init__() # 1. Embedding layer self.embedding = nn.Embedding(vocab_size, d_model) # 2. Positional encoding (learned) self.positional_encoding = nn.Embedding(max_seq_len, d_model) # 3. Decoder blocks (repeat N times) self.decoder_layers = nn.ModuleList([ DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers) ]) # 4. Output layer self.final_layer = nn.Linear(d_model, vocab_size) self.softmax = nn.Softmax(dim=-1) self.d_model = d_model def forward(self, input_ids, mask=None): # input_ids shape: [batch_size, seq_length] batch_size, seq_len = input_ids.shape # 1. Embed tokens x = self.embedding(input_ids) # [batch, seq_len, d_model] # 2. Add positional encoding positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) pos_encoding = self.positional_encoding(positions) x = x + pos_encoding # [batch, seq_len, d_model] # 3. Pass through decoder layers for decoder_layer in self.decoder_layers: x = decoder_layer(x, mask) # 4. Project to vocab logits = self.final_layer(x) # [batch, seq_len, vocab_size] return logits Step 2: Decoder Block class DecoderBlock(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super().__init__() # 1. Masked multi-head attention self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) self.norm1 = nn.LayerNorm(d_model) # 2. Feed-forward network self.feed_forward = FeedForward(d_model, d_ff, dropout) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask=None): # x shape: [batch, seq_len, d_model] # Masked Self-Attention + Residual + Norm attn_output = self.self_attention(x, x, x, mask) # Q=K=V x = x + self.dropout(attn_output) x = self.norm1(x) # Feed-Forward + Residual + Norm ff_output = self.feed_forward(x) x = x + self.dropout(ff_output) x = self.norm2(x) return x Step 3: Multi-Head Attention class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads, dropout): super().__init__() assert d_model % num_heads == 0 self.num_heads = num_heads self.d_k = d_model // num_heads # Linear projections for Q, K, V self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) # Output projection self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, Q, K, V, mask=None): batch_size = Q.shape[0] # 1. Linear projections and split into multiple heads Q = self.W_q(Q) # [batch, seq_len, d_model] K = self.W_k(K) V = self.W_v(V) # Reshape for multi-head: [batch, seq_len, num_heads, d_k] Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Now: [batch, num_heads, seq_len, d_k] # 2. Scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, num_heads, seq_len, seq_len] # 3. Apply mask (for causal masking in decoder) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 4. Softmax attention_weights = torch.softmax(scores, dim=-1) attention_weights = self.dropout(attention_weights) # 5. Multiply by value context = torch.matmul(attention_weights, V) # [batch, num_heads, seq_len, d_k] # 6. Concatenate heads context = context.transpose(1, 2).contiguous() context = context.view(batch_size, -1, self.d_model) # 7. Final linear projection output = self.W_o(context) return output Step 4: Feed-Forward Network class FeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout): super().__init__() self.linear1 = nn.Linear(d_model, d_ff) # 512 → 2048 self.linear2 = nn.Linear(d_ff, d_model) # 2048 → 512 self.relu = nn.ReLU() self.dropout = nn.Dropout(dropout) def forward(self, x): x = self.linear1(x) # Expand x = self.relu(x) # Non-linearity x = self.dropout(x) # Regularization x = self.linear2(x) # Compress return x Step 5: Causal Mask (for preventing future attendance) def create_causal_mask(seq_len, device): \u0026#34;\u0026#34;\u0026#34; Creates a mask that prevents attention to future positions. Output: [1, 0, 0, 0] [1, 1, 0, 0] [1, 1, 1, 0] [1, 1, 1, 1] Position i can only attend to positions 0...i \u0026#34;\u0026#34;\u0026#34; mask = torch.tril(torch.ones(seq_len, seq_len, device=device)) return mask.unsqueeze(0).unsqueeze(0) # [1, 1, seq_len, seq_len] # Usage: mask = create_causal_mask(seq_len=10, device=\u0026#39;cuda\u0026#39;) Training Loop def train_transformer(model, train_loader, epochs=10, learning_rate=0.0001): optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): total_loss = 0 for batch_idx, (input_ids, target_ids) in enumerate(train_loader): # Forward pass logits = model(input_ids) # logits: [batch, seq_len, vocab_size] # target_ids: [batch, seq_len] # Calculate loss loss = loss_fn( logits.view(-1, vocab_size), target_ids.view(-1) ) # Backward pass optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\u0026#34;Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\u0026#34;) Inference (Text Generation) def generate_text(model, start_token, max_length=50, device=\u0026#39;cuda\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Generate text autoregressively using the trained transformer. \u0026#34;\u0026#34;\u0026#34; model.eval() generated = [start_token] with torch.no_grad(): for _ in range(max_length): # Prepare input input_ids = torch.tensor(generated, device=device).unsqueeze(0) # Forward pass logits = model(input_ids) # Get logits for last position last_logits = logits[0, -1, :] # [vocab_size] # Sample or greedy next_token = torch.argmax(last_logits).item() # Greedy # Or: next_token = torch.multinomial(softmax(last_logits), 1).item() # Sample generated.append(next_token) if next_token == end_token: break return generated Complete Working Example # Initialize model model = TransformerDecoder( vocab_size=10000, d_model=512, num_layers=6, num_heads=8, d_ff=2048, max_seq_len=1024, dropout=0.1 ).to(\u0026#39;cuda\u0026#39;) # Create dummy data batch_size, seq_len = 32, 128 input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(\u0026#39;cuda\u0026#39;) # Forward pass output = model(input_ids) print(f\u0026#34;Output shape: {output.shape}\u0026#34;) # [32, 128, 10000] # Create causal mask mask = create_causal_mask(seq_len, \u0026#39;cuda\u0026#39;) # Forward with mask output_masked = model(input_ids, mask) print(f\u0026#34;Masked output shape: {output_masked.shape}\u0026#34;) # Generate text generated = generate_text(model, start_token=101, max_length=20) print(f\u0026#34;Generated sequence: {generated}\u0026#34;) Key Components Summary Component Purpose Size Embedding Token → Vector vocab_size → d_model Positional Encoding Add position info d_model Multi-Head Attention Learn relationships d_model → d_model Feed-Forward Non-linear transform d_model → d_ff → d_model LayerNorm Stabilize training per-element Output Layer Project to vocab d_model → vocab_size Why This Architecture Works ✅ Parallel Processing: All positions processed together (fast!) ✅ Long-range Dependencies: Direct attention to any position (no vanishing gradients!) ✅ Interpretable: Can visualize attention patterns ✅ Scalable: Can grow to billions of parameters\nGPT2 Variants GPT-2 Small: 117M parameters GPT-2 Medium: 345M parameters GPT-2 Large: 762M parameters GPT-2 XL: 1.5B parameters All use the same decoder architecture, just scaled up!\nNext Steps Pre-training: Train on large text corpus (Wikipedia, Books, etc.) Fine-tuning: Adapt to specific tasks (translation, classification, etc.) Evaluation: Measure quality (perplexity, BLEU, human evaluation) Deployment: Use for real-world applications Week 9 Summary We covered:\n✅ Why transformers replaced RNNs ✅ Complete transformer architecture ✅ Scaled dot-product attention mechanics ✅ Self, masked, and encoder-decoder attention ✅ Implementation details and code This is the foundation of modern NLP! All state-of-the-art models (BERT, GPT, T5, Claude, ChatGPT) are based on transformer architecture.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/1.10.5-day50-2025-11-14/",
	"title": "Day 50 - Fine-tuning &amp; Real-World Applications",
	"tags": [],
	"description": "",
	"content": "Date: 2025-11-14 (Friday)\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 10 Complete: From Theory to Production You now understand transfer learning, BERT, and T5. This final day covers how to actually deploy these models.\nFine-tuning: The Art and Science The Big Picture Pre-trained Model (e.g., BERT-base)\r├─ Already knows English\r├─ Already understands grammar\r├─ Already has some world knowledge\r└─ Can be adapted to specific tasks!\rAdd Classification Head\r├─ Simple: Take [CLS] token representation\r├─ Add: Dense layer (768 → hidden_size)\r├─ Add: Classification layer (hidden_size → num_classes)\r└─ Result: Task-specific model\rFine-tune on your data\r├─ Movie reviews: 5,000 examples\r├─ Training time: 2-3 hours on single GPU\r├─ Results: 94-96% accuracy\r└─ Deploy! Fine-tuning Strategies Strategy 1: Full Fine-tuning Update all parameters\n# Pseudocode pretrained_model = load_bert_base() # No frozen layers! for epoch in range(3): for batch in training_data: logits = pretrained_model(batch) loss = classification_loss(logits, batch.labels) loss.backward() optimizer.step() # Results: Best performance # Time: 3+ hours on GPU # Memory: Requires gradients for all 110M params When to use:\nLarge dataset (10,000+ examples) Sufficient compute (GPU/TPU) Task very different from pre-training Strategy 2: Layer Freezing Freeze early layers, fine-tune later layers\n# Freeze first 10 layers for param in model.bert.encoder.layer[:10].parameters(): param.requires_grad = False # Fine-tune layers 11-12 and classification head for param in model.bert.encoder.layer[10:].parameters(): param.requires_grad = True # Results: Fast, good performance # Time: 1-2 hours on GPU # Memory: Only gradients for 2 layers + head When to use:\nMedium dataset (1,000-10,000 examples) Limited compute Task somewhat different from pre-training Strategy 3: Progressive Unfreezing Gradually unfreeze layers from top to bottom\nEpoch 1: Only fine-tune classification head\r├─ Freeze: All 12 layers\r├─ Train: Classification head\r└─ Learning rate: 1e-3\rEpoch 2: Unfreeze last 1 layer\r├─ Freeze: Layers 0-10\r├─ Train: Layer 11 + head\r└─ Learning rate: 1e-4\rEpoch 3: Unfreeze last 2 layers\r├─ Freeze: Layers 0-9\r├─ Train: Layers 10-11 + head\r└─ Learning rate: 1e-4\r...Continue until all unfrozen\rResults: Often best performance!\rTime: 5+ hours, but worth it for important tasks Hyperparameter Selection Learning Rate General Guidelines:\rFor full fine-tuning:\r├─ Start with: 5e-5 (very small!)\r├─ Try: 2e-5, 3e-5, 5e-5, 1e-4\r└─ Don\u0026#39;t use: Learning rates \u0026gt; 1e-4 (catastrophic forgetting)\rFor layer freezing:\r├─ Frozen layers: No learning rate (not updated)\r├─ Fine-tuned layers: 1e-4 - 1e-3\r└─ Classification head: Can use slightly higher\rWhy so small?\r├─ Pre-trained weights are already good\r├─ Don\u0026#39;t want to destroy the knowledge\r├─ Small changes are safer Batch Size Batch Size Impact:\rSmall batches (8-16):\r├─ Pros: Works with limited memory\r├─ Cons: Noisier gradients, unstable\r├─ Use when: Small GPU (\u0026lt; 8GB VRAM)\rMedium batches (32):\r├─ Pros: Good balance\r├─ Cons: Moderate memory usage\r├─ Use when: Standard GPU (8-16GB VRAM)\rLarge batches (64-256):\r├─ Pros: Stable training, better generalization\r├─ Cons: Requires lots of memory or gradient accumulation\r├─ Use when: TPUs, 24GB+ VRAM, or gradient accumulation Number of Epochs Classification Tasks:\r├─ Typical: 3-5 epochs\r├─ Why: Model converges quickly\r└─ Monitor: Stop early if validation stops improving\rGeneration Tasks (T5):\r├─ Typical: 10-20 epochs\r├─ Why: More complex task, slower convergence\r└─ Monitor: Validation BLEU score\rRule of thumb:\r├─ More data → Fewer epochs (e.g., 2 epochs for 100K examples)\r├─ Less data → More epochs (e.g., 5 epochs for 1K examples) Common Fine-tuning Applications 1. Sentiment Analysis Task: Classify reviews as positive/negative\rData: 5,000 movie reviews with labels\r├─ 80%: Training (4,000)\r├─ 20%: Validation (1,000)\rFine-tuning:\r├─ Model: BERT-base\r├─ Epochs: 3\r├─ Batch size: 32\r├─ Learning rate: 2e-5\r├─ Training time: 30 minutes\rResults:\r├─ Accuracy: 94.2%\r├─ Precision: 94.5%\r├─ Recall: 93.9%\r└─ Much better than training from scratch (78%)! 2. Named Entity Recognition (NER) Task: Identify people, places, organizations in text\rExample:\r\u0026#34;John Smith works at Google in New York.\u0026#34;\rLabels: [B-PER, I-PER, O, O, B-ORG, O, B-LOC, I-LOC]\rChallenge: Token-level classification, not sentence-level\rSolution:\r├─ Get BERT token embeddings\r├─ Add linear layer for each token\r├─ Decode with CRF (Conditional Random Field)\rFine-tuning time: 1-2 hours\rPerformance: 92% F1 score 3. Question Answering Task: Find the answer span in a passage\rInput:\rQuestion: \u0026#34;What is the capital of France?\u0026#34;\rPassage: \u0026#34;Paris is the capital and most populous city of France...\u0026#34;\rOutput:\rAnswer: \u0026#34;Paris\u0026#34;\rHow it works:\r├─ Encode question + passage together\r├─ For each token, predict: \u0026#34;Is this the start of answer?\u0026#34;\r├─ For each token, predict: \u0026#34;Is this the end of answer?\u0026#34;\r├─ Extract span between highest probability start and end\rFine-tuning time: 2-3 hours\rPerformance: 89% F1 on SQuAD 4. Text Summarization Task: Condensing long documents\rUsing T5:\rInput:\r\u0026#34;The quick brown fox jumps over the lazy dog. This sentence contains all 26 letters of English alphabet.\rIt\u0026#39;s often used as a test string in computers.\u0026#34;\rFine-tuning with T5:\r├─ Prefix: \u0026#34;summarize:\u0026#34;\r├─ Full training: 10-20 epochs\r├─ Batch size: 16\r├─ Learning rate: 5e-5\rOutput:\r\u0026#34;A pangram sentence commonly used in computing.\u0026#34;\rPerformance: ROUGE score of 35-40 (compared to 20-25 baseline) 5. Semantic Textual Similarity Task: Rate how similar two sentences are (0-5)\rSentence A: \u0026#34;The cat sat on the mat\u0026#34;\rSentence B: \u0026#34;A feline rested on the rug\u0026#34;\rLabel: 4.5 (very similar)\rFine-tuning:\r├─ Take [CLS] token from both sentences\r├─ Encode together\r├─ Regression head: Dense layer to output score (0-5)\r├─ Loss: Mean Squared Error (MSE)\rResults:\r├─ Correlation with human judgments: 0.88 (very good!)\r├─ Spearman correlation: 87% Fine-tuning Pitfalls to Avoid ❌ Pitfall 1: Learning Rate Too High Problem: Model forgets pre-trained knowledge!\rExample:\rLearning rate: 1e-3\rAfter 1 epoch: Loss = 0.5 (good)\rAfter 2 epochs: Loss = 2.0 (terrible!)\rAfter 3 epochs: Loss = 5.0 (worse!)\rWhy: Large updates destroy useful weights\rSolution:\r├─ Use learning rate 1-2 orders of magnitude smaller\r├─ Start with 2e-5, increase only if converges too slowly\r└─ Monitor: Validation loss should decrease ❌ Pitfall 2: Too Few Epochs Problem: Model doesn\u0026#39;t adapt to new task\rExample:\rData: 5,000 examples\rEpochs: 1\rPerformance: 88% accuracy\rSame model with 3 epochs:\rPerformance: 94% accuracy!\rWhy: 1 epoch = each example seen once\rNot enough to learn task-specific patterns\rSolution:\r├─ Use at least 3-5 epochs\r├─ Monitor validation accuracy\r├─ Stop early when validation stops improving ❌ Pitfall 3: Overfitting on Small Data Problem: Model memorizes instead of generalizing\rExample:\rTraining data: 100 examples\rTraining accuracy: 99.8%\rTest accuracy: 72.0%\rModel memorized!\rSolutions:\r├─ Add dropout: Drop 10-20% of neurons randomly\r├─ Early stopping: Stop when validation accuracy plateaus\r├─ Data augmentation: Create more examples from existing ones\r├─ Reduce model size: Use BERT-small instead of BERT-large ❌ Pitfall 4: Not Doing Hyperparameter Tuning Problem: Default hyperparameters aren\u0026#39;t optimal\rExample:\rDefault learning rate (1e-4): 92% accuracy\rTuned learning rate (3e-5): 95% accuracy!\rSolution:\r├─ Try 3-5 different learning rates\r├─ Try 2-3 different batch sizes\r├─ Try 3-5 epochs\r├─ Run small validation set on each combination\r└─ Pick best combination for full training Deployment Considerations Model Size vs Speed For Production Deployment:\rBERT-base (110M):\r├─ Model size: 440 MB\r├─ Inference time: 100-150 ms per example\r├─ Good accuracy\r└─ Can fit on most servers\rBERT-large (340M):\r├─ Model size: 1.3 GB\r├─ Inference time: 300-500 ms per example\r├─ Better accuracy\r└─ Need better hardware\rDistilBERT (40M):\r├─ Model size: 160 MB (60% smaller!)\r├─ Inference time: 30-50 ms (3x faster!)\r├─ Slightly lower accuracy (97% of BERT)\r└─ Perfect for mobile/edge devices!\rDecision tree:\r├─ Accuracy critical? → Use BERT-base or BERT-large\r├─ Speed critical? → Use DistilBERT or quantization\r├─ Balanced? → Use BERT-base Optimization Techniques Before Deployment:\r1. Quantization (8-bit instead of 32-bit):\r├─ Model size: 1/4 of original\r├─ Inference speed: 2-4x faster\r└─ Accuracy: 95-99% of full precision\r2. Knowledge Distillation:\r├─ Train small model on outputs of large model\r├─ Size: 10x smaller\r├─ Speed: 10x faster\r└─ Accuracy: 98% of teacher model\r3. Pruning:\r├─ Remove unimportant weights\r├─ Size: 30-50% smaller\r├─ Speed: 2-3x faster\r└─ Accuracy: 98% of full model\r4. TorchScript/ONNX:\r├─ Compile model for production\r├─ Speed: 1.5-2x faster\r└─ Framework-agnostic (TensorFlow, PyTorch, etc.) Real-World Example: Building a Sentiment Classifier Complete Pipeline Step 1: Prepare Data\r├─ Load: 5,000 movie reviews with labels\r├─ Split: 80% train, 20% test\r├─ Tokenize: Convert to BERT tokens\r└─ Dataloader: Create batches of 32\rStep 2: Load Pre-trained Model\r├─ Download BERT-base from HuggingFace\r├─ Add classification head: 768 → 2 (binary)\r└─ Move to GPU\rStep 3: Fine-tune\r├─ Optimizer: AdamW (best for transformers)\r├─ Learning rate: 2e-5\r├─ Epochs: 3\r├─ Train loop: Forward pass → Loss → Backward → Update\rStep 4: Evaluate\r├─ Validation accuracy: 94.2%\r├─ Test accuracy: 93.8%\r└─ Per-class metrics: Precision 94%, Recall 94%\rStep 5: Save \u0026amp; Deploy\r├─ Save: model.pt, tokenizer, config.json\r├─ Test on new review: \u0026#34;Best movie ever!\u0026#34; → Positive ✓\r└─ Deploy to production!\rTotal time: 2-3 hours\rTotal cost: ~$2-5 on cloud GPU\rPerformance: State-of-the-art! The Transfer Learning Advantage Comparison Baseline (Training from Scratch):\r├─ Training time: 2-4 weeks\r├─ Required data: 100,000+ examples\r├─ Accuracy: 82-85%\r├─ Cost: $1000-10000 in compute\r└─ Difficulty: Requires ML expertise\rTransfer Learning (BERT Fine-tuning):\r├─ Training time: 2-3 hours\r├─ Required data: 100-1000 examples\r├─ Accuracy: 92-95%\r├─ Cost: $1-10 in compute\r└─ Difficulty: Can use HuggingFace library!\rSpeedup: 200-300x faster!\rData reduction: 100x less data needed!\rBetter results: 10-15% higher accuracy! Key Takeaways ✅ Transfer learning is practical: Works great for real problems ✅ Fine-tuning is simple: Add head + train 3-5 epochs ✅ Learning rate matters: Use 1-2 orders of magnitude smaller ✅ Avoid overfitting: Monitor validation, use early stopping ✅ Consider deployment: Optimize for your constraints ✅ HuggingFace is your friend: Use pre-trained models + library\nYour NLP Journey You\u0026rsquo;ve learned:\nWeek 1-7 (Foundation): Basics, text processing, embeddings Week 8 (NLP Foundations): Linguistic fundamentals, voice search, seq2seq, attention Week 9 (Transformers): Self-attention, scaled dot-product, attention mechanisms, implementation Week 10 (Transfer Learning): Transfer learning, BERT, MLM, T5, fine-tuning\nFrom understanding language to building production systems!\nNext Steps To become an NLP expert:\nBuild projects: Fine-tune models on real data Try different models: RoBERTa, ELECTRA, XLNet, GPT-2 Explore advanced techniques: Prompt tuning, in-context learning, RAG Stay updated: Read papers, follow research (Papers with Code, Twitter) Contribute: Open source NLP projects Resources HuggingFace: https://huggingface.co/ (Models \u0026amp; library) BERT Paper: \u0026ldquo;BERT: Pre-training of Deep Bidirectional Transformers\u0026rdquo; (Devlin et al., 2018) T5 Paper: \u0026ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u0026rdquo; (Raffel et al., 2019) Attention Paper: \u0026ldquo;Attention Is All You Need\u0026rdquo; (Vaswani et al., 2017) Congratulations! You\u0026rsquo;ve completed 10 weeks of comprehensive NLP training. You now understand:\nHow transformers work (math \u0026amp; implementation) How transfer learning enables rapid development How to fine-tune models for specific tasks How to deploy models in production Now go build something amazing! 🚀\nThe future of NLP is not about building models from scratch—it\u0026rsquo;s about creatively applying pre-trained models to solve real problems. You have the foundation to do exactly that.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.5-week5/",
	"title": "Week 5 - AWS Security &amp; Identity",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-06 to 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 5 Overview This week concentrated on AWS security and identity management foundations.\nKey Topics Shared Responsibility Model AWS IAM (Users, Groups, Roles, Policies) Amazon Cognito AWS Organizations \u0026amp; SCPs AWS Identity Center (SSO) AWS KMS AWS Security Hub Hands-on Labs Lab 18: AWS Security Hub Lab 22: AWS Lambda Automation with Slack Lab 27: AWS Resource Groups \u0026amp; Tagging Lab 28: IAM Cross-Region Role \u0026amp; Policy Lab 30: IAM Restriction Policy Lab 33: AWS KMS \u0026amp; CloudTrail Integration Lab 44: IAM Advanced Role Control Lab 48: IAM Access Keys \u0026amp; Roles "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/hugo_aws/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/hugo_aws/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.6-week6/",
	"title": "Week 6 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-13 to 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 6 Overview This week focused on the AWS database landscape, covering managed relational engines, purpose-built NoSQL stores, in-memory caching, and analytics data warehouses.\nKey Topics Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Hands-on Labs Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "
},
{
	"uri": "http://localhost:1313/hugo_aws/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.7-week7/",
	"title": "Week 7 - Vertical Slice Delivery",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-20 to 2025-10-24\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 7 Overview This week delivered vertical slice 0 for the Ebook Demo project, emphasizing contract-first development and automated testing to enable an end-to-end demo.\nKey Topics Vertical Slice Architecture and the scope of slice 0 Contract-first development with OpenAPI + Prism mocks Next.js 16 App Router \u0026amp; Server Components FastAPI clean architecture and CORS configuration Schemathesis contract testing and retrospective learnings Hands-on Labs Demo checklist for vertical slice 0 Mock API with Prism and connect it to Next.js Refactor FastAPI backend following clean architecture Run Schemathesis and update the validated workflow "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.8-week8/",
	"title": "Week 8 - Natural Language Processing &amp; Deep Learning",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-27 to 2025-10-31\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 8 Overview This week provides a comprehensive deep-dive into Natural Language Processing (NLP), covering linguistic foundations, modern NLP applications, sequence-to-sequence architectures, and evaluation methodologies. From understanding phonetics to implementing machine translation systems, this week bridges theory and practice in NLP.\nKey Topics Linguistic Foundations Core components: Phonetics, Phonology, Morphology, Syntax, Semantics, Pragmatics Understanding how language structure informs NLP design NLP Applications Search engines and intent recognition Online advertising with NER and relationship extraction Voice assistants and speech recognition Chatbots with NLU/NLG pipelines Machine translation systems Text summarization (extractive \u0026amp; abstractive) Deep Learning Architectures Seq2seq models with encoder-decoder architecture LSTM deep dive: forget gate, input gate, cell state, output gate Attention mechanism and self-attention Neural Machine Translation (NMT) implementation Evaluation \u0026amp; Decoding BLEU score (precision-based) ROUGE score (recall-based) F1 score for MT evaluation Beam search decoding Minimum Bayes Risk (MBR) sampling Hands-on Labs Building voicebot and chatbot workflows Implementing LSTM for sequence modeling Creating encoder-decoder with attention Neural machine translation end-to-end Evaluating translation quality with BLEU/ROUGE Implementing beam search and MBR "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.9-week9/",
	"title": "Week 9 - Transformer Architecture &amp; Implementation",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-03 to 2025-11-07\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 9 Overview This week explores the Transformer architecture, a revolutionary model that replaced RNNs in NLP. We\u0026rsquo;ll understand why transformers are needed, how they work internally, and implement them from scratch. From attention mechanisms to the full encoder-decoder design, this week bridges theory and practical implementation.\nKey Topics RNN Limitations \u0026amp; Transformer Introduction Sequential processing bottlenecks in RNNs Vanishing gradient problems Information bottleneck with long sequences Why attention is all you need Transformer Architecture Encoder-decoder structure Multi-head attention layers Positional encoding Residual connections \u0026amp; layer normalization Feed-forward networks Attention Mechanisms Scale dot-product attention (core mechanism) Self-attention (same sentence) Masked attention (decoder) Encoder-decoder attention Multi-head attention for parallel computation Transformer Decoder \u0026amp; GPT2 Positional embeddings Decoder block implementation Feed-forward layer design Output probability calculation Applications \u0026amp; Models GPT-2 (Generative Pre-trained Transformer) BERT (Bidirectional Encoder Representations) T5 (Text-to-Text Transfer Transformer) Applications: Translation, Classification, QA, Summarization, Sentiment Analysis Learning Objectives ✅ Understand RNN limitations and why transformers solve them ✅ Grasp the complete transformer architecture ✅ Implement attention mechanisms from scratch ✅ Build a transformer decoder (GPT2-style) ✅ Recognize transformer applications and state-of-the-art models Daily Breakdown Day Focus Topics 41 RNN Problems Sequential processing, Vanishing gradients, Information bottleneck 42 Architecture Overview Encoder-decoder, Multi-head attention, Positional encoding 43 Attention Core Scale dot-product attention formula, Matrix operations, GPU efficiency 44 Attention Types Self-attention, Masked attention, Encoder-decoder attention 45 Decoder Implementation GPT2 architecture, Building blocks, Code walkthrough Prerequisites Deep understanding of RNNs, LSTMs, and attention from Week 8 Comfortable with matrix operations and linear algebra PyTorch or TensorFlow knowledge helpful Next Steps Study the paper \u0026ldquo;Attention is All You Need\u0026rdquo; (Vaswani et al., 2017) Implement transformer components incrementally Experiment with pre-trained models (BERT, GPT-2, T5) "
},
{
	"uri": "http://localhost:1313/hugo_aws/1-worklog/1.10-week10/",
	"title": "Week 10 - Transfer Learning, BERT &amp; T5",
	"tags": [],
	"description": "",
	"content": "Week: 2025-11-10 to 2025-11-14\nStatus: \u0026ldquo;Done\u0026rdquo;\nWeek 10 Overview This final week focuses on transfer learning and state-of-the-art models (BERT and T5). We\u0026rsquo;ll learn how pre-training on large unlabeled datasets improves downstream task performance. From masked language modeling to multi-task learning, this week synthesizes everything into production-ready NLP systems.\nKey Topics Transfer Learning Fundamentals Feature-based vs fine-tuning approaches Pre-training and downstream tasks Labeled vs unlabeled data Advantages: faster training, better predictions, smaller datasets BERT Architecture Bidirectional Encoder Representations Multi-layer bidirectional transformers 12 layers, 12 attention heads, 110M parameters (BERT-base) Encoder-only architecture Pre-training Objectives Masked Language Modeling (MLM): mask 15% of tokens, predict them Next Sentence Prediction (NSP): predict if sentences follow each other Loss combination: MLM + NSP BERT Inputs Token embeddings Segment embeddings (sentence A vs B) Positional embeddings [CLS] token for classification [SEP] token for separation T5 Model Text-to-Text Transfer Transformer Encoder-Decoder architecture Multi-task training with task prefixes State-of-the-art on 10+ benchmarks Fine-tuning Applications Sentiment Analysis Named Entity Recognition (NER) Question Answering Paraphrase Detection Semantic Similarity Text Classification Daily Breakdown Day Focus Topics 46 Transfer Learning Feature-based, Fine-tuning, Pre-training objectives 47 BERT Architecture Design, Pre-training, Bidirectionality 48 Pre-training Tasks MLM, NSP, Loss functions, Input representation 49 T5 Model Encoder-Decoder, Multi-tasking, Task prefixes 50 Fine-tuning \u0026amp; Apps Downstream tasks, Practical applications Learning Objectives ✅ Understand transfer learning principles ✅ Grasp BERT\u0026rsquo;s bidirectional context ✅ Implement masked language modeling ✅ Learn multi-task training with T5 ✅ Fine-tune models for downstream tasks Model Comparison Model Type Pre-training Context Parameters Word2Vec Static Skip-gram/CBOW Fixed window N/A ELMo Dynamic Language modeling Bi-directional LSTM 94M GPT Causal Language modeling Uni-directional 117M BERT Masked MLM + NSP Bi-directional 110M T5 Multi-task Masked span Both directions 220M-11B Key Insights ✅ Pre-training matters: Models trained on 800GB of text vastly outperform those trained from scratch ✅ Bidirectionality helps: BERT\u0026rsquo;s bidirectional context improves understanding ✅ Multi-task learning works: T5 handles multiple tasks with one model ✅ Fine-tuning is efficient: Takes days instead of months ✅ Unlabeled data is powerful: Pre-training uses no labels, just raw text\nPrerequisites Complete understanding of transformer architecture (Week 9) Familiar with NLP tasks (classification, NER, QA) PyTorch or TensorFlow knowledge helpful Next Steps Implement BERT fine-tuning for your task Experiment with T5 for text-to-text problems Use Hugging Face transformers library Explore larger models (RoBERTa, ELECTRA, XLNet) Deploy pre-trained models in production "
},
{
	"uri": "http://localhost:1313/hugo_aws/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hugo_aws/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
[
{
	"uri": "http://localhost:1313/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/1.1.1-day01-2025-09-08/",
	"title": "Day 01 - Introduction to Cloud Computing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-08 (Th·ª© Hai)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes What Is Cloud Computing? The on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Benefits of Cloud Computing Pay only for what you use, optimizing cost efficiency. Accelerate development through automation and managed services. Scale resources up or down as needed. Deploy applications globally in minutes. Why AWS? AWS has been the global cloud leader for 13 consecutive years (as of 2023). Unique culture, vision, and long-term customer obsession. AWS pricing philosophy: customers should pay less over time for the same resources. Every AWS Leadership Principle is focused on delivering real customer value. How to Get Started with AWS There are many learning paths‚Äîself-study is completely possible. Register an AWS Free Tier account to explore. Recommended course platforms: Udemy A Cloud Guru Explore AWS learning paths: AWS Learning Paths "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/1.6.1-day26-2025-10-13/",
	"title": "Day 26 - Database Fundamentals",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-13 (Th·ª© Hai)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Database Concepts Review A database is an organized (or semi-structured) collection of information stored on storage devices to support concurrent access by multiple users or programs with different goals. Sessions A session spans from the moment a client connects to the DBMS until the connection is terminated. Primary Key A primary key uniquely identifies each row in a relational table. Foreign Key A foreign key in one table references the primary key of another table, creating a relationship between them. Index An index accelerates data retrieval at the cost of extra writes and storage to maintain the index structure. Indexes locate data without scanning every row; they can be defined over one or more columns. Index Types:\nB-Tree: General purpose, balanced tree structure Hash: Fast equality lookups Bitmap: Efficient for low-cardinality columns Full-Text: Text search optimization Partitioning Partitioning splits a large table into smaller, independent pieces (partitions), potentially placed on different storage. Benefits: better query performance, easier maintenance, and scalability. Common types: Range (e.g., by date) List Hash Composite (combination) Partitioning Example:\n-- Range partitioning by date CREATE TABLE orders ( order_id INT, order_date DATE, amount DECIMAL ) PARTITION BY RANGE (YEAR(order_date)) ( PARTITION p2023 VALUES LESS THAN (2024), PARTITION p2024 VALUES LESS THAN (2025), PARTITION p2025 VALUES LESS THAN (2026) ); Execution Plan / Query Plan A query plan details how the DBMS will execute an SQL statement (access paths, joins, sorts). Types: Estimated plan (before execution) Actual plan (from executed query) Key operators: table scan, index seek/scan, nested loops, hash/merge join, sort, aggregate, filter. Database Logs Database logs record all changes (INSERT/UPDATE/DELETE) and operations. Typical log types: transaction, redo, undo, binary logs. Uses: recovery, integrity, consistency/durability (ACID), replication, performance analysis. Buffers A buffer pool caches pages read from disk to minimize I/O. Management strategies: Replacement: LRU, FIFO, Clock Write policies: immediate vs. deferred Prefetching to warm the cache Hands-On Labs Lab 05 ‚Äì Amazon RDS \u0026amp; EC2 Integration (Part 1) Create a VPC ‚Üí 05-2.1 Create EC2 Security Group ‚Üí 05-2.2 Create RDS Security Group ‚Üí 05-2.3 Create DB Subnet Group ‚Üí 05-2.4 "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: L√™ Tr·ªçng Nh√¢n\nPhone Number: 0397609967\nEmail: nhanle221199@gmail.com\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: SE190525\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/",
	"title": "Week 1 - Cloud Computing Fundamentals",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-08 ƒë·∫øn 2025-09-12\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 1 Tu·∫ßn n√†y t·∫≠p trung v√†o c√°c kh√°i ni·ªám c∆° b·∫£n v·ªÅ Cloud Computing, AWS Infrastructure, v√† c√°c c√¥ng c·ª• qu·∫£n l√Ω AWS.\nN·ªôi dung ch√≠nh Gi·ªõi thi·ªáu Cloud Computing v√† l·ª£i √≠ch AWS Global Infrastructure (Regions, AZs, Edge Locations) AWS Management Tools (Console, CLI, SDK) Cost Optimization v√† Support Plans AWS Well-Architected Framework Labs th·ª±c h√†nh Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "
},
{
	"uri": "http://localhost:1313/1-worklog/",
	"title": "Worklog - AWS Learning Journey",
	"tags": [],
	"description": "",
	"content": "Worklog T·ªïng quan ƒê√¢y l√† worklog ghi l·∫°i qu√° tr√¨nh h·ªçc AWS t·ª´ ng√†y 8/9/2025 ƒë·∫øn 22/10/2025 (33 ng√†y l√†m vi·ªác).\nC·∫•u tr√∫c Worklog ƒë∆∞·ª£c t·ªï ch·ª©c theo tu·∫ßn, m·ªói tu·∫ßn c√≥ 5 ng√†y l√†m vi·ªác (Th·ª© Hai ƒë·∫øn Th·ª© S√°u).\nTi·∫øn ƒë·ªô ‚úÖ Week 1 (8-12/9): Cloud Computing Fundamentals ‚úÖ Week 2 (15-19/9): AWS Networking Services ‚úÖ Week 3 (22-26/9): AWS Compute Services ‚úÖ Week 4 (29/9-3/10): AWS Storage Services ‚úÖ Week 5 (6-10/10): AWS Security \u0026amp; Identity ‚úÖ Week 6 (13-17/10): AWS Database Services üîÑ Week 7 (20-22/10): Advanced Topics (3 ng√†y) Th·ªëng k√™ T·ªïng s·ªë ng√†y: 33 ng√†y l√†m vi·ªác T·ªïng s·ªë tu·∫ßn: 6 tu·∫ßn ƒë·∫ßy ƒë·ªß + 3 ng√†y T·ªïng s·ªë labs: 25+ hands-on labs Ch·ªß ƒë·ªÅ ch√≠nh: 7 domains (Cloud Fundamentals, Networking, Compute, Storage, Security, Database, Advanced) N·ªôi dung ch√≠nh Cloud Computing Fundamentals\nAWS basics, infrastructure, management tools Cost optimization, support plans Well-Architected Framework Networking\nVPC, subnets, security groups, NACLs Load balancing (ALB, NLB, GWLB) VPC Peering, Transit Gateway VPN, Direct Connect Compute\nEC2, AMI, EBS, Instance Store Auto Scaling, pricing models Lightsail, EFS, FSx Storage\nS3, storage classes, Glacier Snow Family, Storage Gateway Disaster Recovery, AWS Backup Security \u0026amp; Identity\nIAM, Cognito, Organizations KMS, Security Hub Identity Center (SSO) Database\nRDS, Aurora, Redshift ElastiCache, DMS Database best practices Advanced Topics\nServerless (Lambda) Containers (ECS, EKS, ECR) Monitoring (CloudWatch, X-Ray, CloudTrail) "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/1.1.2-day02-2025-09-09/",
	"title": "Day 02 - AWS Global Infrastructure",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-09 (Th·ª© Ba)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Infrastructure Data Centers Each data center can host tens of thousands of servers. AWS builds and manages its own custom hardware for efficiency and reliability. Availability Zone (AZ) One or more physically separate data centers within a Region. Each AZ is designed for fault isolation. Connected via low-latency, high-throughput private links. AWS recommends deploying workloads across at least two AZs. Region A Region contains at least three Availability Zones. There are currently 25+ Regions worldwide. Regions are interconnected by the AWS backbone network. Most services are Region-scoped by default. Edge Locations Global network of edge sites designed to serve content with minimal latency. Used by services such as: Amazon CloudFront (CDN) AWS WAF (Web Application Firewall) Amazon Route 53 (DNS Service) Hands-On Labs Lab 01 ‚Äì AWS Account \u0026amp; IAM Setup Create an AWS Account ‚Üí 01-01 Configure Virtual MFA Device ‚Üí 01-02 Create Admin Group and Admin User ‚Üí 01-03 Account Authentication Support ‚Üí 01-04 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/1.2.2-day07-2025-09-16/",
	"title": "Day 07 - VPC Routing &amp; Network Interfaces",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-16 (Th·ª© Ba)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Routing \u0026amp; ENI Route Tables A route table defines how traffic is directed. Each VPC has a default route table containing only a local route allowing internal communication between subnets. Custom route tables can be created, but the local route cannot be deleted. Elastic Network Interface (ENI) An ENI is a virtual network card that can be moved between EC2 instances. It retains its private IP, Elastic IP address, and MAC address when reassigned. Elastic IP (EIP) is a static public IPv4 address that can be associated with an ENI. Unused EIPs incur charges. ENI Use Cases:\nManagement network separate from data network Network and security appliances Dual-homed instances with workloads on distinct subnets Low-budget, high-availability solution VPC Endpoints A VPC Endpoint enables private connectivity to supported AWS services via AWS PrivateLink without using the public Internet. Two types: Interface Endpoint: Uses an ENI with a private IP. Gateway Endpoint: Uses route tables (available for S3 and DynamoDB only). Hands-On Labs Lab 03 ‚Äì Amazon VPC \u0026amp; Networking (continued) Create Internet Gateway (IGW) ‚Üí 03-03.3 Create Route Table (Outbound via IGW) ‚Üí 03-03.4 Create Security Groups ‚Üí 03-03.5 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/1.3.2-day12-2025-09-23/",
	"title": "Day 12 - EC2 Storage &amp; Backup",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-23 (Th·ª© Ba)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Storage \u0026amp; Security Backup in EC2 AWS Backup provides centralized backup for AWS services including EC2. EBS Snapshots back up EBS volumes: Point-in-time backups Incremental (stores only changed blocks) Stored in S3 (not directly accessible) AMI Backup captures the full EC2 configuration as an image. Snapshot Best Practices:\nSchedule regular snapshots Copy snapshots to other regions for DR Tag snapshots for lifecycle management Use Amazon Data Lifecycle Manager (DLM) Key Pair Key Pairs are used for secure authentication when connecting to EC2: Public Key ‚Äì stored on the instance Private Key ‚Äì kept by the user for SSH (Linux) or RDP (Windows) Replaces passwords for better security. Important: If you lose your private key, AWS cannot recover it. Key Pair Management:\nCreate key pairs in AWS or import your own Store private keys securely Use different key pairs for different environments Rotate keys regularly Elastic Block Store (EBS) Amazon EBS provides persistent block storage for EC2 instances. Volume types: General Purpose SSD (gp2/gp3) ‚Äì balance between performance \u0026amp; cost Provisioned IOPS SSD (io1/io2) ‚Äì for high IOPS workloads Throughput Optimized HDD (st1) ‚Äì for large, sequential data Cold HDD (sc1) ‚Äì low-cost, infrequently accessed data Key Features\nAttach/detach volumes from instances Data persists when instances stop Create snapshots for backup or cross-region copy Automatically replicated within an AZ EBS Volume Comparison:\nType Use Case Max IOPS Max Throughput gp3 General purpose 16,000 1,000 MB/s io2 High performance 64,000 1,000 MB/s st1 Big data 500 500 MB/s sc1 Cold storage 250 250 MB/s "
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/1.4.2-day17-2025-09-30/",
	"title": "Day 17 - S3 Advanced Features",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-30 (Th·ª© Ba)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon S3 Static Website Hosting Host static websites (HTML, CSS, JS, images) directly from S3.\nKey Capabilities Simple setup: A few steps to enable static website hosting on a bucket. Low cost: Pay standard S3 storage and data transfer; no separate web server charges. Elastic scaling: Automatically handles traffic spikes. CDN integration: Easily front with Amazon CloudFront for global performance. Static Website Configuration:\n{ \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; }, \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; } } Cross-Origin Resource Sharing (CORS) CORS allows web resources (fonts, JavaScript, etc.) on one domain to request resources from another domain.\nConfiguring CORS on S3 Define policies: Specify which origins are permitted to access a bucket\u0026rsquo;s content. Control methods: Allow specific HTTP methods (GET, PUT, POST, etc.). Security posture: Prevent unauthorized cross-origin access. CORS Configuration Example:\n[ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;https://example.com\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;], \u0026#34;MaxAgeSeconds\u0026#34;: 3000 } ] Performance and Object Key Design Object key naming can significantly affect S3 performance:\nRandomized prefixes: Distribute keys across partitions for higher parallelism. Avoid sequential prefixes: Don\u0026rsquo;t use monotonically increasing prefixes (e.g., timestamps) for high-throughput workloads. Parallel access: Structure keys to enable concurrent reads/writes. Key Design Best Practices:\n‚ùå Bad: 2025-09-30-file1.jpg, 2025-09-30-file2.jpg ‚úÖ Good: a1b2/2025-09-30-file1.jpg, c3d4/2025-09-30-file2.jpg S3 Glacier ‚Äì Long-Term Archival S3 Glacier classes are optimized for ultra‚Äìlow-cost long-term storage.\nRetrieval Options Expedited / Fast: Minutes; highest cost. Standard: 3‚Äì5 hours; balanced cost. Bulk: 5‚Äì12 hours; lowest cost for large restores. Glacier Deep Archive The lowest-cost class for multi-year retention, with ~12-hour retrieval times.\nHands-On Labs Lab 57 ‚Äì Amazon S3 \u0026amp; CloudFront (Part 2) Configure Public Objects ‚Üí 57-5 Test Website ‚Üí 57-6 Block All Public Access ‚Üí 57-7.1 Configure CloudFront ‚Üí 57-7.2 Test CloudFront ‚Üí 57-7.3 Bucket Versioning ‚Üí 57-8 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/1.5.2-day22-2025-10-07/",
	"title": "Day 22 - IAM Policies &amp; Roles",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-07 (Th·ª© Ba)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes IAM Policies JSON documents defining permissions. Types: Identity-based policies (attached to principals) Resource-based policies (attached to resources) Evaluation rule: explicit Deny overrides Allow across all policies. Pattern to constrain S3 administration:\nAllow all s3:* actions on a specific bucket. Explicitly Deny all non-S3 actions. Policy Structure:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34; } } }] } Policy Evaluation Logic:\nBy default, all requests are denied Explicit allow overrides default deny Explicit deny overrides any allows Permissions boundaries limit maximum permissions IAM Roles Roles provide temporary permissions assumed by users, services, or external identities. Common use cases: Let an AWS service act on your behalf (e.g., EC2 ‚Üí S3 writes) Cross-account access Federation from external IdPs Credentials for apps on EC2 without storing access keys Benefits\nNo long-term credentials, short-lived sessions, least privilege, and easier large-scale access management. Role Types:\nService Role: For AWS services (EC2, Lambda, etc.) Cross-Account Role: Access resources in another account Identity Provider Role: For federated users Instance Profile: Container for EC2 instance role Hands-On Labs Lab 48 ‚Äì IAM Access Keys \u0026amp; Roles (Part 2) Use Access Key ‚Üí 48-2.2 Create IAM Role ‚Üí 48-3.1 Use IAM Role ‚Üí 48-3.2 Clean Up Resources ‚Üí 48-4 Lab 28 ‚Äì IAM Cross-Region Role \u0026amp; Policy (Part 1) Create IAM User ‚Üí 28-2.1 Create IAM Policy ‚Üí 28-3 Create IAM Role ‚Üí 28-4 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/1.7.2-day32-2025-10-21/",
	"title": "Day 32 - Container Services",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-21 (Th·ª© Ba)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nLecture Notes Containers Overview What are Containers? Containers package application code with dependencies into a single unit. Lightweight, portable, and consistent across environments. Share the host OS kernel (unlike VMs). Benefits:\nConsistent environments (dev, test, prod) Fast startup times Efficient resource utilization Easy scaling Microservices architecture support Docker Basics Docker Components:\nImage: Read-only template with application and dependencies Container: Running instance of an image Dockerfile: Instructions to build an image Registry: Store and distribute images (Docker Hub, ECR) Basic Dockerfile:\nFROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Amazon Elastic Container Registry (ECR) Amazon ECR is a fully managed Docker container registry. Secure, scalable, and reliable. Integrated with ECS, EKS, and Lambda. ECR Features:\nPrivate and public repositories Image scanning for vulnerabilities Lifecycle policies for image cleanup Cross-region and cross-account replication Encryption at rest IAM-based access control Amazon Elastic Container Service (ECS) Amazon ECS is a fully managed container orchestration service. Run and scale containerized applications. ECS Launch Types:\nEC2 Launch Type You manage EC2 instances More control over infrastructure Use Reserved Instances for cost savings Fargate Launch Type Serverless compute for containers No EC2 instance management Pay only for resources used Automatic scaling ECS Components:\nTask Definition: Blueprint for your application (like a Dockerfile for ECS) Task: Running instance of a task definition Service: Maintains desired number of tasks Cluster: Logical grouping of tasks/services Amazon Elastic Kubernetes Service (EKS) Amazon EKS is a managed Kubernetes service. Run Kubernetes without managing control plane. EKS Features:\nFully managed Kubernetes control plane Automatic version upgrades and patching Integration with AWS services (IAM, VPC, ELB) Multi-AZ for high availability Support for Fargate (serverless Kubernetes) EKS vs ECS:\nFeature ECS EKS Orchestrator AWS proprietary Kubernetes Learning Curve Easier Steeper Portability AWS-specific Multi-cloud Ecosystem AWS services Kubernetes ecosystem Cost Lower Higher (control plane cost) When to Use What? Use Lambda when:\nEvent-driven workloads Short-running tasks (\u0026lt; 15 min) Unpredictable traffic Minimal infrastructure management Use Containers (ECS/EKS) when:\nLong-running applications Complex microservices Need specific runtime environments Existing containerized applications Multi-cloud portability (EKS) Hands-On Labs Labs s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "KenFi ‚Äì AI Fitness Commerce \u0026amp; Membership Platform A Unified AWS Serverless Stack for Membership, E-Commerce, and AI Fitness Coaching 1. Executive Summary KenFi is an AI-powered Fitness Commerce and Membership Platform developed as a prototype for a local gym facility, with a scalable architecture designed to support future expansion into a full multi-branch fitness chain. The platform consolidates three traditionally fragmented operations‚Äîproduct retail, membership registration, and customer assistance‚Äîinto a single digital ecosystem.\nThrough an integrated e-commerce module, customers can purchase fitness products such as whey protein, supplements, apparel, and accessories directly from the platform. Membership plans‚Äîincluding monthly, quarterly, and annual subscriptions‚Äîcan be acquired online, upon which the system automatically generates a unique Customer ID to be presented to gym reception staff for check-in and access control.\nA key differentiator of KenFi is its AI-powered chatbot assistant, designed with a dual-tone personality: approachable and conversational during general interactions, yet precise and professional when providing fitness or nutrition guidance. Phase 1 introduces a calorie and nutrition calculator powered by the USDA FoodData Central API, enabling users to track dietary intake and estimate caloric expenditure based on physical activity levels.\nThe platform is hosted on AWS using a fully serverless architecture to minimize operational overhead and ensure long-term scalability. Secure online payment processing is supported through sandbox integrations with VNPay and PayPal, allowing seamless transition to production gateways upon deployment in a live environment.\nKenFi aims to streamline internal gym operations, reduce manual registration processes, and deliver a modern, data-driven experience for both gym owners and members. While initially deployed for a single facility, the system is technically architected to scale into a centralized platform capable of serving multiple locations under a unified brand.\n2. Problem Statement What‚Äôs the Problem? Most local gyms in Vietnam still rely heavily on manual processes. Memberships are often tracked on paper or basic Excel files. Customers must come directly to the counter to register or renew, causing delays and crowding during peak hours. There is no self-service option for purchasing membership or supplements online. Customer identity is not standardized‚Äîeach branch or staff member may use different formats, leading to confusion and inconsistent records. Additionally, nutrition and calorie consultation is usually informal, depending on whichever trainer is available, resulting in inconsistent advice.\nThe Expected Digital Experience Customers Want: Modern gym members expect the same level of convenience they receive from e-commerce or online banking. They want to subscribe to a training package directly on their phones, pay instantly using common payment gateways such as VNPay or PayPal, and receive a membership QR code or Customer ID immediately. They want clear guidance on calorie intake and supplement usage without needing to constantly ask the front desk. They also expect the system to recognize them instantly at check-in without repeating personal information.\nThe Gap in the Market: While large fitness chains in Vietnam such as California Fitness or CityGym have partial digital systems, there is no unified, affordable platform for small-to-medium gyms to operate with the same level of professionalism. Buying ready-made systems from foreign vendors is expensive and not optimized for Vietnamese workflows. Most ‚Äúgym management‚Äù software products currently available focus only on attendance tracking, without integrating e-commerce, nutrition guidance, or AI-based support.\nKenFi as the Solution: KenFi bridges this gap by offering an AWS-powered fitness commerce and membership platform tailored specifically for local gyms. It centralizes membership registration, digital payments, supplement sales, nutrition calculation, and customer identity into a single friendly ecosystem. Customers receive a unique Customer ID or QR Code immediately after purchasing a package, which can be shown to reception staff for instant recognition. A built-in AI chatbot acts as a friendly gym bro for casual questions, but switches to serious expert mode when discussing technical fitness topics. This platform enables small gyms to operate with the professionalism of major fitness chains without requiring heavy upfront infrastructure or maintenance.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Development Phases\nThe KenFi platform will be implemented in clearly defined stages to ensure rapid prototyping while maintaining long-term scalability.\nPhase 1 ‚Äì Architecture Planning and AWS Service Setup. Define the exact AWS resources required including Amplify, Cognito User Pool, API Gateway, Lambda functions, DynamoDB tables, and S3 buckets. Establish naming conventions, region selection, and IAM roles.\nPhase 2 ‚Äì Core Membership Workflow. Implement the end-to-end flow of registering an account, purchasing a training package, generating Customer ID and QR code, and storing membership validity in DynamoDB.\nPhase 3 ‚Äì Supplement Commerce Module. Build product catalog, add-to-cart flow, checkout via VNPay or PayPal sandbox, and order history tracking.\nPhase 4 ‚Äì Nutrition Calculator Integration. Connect Lambda functions to the USDA FoodData Central API, allowing users to input food items and receive calorie and macro breakdowns.\nPhase 5 ‚Äì AI Chatbot. Integrate a lightweight LLM model running on Lambda or optionally through Amazon Bedrock. Configure behavior switching between casual ‚Äúgym bro‚Äù mode and serious expert mode.\nAWS Services and Key Responsibilities\nAWS Amplify hosts the Next.js frontend and automates CI/CD from GitHub or CodeCommit.\nAmazon Cognito manages sign-up, login, password resets, and identity tokens.\nAmazon API Gateway exposes secure HTTPS endpoints for all application actions.\nAWS Lambda functions handle business logic such as membership purchase, QR code generation, and nutrition lookup.\nAmazon DynamoDB stores customer profiles, membership tokens, transaction records, and product inventory.\nAmazon S3 stores public assets such as QR images, banner images, and downloadable membership cards.\nOptional Amazon SES can be used to send confirmation emails with QR code attachments.\nQR Code Generation Process\nWhen a purchase is confirmed, Lambda generates a unique membership string using a prefix such as KF-YYYYMM-XXXX. A QR code is rendered via a QR generation library, temporarily stored in memory, and saved as an image file in S3 under a path such as /qr-codes/customer-id.png. The URL is returned to the frontend and optionally emailed to the customer.\nPayment Gateway Connection\nVNPay and PayPal sandbox accounts are used during development. The frontend redirects the user to the appropriate payment URL. Upon completion, VNPay or PayPal returns a callback to API Gateway with transaction status. A dedicated Lambda function verifies the payment signature and marks the membership as active.\nDeployment Strategy\nThe entire infrastructure can be provisioned manually through the AWS Console during the prototype stage. Once stable, infrastructure as code will be applied using AWS CDK or CloudFormation for reproducible deployments across environments. Versioning for Lambdas and rollback strategies will be enforced.\nTesting and Quality Control\nUnit testing is done on each Lambda function independently. Frontend integration is verified through Amplify preview builds. Full end-to-end testing is simulated with a dummy user registering, purchasing, receiving QR code, and checking in. Penetration testing focuses on access control and payment spoofing prevention.\n5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Amplify (Web Hosting and CI/CD Pipeline): approximately 0.30 to 0.50 USD per month under light traffic. Amazon Cognito (User Authentication and Identity Management): free for up to 50,000 monthly active users. Amazon API Gateway (HTTPS API Invocation Layer): typically 0.05 to 0.10 USD per month for under 10,000 calls. AWS Lambda (Business Logic Execution): near zero cost due to generous free tier (1 million requests per month). Amazon DynamoDB (Membership, Transaction, and Product Data Storage): approximately 0.03 to 0.05 USD per month for minimal read/write activity. Amazon S3 (Static Asset Storage for QR Codes and Product Images): between 0.05 and 0.10 USD per month depending on image volume. Total: ‚âà $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 Store.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/",
	"title": "Week 2 - AWS Networking Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-15 ƒë·∫øn 2025-09-19\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 2 Tu·∫ßn n√†y t·∫≠p trung v√†o c√°c d·ªãch v·ª• m·∫°ng c·ªßa AWS, t·ª´ VPC c∆° b·∫£n ƒë·∫øn c√°c gi·∫£i ph√°p k·∫øt n·ªëi n√¢ng cao.\nN·ªôi dung ch√≠nh Amazon VPC v√† Subnets Security Groups v√† NACLs Internet Gateway, NAT Gateway VPC Peering v√† Transit Gateway Elastic Load Balancing (ALB, NLB, GWLB) Labs th·ª±c h√†nh Lab 03: Amazon VPC \u0026amp; Networking Basics Lab 10: Hybrid DNS (Route 53 Resolver) Lab 19: VPC Peering Lab 20: AWS Transit Gateway "
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/1.1.3-day03-2025-09-10/",
	"title": "Day 03 - AWS Management Tools",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-10 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Management Tools AWS Management Console Log in as Root User or IAM User (requires 12-digit Account ID). Search and access individual service dashboards. Support Center allows you to open support cases directly. AWS Command Line Interface (CLI) Open-source command-line tool for interacting with AWS services. Provides functionality equivalent to the Console. Key Features:\nCross-platform support (Windows, macOS, Linux) Scriptable and automatable Direct access to AWS service APIs Supports profiles for multiple accounts AWS SDK (Software Development Kit) Simplifies integration of AWS services within applications. Handles authentication, retries, and data serialization/deserialization automatically. Supported Languages:\nPython (Boto3) JavaScript/Node.js Java .NET Ruby, PHP, Go, and more "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/1.2.3-day08-2025-09-17/",
	"title": "Day 08 - VPC Security &amp; Flow Logs",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-17 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Security Security Group (SG) A stateful virtual firewall that controls inbound and outbound traffic to AWS resources. Rules are based on protocol, port, source, or another security group. Only allow rules are supported. Applied to Elastic Network Interfaces (ENIs). Security Group Characteristics:\nStateful: return traffic automatically allowed Supports allow rules only Evaluates all rules before deciding Applies to instance level (ENI) Network Access Control List (NACL) A stateless virtual firewall that operates at the subnet level. Rules control inbound and outbound traffic by protocol, port, and source. Default NACL allows all traffic. NACL Characteristics:\nStateless: must explicitly allow return traffic Supports both allow and deny rules Rules processed in number order Applies to subnet level VPC Flow Logs Capture metadata about IP traffic to and from network interfaces in your VPC. Logs can be delivered to Amazon CloudWatch Logs or S3. Flow Logs do not record packet payloads. Flow Log Use Cases:\nTroubleshoot connectivity issues Monitor traffic patterns Security analysis Compliance requirements Hands-On Labs Lab 03 ‚Äì Amazon VPC \u0026amp; Networking (continued) Launch EC2 Instances in Subnets ‚Üí 04-1 Test Connection Between Instances ‚Üí 04-2 Create NAT Gateway (Private ‚Üî Internet) ‚Üí 04-3 EC2 Instance Connect Endpoint (no bastion) ‚Üí 04-5 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/1.3.3-day13-2025-09-24/",
	"title": "Day 13 - Instance Store &amp; User Data",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-24 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes EC2 Advanced Features Instance Store Instance Store provides temporary block-level storage physically attached to the EC2 host. Characteristics\nVery high I/O and throughput Data lost when instance stops or terminates Cannot be detached or snapshotted Use Cases\nCaching or temporary data processing Applications with their own redundancy or replication Instance Store vs EBS:\nFeature Instance Store EBS Persistence Temporary Persistent Performance Very high High Snapshot No Yes Detachable No Yes Cost Included Additional User Data User Data scripts run automatically at instance launch (once per AMI provision). Linux ‚Äì bash scripts Windows ‚Äì PowerShell scripts User Data Examples:\n#!/bin/bash yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \u0026#34;\u0026lt;h1\u0026gt;Hello from $(hostname -f)\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Metadata EC2 Instance Metadata provides details about the running instance such as private/public IP, hostname, and security groups. Often used in user data scripts for dynamic configuration. Accessing Metadata:\n# Get instance ID curl http://169.254.169.254/latest/meta-data/instance-id # Get public IP curl http://169.254.169.254/latest/meta-data/public-ipv4 # Get IAM role credentials curl http://169.254.169.254/latest/meta-data/iam/security-credentials/role-name Hands-On Labs Lab 07 ‚Äì AWS Budgets \u0026amp; Cost Management Create Budget by Template ‚Üí 07-01 Create Cost Budget Tutorial ‚Üí 07-02 Create Usage Budget ‚Üí 07-03 Create Reserved Instance Budget ‚Üí 07-04 Create Savings Plans Budget ‚Üí 07-05 Clean Up Budgets ‚Üí 07-06 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/1.4.3-day18-2025-10-01/",
	"title": "Day 18 - AWS Snow Family &amp; Hybrid Storage",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-01 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Snow Family Purpose-built devices and services to move large datasets into and out of AWS when networks are limited or data volumes are massive.\nAWS Snowcone: Small, rugged device (~8 TB). Suited for edge and remote sites. AWS Snowball: Snowball Edge Storage Optimized: Up to ~80 TB usable storage. Snowball Edge Compute Optimized: Adds powerful compute with ~42 TB storage. AWS Snowmobile: Exabyte-scale data transfer (up to 100 PB) in a secure containerized data center. Snow Family Comparison:\nDevice Storage Compute Use Case Snowcone 8 TB 2 vCPUs Edge, IoT Snowball Storage 80 TB 40 vCPUs Data migration Snowball Compute 42 TB 52 vCPUs Edge computing Snowmobile 100 PB N/A Datacenter migration When to Use Snow Family:\nLimited or expensive bandwidth Large data volumes (TB to PB) Remote or disconnected locations Edge computing requirements Regulatory data residency AWS Storage Gateway Hybrid cloud storage service that connects on-premises applications with cloud-backed storage.\nGateway Types File Gateway\nNFS/SMB file shares backed by S3 objects. Use cases: user shares, application backups, archives. Volume Gateway\niSCSI block storage backed by S3 with EBS snapshots. Modes: Cached volumes: Primary data in S3; local cache on-prem. Stored volumes: Primary data on-prem; async copy to S3. Use cases: on-prem block workloads with cloud backup/DR. Tape Gateway\nVirtual Tape Library (VTL) for existing backup apps (e.g., NetBackup, Veeam). Writes appear as tape but land in S3/Glacier. Use cases: tape replacement and archival modernization. Hands-On Labs Lab 24 ‚Äì AWS Storage Gateway (On-Premises Integration) Create Storage Gateway ‚Üí 24-2.1 Create File Shares ‚Üí 24-2.2 Mount File Shares On-Prem ‚Üí 24-2.3 Clean Up Resources ‚Üí 24-3 Lab 14 ‚Äì AWS VM Import/Export (Part 1) VMware Workstation ‚Üí 14-01 Export Virtual Machine from On-Premises ‚Üí 14-02.1 Upload Virtual Machine to AWS ‚Üí 14-02.2 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/1.5.3-day23-2025-10-08/",
	"title": "Day 23 - Amazon Cognito &amp; Organizations",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-08 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Cognito Managed authentication/authorization and user management for web \u0026amp; mobile apps. Components: User Pools: Sign-up/sign-in user directories. Identity Pools: Federated identities for temporary AWS credentials to access services. Cognito User Pools Features:\nSign-up and sign-in Social identity providers (Google, Facebook, Amazon) SAML identity providers Multi-factor authentication (MFA) Email and phone verification Custom authentication flows Lambda triggers for customization Cognito Identity Pools Features:\nTemporary AWS credentials Authenticated and unauthenticated access Role-based access control Integration with User Pools Support for external identity providers AWS Organizations Centrally manage multiple AWS accounts in a single organization. Benefits\nCentralized account management Consolidated Billing Hierarchies with Organizational Units (OUs) Guardrails with Service Control Policies (SCPs) Organizational Units (OUs) Group accounts by department, project, or environment; nest OUs for hierarchical policies. Example OU Structure:\nRoot ‚îú‚îÄ‚îÄ Production OU ‚îÇ ‚îú‚îÄ‚îÄ Web App Account ‚îÇ ‚îî‚îÄ‚îÄ Database Account ‚îú‚îÄ‚îÄ Development OU ‚îÇ ‚îú‚îÄ‚îÄ Dev Account ‚îÇ ‚îî‚îÄ‚îÄ Test Account ‚îî‚îÄ‚îÄ Security OU ‚îî‚îÄ‚îÄ Audit Account Consolidated Billing One invoice for all accounts; volume pricing benefits; no extra cost. Benefits:\nVolume discounts across accounts Easier tracking and reporting Simplified payment method Reserved Instance sharing Hands-On Labs Lab 28 ‚Äì IAM Cross-Region Role \u0026amp; Policy (Part 2) Switch Roles ‚Üí 28-5.1 Access EC2 Console ‚Äì Tokyo ‚Üí 28-5.2.1 Access EC2 Console ‚Äì N. Virginia ‚Üí 28-5.2.2 Create EC2 (No Qualified Tags) ‚Üí 28-5.2.3 Edit EC2 Resource Tag ‚Üí 28-5.2.4 Policy Check ‚Üí 28-5.2.5 Lab 27 ‚Äì AWS Resource Groups \u0026amp; Tagging (Part 1) Create EC2 Instance with Tag ‚Üí 27-2.1.1 Manage Tags in AWS Resources ‚Üí 27-2.1.2 Filter Resources by Tag ‚Üí 27-2.1.3 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/1.6.3-day28-2025-10-15/",
	"title": "Day 28 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-15 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Redshift Fully managed cloud data warehouse optimized for large-scale analytics (OLAP).\nColumnar storage, compression, MPP execution; scales from hundreds of GB to PB. Integrations: S3, Kinesis, DynamoDB, BI tools; strong security features. Concurrency Scaling adds capacity automatically during spikes. Architecture: cluster (leader node + compute nodes), each compute node has slices. Deployment options:\nRedshift Provisioned Redshift Serverless Redshift Spectrum (query S3 directly) Use cases: enterprise BI, data lake analytics, dashboards, trend analysis, forecasting.\nRedshift Features:\nColumnar Storage: Optimized for analytics queries Massively Parallel Processing (MPP): Distributes queries across nodes Result Caching: Speeds up repeated queries Automatic Compression: Reduces storage costs Workload Management (WLM): Query prioritization Concurrency Scaling: Handle burst workloads Redshift vs Traditional Data Warehouse:\nFeature Redshift Traditional DW Setup Minutes Weeks/Months Scaling Elastic Fixed capacity Cost Pay-as-you-go Large upfront Maintenance Managed Self-managed Redshift Spectrum:\nQuery data directly in S3 without loading Separate compute and storage Support for various file formats (Parquet, ORC, JSON) Cost-effective for infrequently accessed data Hands-On Labs Lab 43 ‚Äì AWS Database Migration Service (DMS) (Part 2) MSSQL ‚Üí Aurora MySQL Target Config ‚Üí 43-07 MSSQL ‚Üí Aurora MySQL Create Project ‚Üí 43-08 MSSQL ‚Üí Aurora MySQL Schema Conversion ‚Üí 43-09 Oracle ‚Üí MySQL Schema Conversion (1) ‚Üí 43-10 Create Migration Task \u0026amp; Endpoints ‚Üí 43-11 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/1.7.3-day33-2025-10-22/",
	"title": "Day 33 - Monitoring &amp; Logging",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-22 (Th·ª© T∆∞)\nStatus: \u0026ldquo;Planned\u0026rdquo;\nLecture Notes AWS Monitoring Services Amazon CloudWatch Amazon CloudWatch monitors AWS resources and applications in real-time. Collects and tracks metrics, logs, and events. CloudWatch Components:\nCloudWatch Metrics Time-ordered data points (CPU, memory, disk, network, etc.) Default metrics for most AWS services Custom metrics for application-specific data Metric resolution: Standard (1 min) or High (1 sec) Common Metrics:\nEC2: CPUUtilization, NetworkIn/Out, DiskReadOps RDS: DatabaseConnections, ReadLatency, WriteLatency Lambda: Invocations, Duration, Errors, Throttles ELB: RequestCount, TargetResponseTime, HealthyHostCount CloudWatch Logs Centralized log management Real-time monitoring and analysis Log retention policies Export to S3 or stream to other services Log Groups and Streams:\nLog Group: /aws/lambda/my-function ‚îú‚îÄ‚îÄ Log Stream: 2025/10/22/[$LATEST]abc123 ‚îú‚îÄ‚îÄ Log Stream: 2025/10/22/[$LATEST]def456 ‚îî‚îÄ‚îÄ Log Stream: 2025/10/22/[$LATEST]ghi789 CloudWatch Alarms Automated actions based on metric thresholds States: OK, ALARM, INSUFFICIENT_DATA Actions: SNS notifications, Auto Scaling, EC2 actions Alarm Example:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;HighCPU\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;CPUUtilization\u0026#34;, \u0026#34;Threshold\u0026#34;: 80, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanThreshold\u0026#34;, \u0026#34;EvaluationPeriods\u0026#34;: 2, \u0026#34;Period\u0026#34;: 300, \u0026#34;Statistic\u0026#34;: \u0026#34;Average\u0026#34; } CloudWatch Dashboards Customizable views of metrics and logs Share across teams Cross-region and cross-account views AWS X-Ray AWS X-Ray helps analyze and debug distributed applications. Trace requests across microservices. X-Ray Features:\nService map visualization Request tracing Performance bottleneck identification Error and exception analysis Annotations and metadata X-Ray Use Cases:\nMicroservices debugging Performance optimization Dependency analysis Error root cause analysis AWS CloudTrail AWS CloudTrail records AWS API calls for auditing and compliance. Who did what, when, and from where. CloudTrail Features:\nEvent history (90 days free) Trail creation for long-term storage Log file integrity validation Multi-region and multi-account logging Integration with CloudWatch Logs CloudTrail Event Types:\nManagement Events: Control plane operations (CreateInstance, DeleteBucket) Data Events: Data plane operations (GetObject, PutObject) Insights Events: Unusual API activity detection Monitoring Best Practices Metrics Define KPIs: Identify key performance indicators Set baselines: Understand normal behavior Create alarms: Proactive alerting Use dashboards: Visualize important metrics Monitor costs: Track spending patterns Logging Centralize logs: Use CloudWatch Logs Structured logging: Use JSON format Log levels: DEBUG, INFO, WARN, ERROR Retention policies: Balance cost and compliance Log analysis: Use CloudWatch Insights or Athena Alerting Meaningful alerts: Avoid alert fatigue Actionable notifications: Include context Escalation policies: Define who gets notified Test alerts: Verify notification delivery Document runbooks: Response procedures Observability Three Pillars of Observability:\nMetrics: What is happening (quantitative) Logs: Detailed event records (qualitative) Traces: Request flow through system (distributed) AWS Observability Services:\nCloudWatch: Metrics and logs X-Ray: Distributed tracing CloudTrail: API audit logs AWS Distro for OpenTelemetry: Open-source observability Hands-On Labs Labs s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t\nWeek 7 Summary (Partial) 3 ng√†y ƒë·∫ßu tu·∫ßn 7 ƒë√£ gi·ªõi thi·ªáu:\n‚úÖ Serverless Computing v·ªõi AWS Lambda\n‚úÖ Container Services (ECS, EKS, ECR)\n‚úÖ Monitoring \u0026amp; Logging (CloudWatch, X-Ray, CloudTrail)\nT·ªïng k·∫øt 33 ng√†y l√†m vi·ªác (8/9 - 22/10/2025)\nƒê√£ ho√†n th√†nh 6 tu·∫ßn ƒë·∫ßy ƒë·ªß + 3 ng√†y c·ªßa tu·∫ßn 7, bao g·ªìm:\nCloud Fundamentals Networking Compute Storage Security Databases Serverless \u0026amp; Containers Monitoring C√°c tu·∫ßn ti·∫øp theo s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi c√≥ n·ªôi dung m·ªõi!\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/",
	"title": "Week 3 - AWS Compute Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-22 ƒë·∫øn 2025-09-26\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 3 Tu·∫ßn n√†y t·∫≠p trung v√†o c√°c d·ªãch v·ª• Compute c·ªßa AWS, ƒë·∫∑c bi·ªát l√† Amazon EC2 v√† c√°c d·ªãch v·ª• li√™n quan.\nN·ªôi dung ch√≠nh Amazon EC2 v√† Instance Types AMI v√† Backup Strategies EBS v√† Instance Store EC2 Auto Scaling EC2 Pricing Options Amazon Lightsail, EFS, FSx Labs th·ª±c h√†nh Lab 01: AWS Account \u0026amp; IAM Setup Lab 07: AWS Budgets \u0026amp; Cost Management Lab 09: AWS Support Plans "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/1.1.4-day04-2025-09-11/",
	"title": "Day 04 - Cost Optimization on AWS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-11 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Cost Optimization on AWS Cost Optimization Strategies Choose the right resource types and Regions. Use pricing models such as Reserved Instances, Savings Plans, and Spot Instances. Remove or schedule idle resources. Leverage serverless architectures. Continuously review and improve cost efficiency with AWS Budgets and Cost Explorer. Tag resources with Cost Allocation Tags for department-level tracking. AWS Pricing Calculator calculator.aws\nCreate and share cost estimates for common services. Pricing varies by Region. Key Features:\nEstimate costs before deployment Compare pricing across regions Export and share estimates Template-based estimation AWS Support Plans Four tiers: Basic, Developer, Business, and Enterprise. Plans can be upgraded temporarily during critical incidents. Support Plan Comparison Feature Basic Developer Business Enterprise Cost Free $29/month $100/month $15,000/month Response Time N/A 12-24 hours 1 hour (urgent) 15 min (critical) Technical Support Forums only Business hours 24/7 24/7 + TAM Hands-On Labs Lab 07 ‚Äì AWS Budgets \u0026amp; Cost Management Create Budget by Template ‚Üí 07-01 Create Cost Budget Tutorial ‚Üí 07-02 Create Usage Budget ‚Üí 07-03 Create Reserved Instance (RI) Budget ‚Üí 07-04 Create Savings Plans Budget ‚Üí 07-05 Clean Up Budgets ‚Üí 07-06 Lab 09 ‚Äì AWS Support Plans AWS Support Packages ‚Üí 09-01 Types of Support Requests ‚Üí 09-02 Change Support Package ‚Üí 09-03 Manage Support Requests ‚Üí 09-04 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/1.2.4-day09-2025-09-18/",
	"title": "Day 09 - VPC Connectivity &amp; Load Balancing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-18 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes VPC Peering \u0026amp; Transit Gateway VPC Peering Enables direct, private connectivity between two VPCs without traversing the Internet. Does not support transitive routing or overlapping CIDRs. VPC Peering Limitations:\nNo transitive peering No overlapping CIDR blocks Limited to 125 peering connections per VPC Cross-region peering supported AWS Transit Gateway (TGW) Acts as a hub to connect multiple VPCs and on-prem networks, simplifying complex mesh topologies. TGW Attachments associate subnets in specific AZs with a TGW. All subnets within the same AZ can reach the TGW once attached. Transit Gateway Benefits:\nCentralized connectivity hub Simplified network architecture Scalable to thousands of VPCs Supports inter-region peering VPN \u0026amp; Direct Connect Site-to-Site VPN Establishes a secure IPSec connection between an on-premises data center and AWS VPC. Consists of: Virtual Private Gateway (VGW): AWS-managed, multi-AZ endpoints. Customer Gateway (CGW): Customer-managed device or software appliance. AWS Direct Connect Provides a dedicated private network connection between an on-prem data center and AWS. Typical latency: 20‚Äì30 ms. In Vietnam, available through Hosted Connections (via partners). Bandwidth is adjustable. Hands-On Labs Lab 10 ‚Äì Hybrid DNS (Route 53 Resolver) Generate Key Pair ‚Üí 10-02.1 Initialize CloudFormation Template ‚Üí 10-02.2 Configure Security Group ‚Üí 10-02.3 Set up DNS System ‚Üí 10-05 Create Route 53 Outbound Endpoint ‚Üí 10-05.1 Create Resolver Rules ‚Üí 10-05.2 Create Inbound Endpoints ‚Üí 10-05.3 Lab 19 ‚Äì VPC Peering Initialize CloudFormation Templates ‚Üí 19-02.1 Create Security Group ‚Üí 19-02.2 Create EC2 Instance (Test Peering) ‚Üí 19-02.3 Create Peering Connection ‚Üí 19-04 Configure Route Tables (Cross-VPC) ‚Üí 19-05 Enable Cross-Peer DNS ‚Üí 19-06 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/1.3.4-day14-2025-09-25/",
	"title": "Day 14 - EC2 Auto Scaling",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-25 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon EC2 Auto Scaling EC2 Auto Scaling automatically adjusts the number of EC2 instances based on demand. Benefits\nElastic capacity adjustment Increased application availability Cost optimization Components\nAuto Scaling Group (ASG) ‚Äì logical group of EC2 instances Launch Template / Configuration ‚Äì defines instance parameters Scaling Policies ‚Äì rules for adding/removing instances Scaling Policies Simple / Step Scaling ‚Äì add/remove instances when thresholds are met Target Tracking ‚Äì maintain a metric (e.g., CPU = 50%) Scheduled Scaling ‚Äì scale on a predefined schedule Predictive Scaling ‚Äì uses ML to forecast and scale proactively Scaling Policy Examples:\n{ \u0026#34;TargetTrackingScalingPolicyConfiguration\u0026#34;: { \u0026#34;PredefinedMetricSpecification\u0026#34;: { \u0026#34;PredefinedMetricType\u0026#34;: \u0026#34;ASGAverageCPUUtilization\u0026#34; }, \u0026#34;TargetValue\u0026#34;: 50.0 } } Integration with Load Balancer ASGs often pair with Elastic Load Balancers (ELB). New instances automatically register; terminated instances deregister automatically. Auto Scaling Best Practices:\nUse multiple AZs for high availability Set appropriate cooldown periods Monitor CloudWatch metrics Use lifecycle hooks for custom actions Test scaling policies before production EC2 Pricing Options On-Demand: Pay per hour/second. Most expensive but flexible. Reserved Instances: 1- or 3-year commitment for discount; tied to specific instance type/family. Savings Plans: 1- or 3-year commitment; flexible across instance families. Spot Instances: Use spare capacity at up to 90% discount; can be terminated with 2-minute notice. Combine multiple pricing models within an Auto Scaling Group for cost optimization.\nPricing Comparison:\nModel Discount Flexibility Commitment On-Demand 0% High None Reserved 40-60% Low 1-3 years Savings Plans 40-60% Medium 1-3 years Spot 50-90% Low None Hands-On Labs Lab 09 ‚Äì AWS Support Plans AWS Support Packages ‚Üí 09-01 Types of Support Requests ‚Üí 09-02 Change Support Package ‚Üí 09-03 Manage Support Requests ‚Üí 09-04 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/1.4.4-day19-2025-10-02/",
	"title": "Day 19 - Disaster Recovery on AWS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-02 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Disaster Recovery (DR) on AWS Disaster Recovery is about restoring IT services after major incidents (outages, disasters, hardware failures, cyberattacks).\nRTO (Recovery Time Objective): How quickly to restore service. RPO (Recovery Point Objective): How much data loss (time window) is acceptable. DR Strategies (ordered by complexity \u0026amp; cost) Backup \u0026amp; Restore\nMaintain backups only (EBS/RDS snapshots, S3/Glacier). Restore to new infrastructure during incidents. RTO: hours‚Äìdays. RPO: depends on backup frequency. Cost: lowest. Pilot Light\nMinimal core services always running on AWS. Scale out to full production during DR. RTO: hours. RPO: minutes. Cost: moderate. Warm Standby\nFull system running at reduced scale on AWS. Scale up on failover. RTO: minutes‚Äìhours. RPO: seconds‚Äìminutes. Cost: higher. Multi-Site (Active/Active or Active/Passive)\nProduction running across on-prem and AWS, or multi-Region AWS. Traffic can be shifted instantly (Route 53, Global Accelerator). RTO/RPO: near zero. Cost: highest. DR Strategy Comparison:\nStrategy RTO RPO Cost Complexity Backup \u0026amp; Restore Hours-Days Hours $ Low Pilot Light Hours Minutes $$ Medium Warm Standby Minutes Seconds $$$ Medium-High Multi-Site Seconds Near-zero $$$$ High DR Best Practices Planning Define RTO and RPO requirements Document recovery procedures Identify critical systems and dependencies Establish communication plans Implementation Automate recovery processes Use multiple AZs and Regions Implement data replication Regular backup testing Testing Conduct DR drills regularly Test recovery procedures Measure actual RTO/RPO Update documentation Hands-On Labs Lab 14 ‚Äì AWS VM Import/Export (Part 2) Import Virtual Machine to AWS ‚Üí 14-02.3 Deploy Instance from AMI ‚Üí 14-02.4 Set Up S3 Bucket ACL ‚Üí 14-03.1 Export Virtual Machine from Instance ‚Üí 14-03.2 Resource Cleanup on AWS ‚Üí 14-05 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/1.5.4-day24-2025-10-09/",
	"title": "Day 24 - SCPs, Identity Center &amp; KMS",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-09 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Service Control Policies (SCPs) Define maximum permissions for accounts; they limit but do not grant permissions. Apply to accounts or OUs; affect all users/roles, including root; Deny overrides Allow. Example SCP (deny bucket deletion)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:DeleteBucket\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } SCP Use Cases:\nPrevent accounts from leaving the organization Restrict regions where resources can be created Enforce encryption requirements Prevent disabling security services Require specific tags on resources SCP Best Practices:\nStart with least privilege Test in non-production first Use explicit denies for critical controls Document SCP purposes Regular review and updates AWS Identity Center (formerly AWS SSO) Centralizes access to AWS accounts and external applications. Identity sources: built-in, AWS Managed Microsoft AD, on-prem AD (trust/AD Connector), or external IdPs. Permission Sets define what users/groups can do in target accounts (materialized as IAM roles). Multiple permission sets per user are supported. Identity Center Features:\nSingle sign-on to multiple AWS accounts Integration with Microsoft Active Directory SAML 2.0 support Multi-factor authentication Centralized permission management Audit logging with CloudTrail AWS Key Management Service (KMS) Managed keys for data protection with deep service integration and full auditability. Highlights\nCreate/manage keys without operating your own HSM infrastructure. Fine-grained access via IAM \u0026amp; key policies; usage logged in CloudTrail. Key categories\nCustomer-managed keys, AWS-managed keys, and AWS-owned keys. KMS Key Types:\nSymmetric: Single encryption key (AES-256) Asymmetric: Public/private key pair (RSA, ECC) KMS Features:\nAutomatic key rotation Key policies and grants Envelope encryption Integration with AWS services CloudTrail logging Multi-region keys Hands-On Labs Lab 33 ‚Äì AWS KMS \u0026amp; CloudTrail Integration (Part 1) Create Policy and Role ‚Üí 33-2.1 Create Group and User ‚Üí 33-2.2 Create KMS Key ‚Üí 33-3 Create S3 Bucket ‚Üí 33-4.1 Upload Data to S3 ‚Üí 33-4.2 Lab 30 ‚Äì IAM Restriction Policy Create Restriction Policy ‚Üí 30-3 Create IAM Limited User ‚Üí 30-4 Test IAM User Limits ‚Üí 30-5 Clean Up Resources ‚Üí 30-6 "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/1.6.4-day29-2025-10-16/",
	"title": "Day 29 - Amazon ElastiCache",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-16 (Th·ª© NƒÉm)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon ElastiCache Managed in-memory caching service for Redis and Memcached to reduce latency and offload databases.\nMicrosecond reads, Multi-AZ with failover, simple scaling, encryption/auth, automated ops. Redis: rich data structures, backups, replication, cluster mode. Memcached: simple, horizontally scalable cache with auto-discovery. Common uses: web/mobile acceleration, DB query caching, session stores, leaderboards, pub/sub, queues.\nElastiCache for Redis Features:\nData Structures: Strings, lists, sets, sorted sets, hashes, bitmaps, hyperloglogs Persistence: Snapshots and AOF (Append-Only File) Replication: Primary-replica with automatic failover Cluster Mode: Partition data across multiple shards Pub/Sub: Real-time messaging Lua Scripting: Server-side scripting Geospatial: Location-based queries ElastiCache for Memcached Features:\nMulti-threaded: Utilize multiple cores Auto Discovery: Automatic node discovery Horizontal Scaling: Add/remove nodes easily Simple: Easy to use, no persistence Redis vs Memcached:\nFeature Redis Memcached Data Structures Rich (lists, sets, etc.) Simple (key-value) Persistence Yes No Replication Yes No Multi-AZ Yes No Backup/Restore Yes No Pub/Sub Yes No Multi-threaded No Yes Caching Strategies Cache-Aside (Lazy Loading) Application checks cache first On miss, load from database and populate cache Pros: Only requested data is cached Cons: Cache miss penalty, stale data possible Write-Through Write to cache and database simultaneously Pros: Data always fresh, no cache misses on reads Cons: Write penalty, unused data may be cached Write-Behind (Write-Back) Write to cache immediately, async write to database Pros: Fast writes, reduced database load Cons: Risk of data loss, complexity Use Cases Session Store:\n# Store user session in Redis redis.setex(f\u0026#34;session:{user_id}\u0026#34;, 3600, session_data) # Retrieve session session = redis.get(f\u0026#34;session:{user_id}\u0026#34;) Leaderboard:\n# Add score to sorted set redis.zadd(\u0026#34;leaderboard\u0026#34;, {user_id: score}) # Get top 10 top_10 = redis.zrevrange(\u0026#34;leaderboard\u0026#34;, 0, 9, withscores=True) Rate Limiting:\n# Increment counter with expiry pipe = redis.pipeline() pipe.incr(f\u0026#34;rate:{user_id}\u0026#34;) pipe.expire(f\u0026#34;rate:{user_id}\u0026#34;, 60) count = pipe.execute()[0] if count \u0026gt; 100: raise RateLimitExceeded() Hands-On Labs Lab 43 ‚Äì AWS Database Migration Service (DMS) (Part 3) Inspect S3 ‚Üí 43-12 Create Serverless Migration ‚Üí 43-13 Create Event Notification ‚Üí 43-14 Logs ‚Üí 43-15 Troubleshoot: Memory Pressure ‚Üí 43-16 Troubleshoot: Table Error ‚Üí 43-17 "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/",
	"title": "Week 4 - AWS Storage Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-09-29 ƒë·∫øn 2025-10-03\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 4 Tu·∫ßn n√†y t·∫≠p trung v√†o c√°c d·ªãch v·ª• l∆∞u tr·ªØ c·ªßa AWS, t·ª´ S3 object storage ƒë·∫øn c√°c gi·∫£i ph√°p hybrid storage.\nN·ªôi dung ch√≠nh Amazon S3 v√† Storage Classes S3 Static Website Hosting S3 Glacier for Archival AWS Snow Family AWS Storage Gateway Disaster Recovery Strategies AWS Backup Labs th·ª±c h√†nh Lab 13: AWS Backup Lab 14: AWS VM Import/Export Lab 24: AWS Storage Gateway Lab 25: Amazon FSx Lab 57: Amazon S3 \u0026amp; CloudFront "
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/1.1.5-day05-2025-09-12/",
	"title": "Day 05 - AWS Well-Architected Framework",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-12 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nExploration AWS Well-Architected Framework A set of design principles and best practices for building reliable, secure, efficient, and cost-effective cloud architectures. The Well-Architected Tool in the Console provides self-assessments and improvement guidance. Six Pillars of Well-Architected Framework 1. Operational Excellence Focus on running and monitoring systems Continuous improvement of processes Automation of changes Response to events 2. Security Protect information and systems Identity and access management Detective controls Infrastructure protection Data protection 3. Reliability Recover from failures automatically Scale horizontally for resilience Test recovery procedures Manage change through automation 4. Performance Efficiency Use computing resources efficiently Select the right resource types Monitor performance Make informed decisions 5. Cost Optimization Avoid unnecessary costs Understand spending patterns Select appropriate services Optimize over time 6. Sustainability Minimize environmental impact Understand your impact Maximize utilization Use managed services Best Practices Review Design Principles Stop guessing capacity needs: Use auto-scaling Test at production scale: Clone environments easily Automate architecture experimentation: Use IaC Allow for evolutionary architectures: Design for change Drive architectures using data: Monitor and measure Improve through game days: Practice failure scenarios Week 1 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh c√°c ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªÅ AWS:\n‚úÖ Hi·ªÉu v·ªÅ Cloud Computing v√† l·ª£i √≠ch\n‚úÖ N·∫Øm ƒë∆∞·ª£c AWS Global Infrastructure\n‚úÖ Bi·∫øt c√°ch s·ª≠ d·ª•ng AWS Management Tools\n‚úÖ H·ªçc v·ªÅ Cost Optimization strategies\n‚úÖ T√¨m hi·ªÉu AWS Well-Architected Framework\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/1.2.5-day10-2025-09-19/",
	"title": "Day 10 - Elastic Load Balancing",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-19 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Elastic Load Balancing (ELB) Overview A fully managed service distributing traffic across multiple targets (EC2, containers, etc.). Supports HTTP, HTTPS, TCP, TLS. Can be deployed in public or private subnets. Provides DNS names; only NLB supports static IPs. Includes health checks and access logs (to S3). Supports sticky sessions (session affinity). Types: Application, Network, Classic, and Gateway Load Balancer. Application Load Balancer (ALB) Operates at Layer 7 (HTTP/HTTPS). Supports path-based routing (e.g., /mobile vs /desktop). Targets: EC2, Lambda, IP addresses, containers (ECS/EKS). ALB Features:\nHost-based routing Path-based routing HTTP header-based routing Query string parameter-based routing WebSocket support HTTP/2 support Network Load Balancer (NLB) Operates at Layer 4 (TCP/TLS). Supports static IPs and handles millions of requests per second. Targets: EC2, IP addresses, containers (ECS/EKS). NLB Features:\nUltra-low latency Static IP addresses Preserve source IP Long-lived TCP connections TLS termination Gateway Load Balancer (GWLB) Operates at Layer 3 (IP packets). Uses the GENEVE protocol on port 6081. Routes traffic to virtual appliances such as firewalls or monitoring tools. Partner list: aws.amazon.com/elasticloadbalancing/partners Exploration AWS Advanced Networking ‚Äì Specialty Study Guide Official study guide covering exam topics, AWS network design principles, and real-world architecture scenarios. Hands-On Labs Lab 20 ‚Äì AWS Transit Gateway Preparation Steps ‚Üí 20-02 Create Transit Gateway ‚Üí 20-03 Create TGW Attachments ‚Üí 20-04 Create TGW Route Tables ‚Üí 20-05 Add TGW Routes to VPC Route Tables ‚Üí 20-06 Week 2 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh ki·∫øn th·ª©c v·ªÅ AWS Networking:\n‚úÖ Amazon VPC v√† Subnets\n‚úÖ Security Groups v√† NACLs\n‚úÖ VPC Peering v√† Transit Gateway\n‚úÖ VPN v√† Direct Connect\n‚úÖ Elastic Load Balancing (ALB, NLB, GWLB)\nLabs completed: 4 labs (VPC Basics, Hybrid DNS, VPC Peering, Transit Gateway)\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/1.3.5-day15-2025-09-26/",
	"title": "Day 15 - Lightsail, EFS &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Date: 2025-09-26 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes Amazon Lightsail Simplified compute service with predictable monthly pricing (starting ~$3.5/month). Includes bundled data transfer at lower rates than EC2. Ideal for small workloads, development, or testing environments. Supports snapshots for backups. Runs inside a managed VPC and can connect to standard VPCs via one-click peering. Lightsail Use Cases:\nSimple web applications WordPress sites Development/test environments Small business applications Learning and experimentation Lightsail vs EC2:\nFeature Lightsail EC2 Pricing Fixed monthly Pay-as-you-go Complexity Simple Advanced Scalability Limited Unlimited Target Small projects Enterprise Amazon EFS (Elastic File System) Fully managed NFSv4 file system mountable by multiple EC2 instances simultaneously. Scales automatically to petabytes. Pay only for the storage used (unlike EBS provisioned size). Can be mounted from on-prem via VPN or Direct Connect. EFS Features:\nConcurrent access from multiple instances Automatic scaling Regional service (multi-AZ) Lifecycle management Encryption at rest and in transit EFS Storage Classes:\nStandard: Frequently accessed files Infrequent Access (IA): Lower cost for rarely accessed files One Zone: Single AZ for cost savings Amazon FSx Managed, scalable file systems for Windows, Lustre, and NetApp ONTAP. AWS handles setup, scaling, and backups. Accessible from EC2, on-prem servers, or users via SMB or NFS protocols. FSx Variants:\nFSx for Windows File Server Native Windows file system SMB protocol support Active Directory integration DFS namespaces FSx for Lustre High-performance computing Machine learning workloads Sub-millisecond latencies S3 integration FSx for NetApp ONTAP Multi-protocol (NFS, SMB, iSCSI) Data deduplication and compression Snapshots and replication AWS Application Migration Service (MGN) Used for migrating or replicating physical/virtual servers to AWS for DR or modernization. Continuously replicates source machines to lightweight staging instances on EC2. During cut-over, MGN launches fully functional EC2 instances from the replicated data. Migration Phases:\nInstall agent on source servers Continuous replication to AWS Testing with non-disruptive test instances Cutover to production Exploration Microsoft Workloads on AWS A curated series covering deployment, optimization, and best practices for running Microsoft workloads on AWS. Week 3 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh ki·∫øn th·ª©c v·ªÅ AWS Compute:\n‚úÖ Amazon EC2 v√† Instance Types\n‚úÖ AMI, EBS, Instance Store\n‚úÖ EC2 Auto Scaling\n‚úÖ EC2 Pricing Options\n‚úÖ Lightsail, EFS, FSx\nLabs completed: 3 labs (IAM Setup, Budgets, Support Plans)\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/1.4.5-day20-2025-10-03/",
	"title": "Day 20 - AWS Backup &amp; FSx",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-03 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Backup Centralized backup service for automating and governing data protection at scale.\nKey Capabilities Central management: Define and apply backup policies across services. Multi-service support: EC2, EBS, RDS, DynamoDB, EFS, Storage Gateway, S3, and more. Scheduling \u0026amp; lifecycle: Automate backups and retention. Compliance: Support for governance and audit requirements. Benefits Operational simplicity: No custom scripts or disparate tools. Time savings: Automated, policy-driven protection. Reporting \u0026amp; audit: Visibility into backup status and compliance. Backup Vault Lock Immutability controls to prevent modifications or deletions of protected backups for strict compliance. AWS Backup Features:\nCross-region backup copy Cross-account backup Backup policies (plans) Lifecycle management Encryption at rest Tag-based backup policies Backup Plan Example:\n{ \u0026#34;BackupPlanName\u0026#34;: \u0026#34;DailyBackups\u0026#34;, \u0026#34;Rules\u0026#34;: [{ \u0026#34;RuleName\u0026#34;: \u0026#34;DailyRule\u0026#34;, \u0026#34;ScheduleExpression\u0026#34;: \u0026#34;cron(0 5 ? * * *)\u0026#34;, \u0026#34;StartWindowMinutes\u0026#34;: 60, \u0026#34;CompletionWindowMinutes\u0026#34;: 120, \u0026#34;Lifecycle\u0026#34;: { \u0026#34;DeleteAfterDays\u0026#34;: 30, \u0026#34;MoveToColdStorageAfterDays\u0026#34;: 7 } }] } Exploration AWS Skill Builder Curated learning plans and deep-dive content for storage specialists: Storage Learning Plan: Block Storage Storage Learning Plan: Object Storage Hands-On Labs Lab 13 ‚Äì AWS Backup Create S3 Bucket ‚Üí 13-02.1 Deploy Infrastructure ‚Üí 13-02.2 Create Backup Plan ‚Üí 13-03 Set Up Notifications ‚Üí 13-04 Test Restore ‚Üí 13-05 Clean Up Resources ‚Üí 13-06 Lab 25 ‚Äì Amazon FSx (File Systems) Create SSD Multi-AZ File System ‚Üí 25-2.2 Create HDD Multi-AZ File System ‚Üí 25-2.3 Create New File Shares ‚Üí 25-3 Test Performance ‚Üí 25-4 Monitor Performance ‚Üí 25-5 Enable Data Deduplication ‚Üí 25-6 Enable Shadow Copies ‚Üí 25-7 Manage User Sessions and Open Files ‚Üí 25-8 Enable User Storage Quotas ‚Üí 25-9 Scale Throughput Capacity ‚Üí 25-11 Scale Storage Capacity ‚Üí 25-12 Delete Environment ‚Üí 25-13 Week 4 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh ki·∫øn th·ª©c v·ªÅ AWS Storage:\n‚úÖ Amazon S3 v√† Storage Classes\n‚úÖ S3 Static Website v√† CORS\n‚úÖ AWS Snow Family\n‚úÖ AWS Storage Gateway\n‚úÖ Disaster Recovery Strategies\n‚úÖ AWS Backup\nLabs completed: 5 labs (Backup, VM Import/Export, Storage Gateway, FSx, S3 \u0026amp; CloudFront)\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/1.5.5-day25-2025-10-10/",
	"title": "Day 25 - AWS Security Hub &amp; Automation",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-10 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Security Hub Aggregates and prioritizes security findings and posture across accounts/services. Capabilities\nAutomated checks, normalized findings, prioritized remediation workflows. Compliance standards: CIS AWS Foundations, PCI DSS, AWS Foundational Security Best Practices. Integrations\nGuardDuty, Inspector, Macie, Firewall Manager, IAM Access Analyzer, plus partner tools. Outcomes\nLess time aggregating, more time fixing; unified visibility and improved security hygiene. Security Hub Features:\nContinuous security posture monitoring Automated compliance checks Centralized findings across accounts Integration with 50+ AWS and partner services Custom insights and dashboards Automated remediation with EventBridge Security Standards:\nAWS Foundational Security Best Practices: 50+ controls CIS AWS Foundations Benchmark: Industry best practices PCI DSS: Payment card industry standards NIST: National Institute of Standards framework Security Automation AWS Services for Automation:\nAWS Config: Track resource configuration changes Amazon EventBridge: Event-driven automation AWS Lambda: Serverless remediation functions AWS Systems Manager: Automated patching and compliance Common Automation Patterns:\nAuto-remediate non-compliant resources Automated incident response Security group rule validation Encryption enforcement Tag compliance Exploration AWS Certified Security ‚Äì Specialty: All-in-One Exam Guide (SCS-C01) Comprehensive preparation material for the Security Specialty certification. Hands-On Labs Lab 18 ‚Äì AWS Security Hub Enable Security Hub ‚Üí 18-02 Score for Each Set of Criteria ‚Üí 18-03 Clean Up Resources ‚Üí 18-04 Lab 22 ‚Äì AWS Lambda Automation with Slack Create VPC ‚Üí 22-2.1 Create Security Group ‚Üí 22-2.2 Create EC2 Instance ‚Üí 22-2.3 Incoming Webhooks (Slack) ‚Üí 22-2.4 Create Tag for Instance ‚Üí 22-3 Create Role for Lambda ‚Üí 22-4 Function: Stop Instance ‚Üí 22-5.1 Function: Start Instance ‚Üí 22-5.2 Check Result ‚Üí 22-6 Clean Up Resources ‚Üí 22-7 Lab 27 ‚Äì AWS Resource Groups \u0026amp; Tagging (Part 2) Use Tags with CLI ‚Üí 27-2.2 Create a Resource Group ‚Üí 27-3 Clean Up Resources ‚Üí 27-4 Lab 33 ‚Äì AWS KMS \u0026amp; CloudTrail Integration (Part 2) Create CloudTrail ‚Üí 33-5.1 Log to CloudTrail ‚Üí 33-5.2 Create Amazon Athena ‚Üí 33-5.3 Query with Athena ‚Üí 33-5.4 Test \u0026amp; Share Encrypted S3 Data ‚Üí 33-6 Resource Cleanup ‚Üí 33-7 Lab 44 ‚Äì IAM Advanced Role Control Create IAM Group ‚Üí 44-2 Create IAM Users ‚Üí 44-3.1 Check Permissions ‚Üí 44-3.2 Create Admin IAM Role ‚Üí 44-4.1 Configure Switch Role ‚Üí 44-4.2 Restrict Switch Role by IP ‚Üí 44-4.3.1 Restrict Switch Role by Time ‚Üí 44-4.3.2 Clean Up Resources ‚Üí 44-5 Week 5 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh ki·∫øn th·ª©c v·ªÅ AWS Security:\n‚úÖ Shared Responsibility Model\n‚úÖ AWS IAM (Users, Groups, Roles, Policies)\n‚úÖ Amazon Cognito\n‚úÖ AWS Organizations \u0026amp; SCPs\n‚úÖ AWS Identity Center\n‚úÖ AWS KMS\n‚úÖ AWS Security Hub\nLabs completed: 8 labs (Security Hub, Lambda Automation, Resource Groups, IAM Policies, KMS \u0026amp; CloudTrail, Advanced Role Control)\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/1.6.5-day30-2025-10-17/",
	"title": "Day 30 - Database Migration &amp; Best Practices",
	"tags": [],
	"description": "",
	"content": "Date: 2025-10-17 (Th·ª© S√°u)\nStatus: \u0026ldquo;Done\u0026rdquo;\nLecture Notes AWS Database Migration Service (DMS) AWS DMS helps migrate databases to AWS quickly and securely with minimal downtime.\nKey Features:\nHomogeneous Migrations: Same database engine (e.g., Oracle to Oracle) Heterogeneous Migrations: Different engines (e.g., Oracle to Aurora) Continuous Replication: Keep source and target in sync Schema Conversion: AWS Schema Conversion Tool (SCT) Migration Types:\nFull Load: One-time migration of existing data Full Load + CDC: Initial load plus ongoing changes CDC Only: Replicate only ongoing changes Supported Sources:\nOracle, SQL Server, MySQL, PostgreSQL, MongoDB, SAP ASE, IBM Db2 Amazon RDS, Amazon Aurora, Amazon S3 Supported Targets:\nAmazon RDS, Amazon Aurora, Amazon Redshift, Amazon DynamoDB Amazon S3, Amazon Elasticsearch, Amazon Kinesis Data Streams Database Best Practices Performance Optimization RDS/Aurora:\nUse appropriate instance types Enable Enhanced Monitoring Optimize queries and indexes Use Read Replicas for read-heavy workloads Enable Performance Insights Redshift:\nChoose appropriate distribution keys Use sort keys for frequently filtered columns Vacuum and analyze tables regularly Use columnar compression Implement workload management (WLM) ElastiCache:\nChoose appropriate node types Use cluster mode for Redis scalability Implement proper cache eviction policies Monitor cache hit rates Use connection pooling Security Best Practices Encryption at Rest: Enable for all databases Encryption in Transit: Use SSL/TLS connections Network Isolation: Deploy in private subnets IAM Authentication: Use for RDS/Aurora when possible Secrets Manager: Store database credentials securely Security Groups: Restrict access to minimum required Audit Logging: Enable CloudWatch Logs and CloudTrail High Availability \u0026amp; Disaster Recovery RDS/Aurora:\nEnable Multi-AZ for production workloads Configure automated backups Test restore procedures regularly Use Aurora Global Database for multi-region DR Implement read replicas in different regions Redshift:\nEnable automated snapshots Copy snapshots to other regions Use Redshift Spectrum for data lake integration Implement cross-region snapshot copy ElastiCache:\nEnable Multi-AZ with automatic failover (Redis) Configure backup and restore (Redis) Use cluster mode for Redis scalability Implement application-level retry logic Cost Optimization Right-sizing: Choose appropriate instance types Reserved Instances: Commit for 1-3 years for discounts Aurora Serverless: For variable workloads Redshift Serverless: For intermittent analytics Storage Optimization: Use appropriate storage types Lifecycle Policies: Archive old data to S3/Glacier Monitor Usage: Use Cost Explorer and Budgets Exploration The Data Warehouse Toolkit Canonical reference for dimensional modeling and DW design patterns. Week 6 Summary Tu·∫ßn n√†y ƒë√£ ho√†n th√†nh ki·∫øn th·ª©c v·ªÅ AWS Database Services:\n‚úÖ Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP)\n‚úÖ Amazon RDS \u0026amp; Aurora\n‚úÖ Amazon Redshift\n‚úÖ Amazon ElastiCache\n‚úÖ AWS Database Migration Service\nLabs completed: 2 labs (RDS \u0026amp; EC2 Integration, Database Migration Service)\nT·ªïng k·∫øt 6 tu·∫ßn ƒë·∫ßu (8/9 - 17/10/2025) 30 ng√†y l√†m vi·ªác ƒë√£ ho√†n th√†nh:\nWeek 1: Cloud Computing Fundamentals AWS basics, infrastructure, management tools, cost optimization Week 2: AWS Networking Services VPC, subnets, security groups, load balancing, hybrid connectivity Week 3: AWS Compute Services EC2, AMI, storage, auto scaling, pricing models Week 4: AWS Storage Services S3, Glacier, Snow Family, Storage Gateway, backup \u0026amp; DR Week 5: AWS Security \u0026amp; Identity IAM, Cognito, Organizations, KMS, Security Hub Week 6: AWS Database Services RDS, Aurora, Redshift, ElastiCache, DMS T·ªïng s·ªë labs ho√†n th√†nh: 25+ labs\nTi·∫øp theo: Tu·∫ßn 7-8 s·∫Ω b·∫Øt ƒë·∫ßu t·ª´ ng√†y 20/10/2025 (Th·ª© Hai)\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 - AWS Security &amp; Identity",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-06 ƒë·∫øn 2025-10-10\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 5 Tu·∫ßn n√†y t·∫≠p trung v√†o b·∫£o m·∫≠t v√† qu·∫£n l√Ω danh t√≠nh tr√™n AWS.\nN·ªôi dung ch√≠nh Shared Responsibility Model AWS IAM (Users, Groups, Roles, Policies) Amazon Cognito AWS Organizations \u0026amp; SCPs AWS Identity Center (SSO) AWS KMS AWS Security Hub Labs th·ª±c h√†nh Lab 18: AWS Security Hub Lab 22: AWS Lambda Automation with Slack Lab 27: AWS Resource Groups \u0026amp; Tagging Lab 28: IAM Cross-Region Role \u0026amp; Policy Lab 30: IAM Restriction Policy Lab 33: AWS KMS \u0026amp; CloudTrail Integration Lab 44: IAM Advanced Role Control Lab 48: IAM Access Keys \u0026amp; Roles "
},
{
	"uri": "http://localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-13 ƒë·∫øn 2025-10-17\nStatus: \u0026ldquo;Done\u0026rdquo;\nT·ªïng quan tu·∫ßn 6 Tu·∫ßn n√†y t·∫≠p trung v√†o c√°c d·ªãch v·ª• c∆° s·ªü d·ªØ li·ªáu c·ªßa AWS, t·ª´ RDBMS ƒë·∫øn NoSQL v√† Data Warehouse.\nN·ªôi dung ch√≠nh Database Fundamentals (RDBMS, NoSQL, OLTP vs OLAP) Amazon RDS \u0026amp; Aurora Amazon Redshift Amazon ElastiCache AWS Database Migration Service (DMS) Labs th·ª±c h√†nh Lab 05: Amazon RDS \u0026amp; EC2 Integration Lab 43: AWS Database Migration Service (DMS) "
},
{
	"uri": "http://localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 - Advanced Topics (Partial)",
	"tags": [],
	"description": "",
	"content": "Week: 2025-10-20 ƒë·∫øn 2025-10-22\nStatus: \u0026ldquo;In Progress\u0026rdquo;\nT·ªïng quan tu·∫ßn 7 Tu·∫ßn n√†y b·∫Øt ƒë·∫ßu c√°c ch·ªß ƒë·ªÅ n√¢ng cao v·ªÅ AWS (ch·ªâ 3 ng√†y ƒë·∫ßu tu·∫ßn).\nN·ªôi dung ch√≠nh Serverless Computing Container Services Monitoring \u0026amp; Logging Labs th·ª±c h√†nh C√°c labs s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi c√≥ n·ªôi dung m·ªõi "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/1.2.1-day06-2025-09-15/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/1.3.1-day11-2025-09-22/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/1.4.1-day16-2025-09-29/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/1.5.1-day21-2025-10-06/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/1.6.2-day27-2025-10-14/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/1.7.1-day31-2025-10-20/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "TƒÉng t·ªëc lu·ªìng d·ªØ li·ªáu v√† AI c·ªßa b·∫°n b·∫±ng c√°ch k·∫øt n·ªëi ƒë·∫øn Amazon SageMaker Unified Studio t·ª´ Visual Studio Code b·ªüi Lauren Mullennex, Anagha Barve, Anchit Gupta, v√† Bhargava Varadharajan v√†o ng√†y 12 TH√ÅNG 9 2025 trong Amazon SageMaker AI, Amazon SageMaker Unified Studio, Announcements, Intermediate (200), Technical How-to\nC√°c nh√† ph√°t tri·ªÉn v√† k·ªπ s∆∞ h·ªçc m√°y (ML) gi·ªù ƒë√¢y c√≥ th·ªÉ k·∫øt n·ªëi tr·ª±c ti·∫øp t·ªõi Amazon SageMaker Unified Studio t·ª´ tr√¨nh so·∫°n th·∫£o Visual Studio Code (VS Code) c·ª•c b·ªô c·ªßa h·ªç. V·ªõi kh·∫£ nƒÉng n√†y, b·∫°n c√≥ th·ªÉ gi·ªØ nguy√™n quy tr√¨nh ph√°t tri·ªÉn hi·ªán c√≥ v√† c·∫•u h√¨nh m√¥i tr∆∞·ªùng ph√°t tri·ªÉn t√≠ch h·ª£p (IDE) c√° nh√¢n h√≥a, ƒë·ªìng th·ªùi truy c·∫≠p c√°c d·ªãch v·ª• ph√¢n t√≠ch AWS v√† tr√≠ tu·ªá nh√¢n t·∫°o \u0026amp; m√°y h·ªçc (AI/ML) trong m·ªôt m√¥i tr∆∞·ªùng ph√°t tri·ªÉn d·ªØ li·ªáu v√† AI h·ª£p nh·∫•t. S·ª± t√≠ch h·ª£p n√†y cung c·∫•p truy c·∫≠p li·ªÅn m·∫°ch t·ª´ m√¥i tr∆∞·ªùng ph√°t tri·ªÉn c·ª•c b·ªô c·ªßa b·∫°n ƒë·∫øn c∆° s·ªü h·∫° t·∫ßng c√≥ th·ªÉ m·ªü r·ªông ƒë·ªÉ ch·∫°y x·ª≠ l√Ω d·ªØ li·ªáu, ph√¢n t√≠ch SQL v√† c√°c lu·ªìng c√¥ng vi·ªác ML. B·∫±ng c√°ch k·∫øt n·ªëi IDE c·ª•c b·ªô c·ªßa b·∫°n v·ªõi SageMaker Unified Studio, b·∫°n c√≥ th·ªÉ t·ªëi ∆∞u h√≥a lu·ªìng ph√°t tri·ªÉn d·ªØ li·ªáu v√† AI m√† kh√¥ng l√†m gi√°n ƒëo·∫°n c√°c th·ª±c ti·ªÖn ph√°t tri·ªÉn ƒë√£ thi·∫øt l·∫≠p.\nTrong b√†i vi·∫øt n√†y, ch√∫ng t√¥i minh h·ªça c√°ch k·∫øt n·ªëi VS Code c·ª•c b·ªô c·ªßa b·∫°n ƒë·∫øn SageMaker Unified Studio ƒë·ªÉ b·∫°n c√≥ th·ªÉ x√¢y d·ª±ng lu·ªìng c√¥ng vi·ªác d·ªØ li·ªáu v√† AI ƒë·∫ßu-cu·ªëi trong khi l√†m vi·ªác trong m√¥i tr∆∞·ªùng ph√°t tri·ªÉn ∆∞a th√≠ch c·ªßa b·∫°n.\nT·ªïng quan gi·∫£i ph√°p Ki·∫øn tr√∫c gi·∫£i ph√°p bao g·ªìm ba th√†nh ph·∫ßn ch√≠nh:\nM√°y t√≠nh c·ª•c b·ªô ‚Äì M√°y ph√°t tri·ªÉn c·ªßa b·∫°n ch·∫°y VS Code v·ªõi AWS Toolkit cho Visual Studio Code v√† Microsoft Remote SSH ƒë∆∞·ª£c c√†i ƒë·∫∑t. B·∫°n c√≥ th·ªÉ k·∫øt n·ªëi th√¥ng qua extension Toolkit cho VS Code b·∫±ng c√°ch duy·ªát c√°c kh√¥ng gian (spaces) SageMaker Unified Studio c√≥ s·∫µn v√† ch·ªçn m√¥i tr∆∞·ªùng m·ª•c ti√™u c·ªßa ch√∫ng.\nSageMaker Unified Studio ‚Äì L√† ph·∫ßn c·ªßa th·∫ø h·ªá k·∫ø ti·∫øp c·ªßa Amazon SageMaker, SageMaker Unified Studio l√† m·ªôt m√¥i tr∆∞·ªùng ph√°t tri·ªÉn d·ªØ li·ªáu v√† AI duy nh·∫•t, n∆°i b·∫°n c√≥ th·ªÉ t√¨m v√† truy c·∫≠p d·ªØ li·ªáu c·ªßa m√¨nh v√† thao t√°c n√≥ b·∫±ng c√°c c√¥ng c·ª• AWS quen thu·ªôc cho ph√¢n t√≠ch SQL, x·ª≠ l√Ω d·ªØ li·ªáu, ph√°t tri·ªÉn m√¥ h√¨nh v√† ph√°t tri·ªÉn ·ª©ng d·ª•ng AI t·∫°o sinh.\nAWS Systems Manager ‚Äì M·ªôt d·ªãch v·ª• truy c·∫≠p t·ª´ xa v√† qu·∫£n l√Ω an to√†n, c√≥ kh·∫£ nƒÉng m·ªü r·ªông, gi√∫p k·∫øt n·ªëi li·ªÅn m·∫°ch gi·ªØa VS Code c·ª•c b·ªô c·ªßa b·∫°n v√† c√°c kh√¥ng gian SageMaker Unified Studio ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a lu·ªìng ph√°t tri·ªÉn d·ªØ li·ªáu v√† AI.\nS∆° ƒë·ªì sau ƒë√¢y bi·ªÉu di·ªÖn s·ª± t∆∞∆°ng t√°c gi·ªØa IDE c·ª•c b·ªô c·ªßa b·∫°n v√† c√°c kh√¥ng gian SageMaker Unified Studio.\nC√°c ƒëi·ªÅu ki·ªán ti√™n quy·∫øt ƒê·ªÉ th·ª≠ k·∫øt n·ªëi IDE t·ª´ xa, b·∫°n ph·∫£i c√≥ c√°c ƒëi·ªÅu ki·ªán sau:\nTruy c·∫≠p v√†o domain SageMaker Unified Studio c√≥ k·∫øt n·ªëi Internet. V·ªõi c√°c domain ƒë∆∞·ª£c c·∫•u h√¨nh ·ªü ch·∫ø ƒë·ªô ch·ªâ VPC, domain c·ªßa b·∫°n ph·∫£i c√≥ tuy·∫øn ra Internet qua proxy ho·∫∑c NAT gateway. N·∫øu domain c·ªßa b·∫°n ho√†n to√†n c√¥ l·∫≠p kh·ªèi Internet, tham kh·∫£o t√†i li·ªáu ƒë·ªÉ thi·∫øt l·∫≠p k·∫øt n·ªëi t·ª´ xa. N·∫øu b·∫°n ch∆∞a c√≥ domain SageMaker Unified Studio, b·∫°n c√≥ th·ªÉ t·∫°o m·ªôt domain b·∫±ng t√πy ch·ªçn thi·∫øt l·∫≠p nhanh (quick setup) ho·∫∑c thi·∫øt l·∫≠p th·ªß c√¥ng (manual setup).\nM·ªôt ng∆∞·ªùi d√πng v·ªõi th√¥ng tin ƒëƒÉng nh·∫≠p SSO th√¥ng qua IAM Identity Center ƒë∆∞·ª£c y√™u c·∫ßu. ƒê·ªÉ c·∫•u h√¨nh truy c·∫≠p ng∆∞·ªùi d√πng SSO, h√£y xem t√†i li·ªáu.\nTruy c·∫≠p ho·∫∑c c√≥ th·ªÉ t·∫°o m·ªôt d·ª± √°n SageMaker Unified Studio.\nCompute Space JupyterLab ho·∫∑c Code Editor v·ªõi y√™u c·∫ßu lo·∫°i instance t·ªëi thi·ªÉu 8 GB b·ªô nh·ªõ. Trong b√†i vi·∫øt n√†y, ch√∫ng t√¥i d√πng instance ml.t3.large. Phi√™n b·∫£n ·∫£nh ph√¢n ph·ªëi SageMaker Distribution image phi√™n b·∫£n 2.8 tr·ªü l√™n ƒë∆∞·ª£c h·ªó tr·ª£.\nB·∫°n c√≥ VS Code b·∫£n ·ªïn ƒë·ªãnh m·ªõi nh·∫•t v·ªõi Microsoft Remote SSH (phi√™n b·∫£n 0.74.0 tr·ªü l√™n) v√† extension AWS Toolkit (phi√™n b·∫£n 3.74.0) ƒë∆∞·ª£c c√†i ƒë·∫∑t tr√™n m√°y c·ª•c b·ªô c·ªßa b·∫°n.\nTri·ªÉn khai gi·∫£i ph√°p ƒê·ªÉ cho ph√©p k·∫øt n·ªëi t·ª´ xa v√† k·∫øt n·ªëi t·ªõi kh√¥ng gian t·ª´ VS Code, ho√†n t·∫•t c√°c b∆∞·ªõc sau. ƒê·ªÉ k·∫øt n·ªëi t·ªõi m·ªôt space SageMaker Unified Studio t·ª´ xa, space ƒë√≥ ph·∫£i ƒë∆∞·ª£c b·∫≠t t√≠nh nƒÉng truy c·∫≠p t·ª´ xa.\nƒêi·ªÅu h∆∞·ªõng t·ªõi space JupyterLab ho·∫∑c Code Editor c·ªßa b·∫°n. N·∫øu n√≥ ƒëang ch·∫°y, d·ª´ng space v√† ch·ªçn Configure space ƒë·ªÉ b·∫≠t truy c·∫≠p t·ª´ xa\nB·∫≠t Remote access ƒë·ªÉ k√≠ch ho·∫°t t√≠nh nƒÉng v√† ch·ªçn Save and restart.\nƒêi·ªÅu h∆∞·ªõng t·ªõi AWS Toolkit trong c√†i ƒë·∫∑t VS Code c·ª•c b·ªô c·ªßa b·∫°n.\nTr√™n tab SageMaker Unified Studio, ch·ªçn Sign in ƒë·ªÉ b·∫Øt ƒë·∫ßu v√† cung c·∫•p URL domain SageMaker Unified Studio, v√≠ d·ª• https://\u0026lt;domain‚Äëid\u0026gt;.sagemaker.\u0026lt;region\u0026gt;.on.aws.\nB·∫°n s·∫Ω ƒë∆∞·ª£c y√™u c·∫ßu chuy·ªÉn h∆∞·ªõng sang tr√¨nh duy·ªát web ƒë·ªÉ cho ph√©p truy c·∫≠p c√°c extension IDE AWS. Ch·ªçn Open ƒë·ªÉ m·ªü tab tr√¨nh duy·ªát m·ªõi.\nCh·ªçn Allow access ƒë·ªÉ k·∫øt n·ªëi t·ªõi d·ª± √°n qua VS Code.\nB·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c th√¥ng b√°o Request approved, cho th·∫•y b·∫°n ƒë√£ c√≥ quy·ªÅn truy c·∫≠p domain t·ª´ xa.\nQuay l·∫°i VS Code c·ª•c b·ªô c·ªßa b·∫°n ƒë·ªÉ truy c·∫≠p d·ª± √°n v√† ti·∫øp t·ª•c x√¢y d·ª±ng c√°c c√¥ng vi·ªác ETL, pipeline d·ªØ li·ªáu, ƒë√†o t·∫°o \u0026amp; tri·ªÉn khai m√¥ h√¨nh ML ho·∫∑c x√¢y ·ª©ng d·ª•ng AI t·∫°o sinh. ƒê·ªÉ k·∫øt n·ªëi t·ªõi d·ª± √°n cho x·ª≠ l√Ω d·ªØ li·ªáu v√† ph√°t tri·ªÉn ML, th·ª±c hi·ªán c√°c b∆∞·ªõc:\nCh·ªçn Select a project ƒë·ªÉ xem d·ªØ li·ªáu v√† t√†i nguy√™n t√≠nh to√°n. T·∫•t c·∫£ c√°c d·ª± √°n trong domain ƒë∆∞·ª£c li·ªát k√™, nh∆∞ng b·∫°n ch·ªâ ƒë∆∞·ª£c ph√©p truy c·∫≠p c√°c d·ª± √°n m√† b·∫°n l√† th√†nh vi√™n.\nB·∫°n ch·ªâ c√≥ th·ªÉ xem m·ªôt domain v√† m·ªôt d·ª± √°n t·∫°i m·ªôt th·ªùi ƒëi·ªÉm. ƒê·ªÉ chuy·ªÉn d·ª± √°n ho·∫∑c ƒëƒÉng xu·∫•t kh·ªèi domain, ch·ªçn bi·ªÉu t∆∞·ª£ng d·∫•u ba ch·∫•m.B·∫°n c≈©ng c√≥ th·ªÉ xem t√†i nguy√™n d·ªØ li·ªáu v√† t√≠nh to√°n m√† b·∫°n ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥.\nK·∫øt n·ªëi space JupyterLab ho·∫∑c Code Editor b·∫±ng c√°ch ch·ªçn bi·ªÉu t∆∞·ª£ng k·∫øt n·ªëi. N·∫øu t√πy ch·ªçn n√†y kh√¥ng hi·ªÉn th·ªã, c√≥ th·ªÉ b·∫°n ƒë√£ t·∫Øt truy c·∫≠p t·ª´ xa trong space. N·∫øu space ƒëang ·ªü tr·∫°ng th√°i ‚ÄúStopped‚Äù, di chu·ªôt l√™n space v√† ch·ªçn n√∫t connect, ƒëi·ªÅu n√†y s·∫Ω b·∫≠t truy c·∫≠p t·ª´ xa, kh·ªüi ƒë·ªông space v√† k·∫øt n·ªëi n√≥. N·∫øu space ƒëang ·ªü tr·∫°ng th√°i ‚ÄúRunning‚Äù, space ph·∫£i ƒë∆∞·ª£c kh·ªüi ƒë·ªông l·∫°i v·ªõi truy c·∫≠p t·ª´ xa ƒë∆∞·ª£c b·∫≠t b·∫±ng c√°ch d·ª´ng space r·ªìi k·∫øt n·ªëi l·∫°i t·ª´ toolkit.\nM·ªôt c·ª≠a s·ªï VS Code kh√°c s·∫Ω m·ªü ra v√† ƒë∆∞·ª£c k·∫øt n·ªëi t·ªõi space SageMaker Unified Studio c·ªßa b·∫°n qua remote SSH.\nƒêi·ªÅu h∆∞·ªõng t·ªõi Explorer ƒë·ªÉ xem notebook, file v√† script c·ªßa space. T·ª´ AWS Toolkit, b·∫°n c≈©ng c√≥ th·ªÉ xem ngu·ªìn d·ªØ li·ªáu c·ªßa b·∫°n.\nS·ª≠ d·ª•ng thi·∫øt l·∫≠p VS Code t√πy ch·ªânh v·ªõi t√†i nguy√™n SageMaker Unified Studio Khi b·∫°n k·∫øt n·ªëi VS Code v·ªõi SageMaker Unified Studio, b·∫°n gi·ªØ nguy√™n t·∫•t c·∫£ ph√≠m t·∫Øt c√° nh√¢n v√† t√πy ch·ªânh c·ªßa b·∫°n. V√≠ d·ª•, n·∫øu b·∫°n d√πng ƒëo·∫°n code snippet ƒë·ªÉ nhanh ch√≥ng ch√®n c√°c m·∫´u m√£ ph√¢n t√≠ch v√† ML ph·ªï bi·∫øn, ch√∫ng v·∫´n ho·∫°t ƒë·ªông v·ªõi c∆° s·ªü h·∫° t·∫ßng ƒë∆∞·ª£c qu·∫£n l√Ω c·ªßa SageMaker Unified Studio.\nTrong h√¨nh minh h·ªça, ch√∫ng t√¥i th·ªÉ hi·ªán c√°ch s·ª≠ d·ª•ng c√°c snippet lu·ªìng ph√¢n t√≠ch: snippet ‚Äúshow-databases‚Äù truy v·∫•n Athena ƒë·ªÉ hi·ªÉn th·ªã c√°c database c√≥ s·∫µn, ‚Äúshow-glue-tables‚Äù li·ªát k√™ b·∫£ng trong AWS Glue Data Catalog, v√† ‚Äúquery-ecommerce‚Äù l·∫•y d·ªØ li·ªáu s·ª≠ d·ª•ng Spark SQL ƒë·ªÉ ph√¢n t√≠ch.\nB·∫°n c≈©ng c√≥ th·ªÉ d√πng c√°c snippet ƒë·ªÉ t·ª± ƒë·ªông h√≥a vi·ªác build v√† training m√¥ h√¨nh ML tr√™n SageMaker AI. Trong h√¨nh b√™n d∆∞·ªõi, c√°c snippet m√£ th·ªÉ hi·ªán x·ª≠ l√Ω d·ªØ li·ªáu, c·∫•u h√¨nh v√† kh·ªüi ch·∫°y job ƒë√†o t·∫°o SageMaker AI. C√°ch ti·∫øp c·∫≠n n√†y cho th·∫•y ng∆∞·ªùi l√†m d·ªØ li·ªáu c√≥ th·ªÉ gi·ªØ thi·∫øt l·∫≠p ph√°t tri·ªÉn quen thu·ªôc c·ªßa m√¨nh trong khi s·ª≠ d·ª•ng t√†i nguy√™n d·ªØ li·ªáu v√† AI ƒë∆∞·ª£c qu·∫£n l√Ω trong SageMaker Unified Studio.\nV√¥ hi·ªáu h√≥a truy c·∫≠p t·ª´ xa trong SageMaker Unified Studio Nh∆∞ m·ªôt qu·∫£n tr·ªã vi√™n, n·∫øu b·∫°n mu·ªën v√¥ hi·ªáu h√≥a t√≠nh nƒÉng n√†y cho ng∆∞·ªùi d√πng, b·∫°n c√≥ th·ªÉ th·ª±c thi n√≥ b·∫±ng c√°ch th√™m ch√≠nh s√°ch sau v√†o vai tr√≤ IAM c·ªßa d·ª± √°n:\n{\n\u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;,\n\u0026ldquo;Statement\u0026rdquo;: [\n{\n\u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;DenyStartSessionForSpaces\u0026rdquo;,\n\u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Deny\u0026rdquo;,\n\u0026ldquo;Action\u0026rdquo;: [\n\u0026ldquo;sagemaker:StartSession\u0026rdquo;\n],\n\u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:sagemaker:*:*:space/*/*\u0026rdquo;\n}\n]\n}\nD·ªçn d·∫πp Theo m·∫∑c ƒë·ªãnh, SageMaker Unified Studio t·∫Øt c√°c t√†i nguy√™n nh√†n r·ªói nh∆∞ c√°c space JupyterLab v√† Code Editor sau 1 gi·ªù. N·∫øu b·∫°n ƒë√£ t·∫°o m·ªôt domain SageMaker Unified Studio cho m·ª•c ƒë√≠ch b√†i vi·∫øt n√†y, nh·ªõ x√≥a domain ƒë√≥.\nK·∫øt lu·∫≠n K·∫øt n·ªëi tr·ª±c ti·∫øp t·ª´ IDE c·ª•c b·ªô c·ªßa b·∫°n ƒë·∫øn Amazon SageMaker Unified Studio gi·∫£m ma s√°t khi chuy·ªÉn gi·ªØa ph√°t tri·ªÉn c·ª•c b·ªô v√† h·∫° t·∫ßng d·ªØ li·ªáu \u0026amp; AI c√≥ th·ªÉ m·ªü r·ªông. B·∫±ng c√°ch gi·ªØ c·∫•u h√¨nh IDE c√° nh√¢n h√≥a, ƒëi·ªÅu n√†y gi·∫£m s·ª± c·∫ßn thi·∫øt ph·∫£i th√≠ch nghi gi·ªØa c√°c m√¥i tr∆∞·ªùng ph√°t tri·ªÉn kh√°c nhau. D√π b·∫°n ƒëang x·ª≠ l√Ω c√°c t·∫≠p d·ªØ li·ªáu l·ªõn, ƒë√†o t·∫°o m√¥ h√¨nh n·ªÅn t·∫£ng (foundation models, FMs), ho·∫∑c x√¢y d·ª±ng ·ª©ng d·ª•ng AI t·∫°o sinh, b·∫°n gi·ªù c√≥ th·ªÉ l√†m vi·ªác t·ª´ thi·∫øt l·∫≠p c·ª•c b·ªô c·ªßa m√¨nh trong khi truy c·∫≠p c√°c kh·∫£ nƒÉng c·ªßa SageMaker Unified Studio. B·∫Øt ƒë·∫ßu ngay h√¥m nay b·∫±ng c√°ch k·∫øt n·ªëi IDE c·ª•c b·ªô c·ªßa b·∫°n ƒë·∫øn SageMaker Unified Studio ƒë·ªÉ h·ª£p l√Ω h√≥a lu·ªìng x·ª≠ l√Ω d·ªØ li·ªáu v√† tƒÉng t·ªëc ph√°t tri·ªÉn m√¥ h√¨nh ML.\nGi·ªõi thi·ªáu v·ªÅ c√°c t√°c gi·∫£ Lauren Mullennex Lauren l√† Ki·∫øn tr√∫c s∆∞ Gi·∫£i ph√°p Chuy√™n gia v·ªÅ GenAI/ML C·∫•p cao t·∫°i AWS.\nC√¥ c√≥ h∆°n m·ªôt th·∫≠p k·ª∑ kinh nghi·ªám trong lƒ©nh v·ª±c h·ªçc m√°y, DevOps v√† h·∫° t·∫ßng.\nC√¥ l√† t√°c gi·∫£ ƒë√£ xu·∫•t b·∫£n m·ªôt cu·ªën s√°ch v·ªÅ th·ªã gi√°c m√°y t√≠nh.\nNgo√†i c√¥ng vi·ªác, b·∫°n c√≥ th·ªÉ th·∫•y c√¥ ƒëi du l·ªãch v√† ƒëi b·ªô ƒë∆∞·ªùng d√†i c√πng hai ch√∫ ch√≥ c·ªßa m√¨nh.\nBhargava Varadharajan Bhargava l√† K·ªπ s∆∞ Ph·∫ßn m·ªÅm C·∫•p cao t·∫°i Amazon Web Services, n∆°i anh ph√°t tri·ªÉn c√°c s·∫£n ph·∫©m AI \u0026amp; ML nh∆∞ SageMaker Studio, Studio Lab v√† Unified Studio.\nTrong h∆°n nƒÉm nƒÉm qua, anh ƒë√£ t·∫≠p trung v√†o vi·ªác bi·∫øn c√°c quy tr√¨nh AI \u0026amp; ML ph·ª©c t·∫°p th√†nh tr·∫£i nghi·ªám li·ªÅn m·∫°ch.\nKhi kh√¥ng thi·∫øt k·∫ø c√°c h·ªá th·ªëng quy m√¥ l·ªõn, Bhargava theo ƒëu·ªïi m·ª•c ti√™u kh√°m ph√° to√†n b·ªô 63 c√¥ng vi√™n qu·ªëc gia Hoa K·ª≥ v√† t√¨m ki·∫øm nh·ªØng cu·ªôc phi√™u l∆∞u qua leo n√∫i, b√≥ng ƒë√° v√† tr∆∞·ª£t tuy·∫øt.\nTh·ªùi gian r·∫£nh c·ªßa anh ƒë∆∞·ª£c chia cho c√°c d·ª± √°n DIY (t·ª± l√†m) v√† nu√¥i d∆∞·ª°ng s·ª± t√≤ m√≤ th√¥ng qua s√°ch.\nAnagha Barve Anagha l√† Qu·∫£n l√Ω Ph√°t tri·ªÉn Ph·∫ßn m·ªÅm trong nh√≥m Amazon SageMaker Unified Studio.\nAnchit Gupta Anchit l√† Qu·∫£n l√Ω S·∫£n ph·∫©m C·∫•p cao cho Amazon SageMaker Unified Studio.\nC√¥ t·∫≠p trung v√†o vi·ªác ph√°t tri·ªÉn c√°c s·∫£n ph·∫©m gi√∫p vi·ªác x√¢y d·ª±ng gi·∫£i ph√°p h·ªçc m√°y tr·ªü n√™n d·ªÖ d√†ng h∆°n.\nV√†o th·ªùi gian r·∫£nh, c√¥ th√≠ch n·∫•u ƒÉn, ch∆°i c√°c tr√≤ ch∆°i tr√™n b√†n/c·ªù, v√† ƒë·ªçc s√°ch.\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Th√¥ng b√°o Amazon EC2 M4 v√† M4 Pro Mac instances b·ªüi S√©bastien Stormacq v√†o ng√†y 12 TH√ÅNG 9 2025 trong Amazon EC2 Mac Instances, Launch, News Permalink Comments\nPermalink Comments Share\nVoiced by Polly\nL√† ng∆∞·ªùi ƒë√£ s·ª≠ d·ª•ng macOS t·ª´ nƒÉm 2001 v√† c√°c Amazon EC2 Mac instances t·ª´ khi ch√∫ng ra m·∫Øt 4 nƒÉm tr∆∞·ªõc, t√¥i ƒë√£ gi√∫p nhi·ªÅu kh√°ch h√†ng m·ªü r·ªông c√°c pipeline t√≠ch h·ª£p \u0026amp; ph√¢n ph·ªëi li√™n t·ª•c (CI/CD) tr√™n AWS. H√¥m nay, t√¥i r·∫•t h√†o h·ª©ng chia s·∫ª r·∫±ng c√°c instance Amazon EC2 M4 v√† M4 Pro Mac hi·ªán ƒë√£ kh·∫£ d·ª•ng ch√≠nh th·ª©c.\nC√°c nh√≥m ph√°t tri·ªÉn x√¢y ·ª©ng d·ª•ng cho c√°c n·ªÅn t·∫£ng Apple c·∫ßn t√†i nguy√™n t√≠nh to√°n m·∫°nh ƒë·ªÉ x·ª≠ l√Ω c√°c quy tr√¨nh build ph·ª©c t·∫°p v√† ch·∫°y nhi·ªÅu gi·∫£ l·∫≠p iOS c√πng l√∫c. Khi c√°c d·ª± √°n ph√°t tri·ªÉn ng√†y c√†ng l·ªõn v√† tinh vi, c√°c nh√≥m c·∫ßn hi·ªáu nƒÉng v√† dung l∆∞·ª£ng b·ªô nh·ªõ cao h∆°n ƒë·ªÉ duy tr√¨ chu k·ª≥ ph√°t tri·ªÉn nhanh.\nApple M4 Mac mini l√†m l√µi C√°c instance EC2 M4 Mac (ƒë∆∞·ª£c g·ªçi l√† mac-m4.metal trong API) ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n m√°y Apple M4 Mac mini v√† s·ª≠ d·ª•ng h·ªá th·ªëng AWS Nitro System. Ch√∫ng c√≥ chip Apple silicon M4 v·ªõi 10 l√µi CPU (b·ªën l√µi hi·ªáu nƒÉng v√† s√°u l√µi hi·ªáu qu·∫£), GPU 10 l√µi, Neural Engine 16 l√µi, v√† b·ªô nh·ªõ h·ª£p nh·∫•t 24 GB, mang l·∫°i hi·ªáu nƒÉng c·∫£i thi·ªán cho c√°c workload build ·ª©ng d·ª•ng iOS v√† macOS. Khi x√¢y d·ª±ng v√† ki·ªÉm th·ª≠ ·ª©ng d·ª•ng, c√°c instance M4 Mac cho hi·ªáu nƒÉng build ·ª©ng d·ª•ng t·ªët h∆°n t·ªõi 20% so v·ªõi c√°c instance EC2 M2 Mac.\nInstance EC2 M4 Pro Mac ( mac-m4pro.metal trong API ) ƒë∆∞·ª£c trang b·ªã chip Apple silicon M4 Pro v·ªõi 14 l√µi CPU, 20 l√µi GPU, Neural Engine 16 l√µi v√† b·ªô nh·ªõ h·ª£p nh·∫•t 48 GB. Nh·ªØng instance n√†y cung c·∫•p hi·ªáu nƒÉng build ·ª©ng d·ª•ng t·ªët h∆°n t·ªõi 15% so v·ªõi c√°c instance EC2 M2 Pro Mac. Th√™m dung l∆∞·ª£ng b·ªô nh·ªõ v√† c√¥ng su·∫•t t√≠nh to√°n cho ph√©p ch·∫°y nhi·ªÅu b√†i ki·ªÉm th·ª≠ song song b·∫±ng nhi·ªÅu gi·∫£ l·∫≠p thi·∫øt b·ªã.\nM·ªói instance M4 v√† M4 Pro Mac gi·ªù ƒë√¢y ƒëi k√®m v·ªõi 2 TB l∆∞u tr·ªØ n·ªôi b·ªô (local storage), cung c·∫•p l∆∞u tr·ªØ ƒë·ªô tr·ªÖ th·∫•p ƒë·ªÉ c·∫£i thi·ªán caching v√† hi·ªáu nƒÉng build \u0026amp; test.\nC·∫£ hai lo·∫°i instance ƒë·ªÅu h·ªó tr·ª£ macOS Sonoma phi√™n b·∫£n 15.6 v√† m·ªõi h∆°n nh∆∞ c√°c AMI (Amazon Machine Images (AMIs).). H·ªá th·ªëng AWS Nitro cung c·∫•p bƒÉng th√¥ng m·∫°ng Amazon Virtual Private Cloud (Amazon VPC) l√™n ƒë·∫øn 10 Gbps v√† bƒÉng th√¥ng l∆∞u tr·ªØ Amazon Elastic Block Store (Amazon EBS) 8 Gbps qua k·∫øt n·ªëi Thunderbolt t·ªëc ƒë·ªô cao.\nC√°c instance EC2 Mac t√≠ch h·ª£p li·ªÅn m·∫°ch v·ªõi c√°c d·ªãch v·ª• AWS, nghƒ©a l√† b·∫°n c√≥ th·ªÉ:\nX√¢y d·ª±ng pipeline CI/CD t·ª± ƒë·ªông s·ª≠ d·ª•ng AWS CodeBuild v√† AWS CodePipeline\nL∆∞u tr·ªØ v√† qu·∫£n l√Ω nhi·ªÅu phi√™n b·∫£n b√≠ m·∫≠t build c·ªßa b·∫°n, nh∆∞ ch·ª©ng ch·ªâ ph√°t tri·ªÉn Apple v√† kh√≥a, tr√™n AWS Secrets Manager\nQu·∫£n l√Ω h·∫° t·∫ßng ph√°t tri·ªÉn c·ªßa b·∫°n b·∫±ng AWS CloudFormation\nGi√°m s√°t hi·ªáu nƒÉng instance v·ªõi Amazon CloudWatch\nC√°ch b·∫Øt ƒë·∫ßu B·∫°n c√≥ th·ªÉ kh·ªüi ch·∫°y m·ªôt instance EC2 M4 ho·∫∑c M4 Pro Mac qua AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs.\nV√≠ d·ª• trong demo n√†y, t√¥i s·∫Ω kh·ªüi ƒë·ªông m·ªôt instance M4 Pro t·ª´ console. T√¥i ƒë·∫ßu ti√™n c·∫•p ph√°t m·ªôt dedicated host ƒë·ªÉ ch·∫°y c√°c instance c·ªßa m√¨nh. Tr√™n AWS Management Console t√¥i v√†o EC2, r·ªìi Dedicated Hosts, v√† ch·ªçn Allocate Dedicated Host.\nR·ªìi, t√¥i nh·∫≠p tag Name v√† ch·ªçn Family instance (mac‚Äëm4pro) v√† lo·∫°i instance (mac‚Äëm4pro.metal). T√¥i ch·ªçn m·ªôt Availability Zone v√† b·ªè ch·ªçn Host maintenance.\nEC2 Mac M$ ‚Äì Dedicated hosts\nHo·∫∑c t√¥i c√≥ th·ªÉ d√πng CLI:\naws ec2 allocate-hosts \\\n--availability-zone-id \u0026ldquo;usw2-az4\u0026rdquo; \\\n--auto-placement \u0026ldquo;off\u0026rdquo; \\\n--host-recovery \u0026ldquo;off\u0026rdquo; \\\n--host-maintenance \u0026ldquo;off\u0026rdquo; \\\n--quantity 1 \\\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo;\nSau khi host dedicated ƒë∆∞·ª£c c·∫•p cho t√†i kho·∫£n c·ªßa t√¥i, t√¥i ch·ªçn host v·ª´a c·∫•p, r·ªìi ch·ªçn menu Actions v√† ch·ªçn Launch instance(s) onto host.\nL∆∞u √Ω console cung c·∫•p cho b·∫°n, b√™n c·∫°nh c√°c th√¥ng tin kh√°c, c√°c phi√™n b·∫£n macOS h·ªó tr·ª£ m·ªõi nh·∫•t cho lo·∫°i host n√†y. Trong tr∆∞·ªùng h·ª£p n√†y, l√† macOS 15.6.\nTr√™n trang Launch an instance, t√¥i nh·∫≠p Name. T√¥i ch·ªçn m·ªôt AMI macOS Sequoia. T√¥i ƒë·∫£m b·∫£o Architecture l√† Arm 64-bit v√† lo·∫°i instance l√† mac-m4pro.metal.\nPh·∫ßn c√≤n l·∫°i c√°c tham s·ªë kh√¥ng ƒë·∫∑c th√π cho EC2 Mac: c·∫•u h√¨nh m·∫°ng v√† l∆∞u tr·ªØ. Khi kh·ªüi ƒë·ªông m·ªôt instance d√πng cho ph√°t tri·ªÉn, h√£y ch·∫Øc ch·ªçn volume t·ªëi thi·ªÉu 200 GB tr·ªü l√™n. Volume m·∫∑c ƒë·ªãnh 100 GB kh√¥ng ƒë·ªß ƒë·ªÉ t·∫£i xu·ªëng v√† c√†i Xcode.\nKhi ƒë√£ s·∫µn s√†ng, t√¥i nh·∫•n n√∫t Launch instance m√†u cam ·ªü cu·ªëi trang. Instance s·∫Ω nhanh ch√≥ng xu·∫•t hi·ªán ·ªü tr·∫°ng th√°i Running trong console. Tuy nhi√™n, c√≥ th·ªÉ m·∫•t t·ªõi 15 ph√∫t ƒë·ªÉ b·∫°n c√≥ th·ªÉ k·∫øt n·ªëi qua SSH.\nHo·∫∑c t√¥i c√≥ th·ªÉ d√πng l·ªánh n√†y:\naws ec2 run-instances \\\n--image-id \u0026ldquo;ami-000420887c24e4ac8\u0026rdquo; \\ # ID AMI t√πy v√πng !\n--instance-type \u0026ldquo;mac-m4pro.metal\u0026rdquo; \\\n--key-name \u0026ldquo;my-ssh-key-name\u0026rdquo; \\\n--network-interfaces \u0026lsquo;{\u0026ldquo;AssociatePublicIpAddress\u0026rdquo;:true,\u0026ldquo;DeviceIndex\u0026rdquo;:0,\u0026ldquo;Groups\u0026rdquo;:[\u0026ldquo;sg-0c2f1a3e01b84f3a3\u0026rdquo;]}\u0026rsquo; \\ # Security Group ID ph·ª• thu·ªôc config c·ªßa b·∫°n \\\n--tag-specifications \u0026lsquo;{\u0026ldquo;ResourceType\u0026rdquo;:\u0026ldquo;instance\u0026rdquo;,\u0026ldquo;Tags\u0026rdquo;:[{\u0026ldquo;Key\u0026rdquo;:\u0026ldquo;Name\u0026rdquo;,\u0026ldquo;Value\u0026rdquo;:\u0026ldquo;My Dev Server\u0026rdquo;}]}\u0026rsquo; \\\n--placement \u0026lsquo;{\u0026ldquo;HostId\u0026rdquo;:\u0026ldquo;h-0e984064522b4b60b\u0026rdquo;,\u0026ldquo;Tenancy\u0026rdquo;:\u0026ldquo;host\u0026rdquo;}\u0026rsquo; \\ # Host ID t√πy config c·ªßa b·∫°n --private-dns-name-options \u0026lsquo;{\u0026ldquo;HostnameType\u0026rdquo;:\u0026ldquo;ip-name\u0026rdquo;,\u0026ldquo;EnableResourceNameDnsARecord\u0026rdquo;:true,\u0026ldquo;EnableResourceNameDnsAAAARecord\u0026rdquo;:false}\u0026rsquo; \\\n--count \u0026ldquo;1\u0026rdquo;\nC√†i Xcode t·ª´ Terminal Sau khi instance c√≥ th·ªÉ truy c·∫≠p, t√¥i c√≥ th·ªÉ k·∫øt n·ªëi b·∫±ng SSH v√† c√†i c√¥ng c·ª• ph√°t tri·ªÉn. T√¥i d√πng xcodeinstall ƒë·ªÉ t·∫£i v√† c√†i Xcode 16.4.\nT·ª´ laptop c·ªßa t√¥i, t√¥i m·ªü session v·ªõi credentials Apple developer:\n# on my laptop, with permissions to access AWS Secret Manager\n¬ª xcodeinstall authenticate -s eu-central-1\nRetrieving Apple Developer Portal credentials\u0026hellip;\nAuthenticating\u0026hellip;\nüîê Two factors authentication is enabled, enter your 2FA code: 067785\n‚úÖ Authenticated with MFA.\nT√¥i k·∫øt n·ªëi v·ªõi EC2 Mac instance c√°i m√† t√¥i v·ª´a m·ªõi launched. Sau ƒë√≥, t√¥i t·∫£i v√† c√†i ƒë·∫∑c Xcode:\n¬ª ssh ec2-user@44.234.115.119\nWarning: Permanently added \u0026lsquo;44.234.115.119\u0026rsquo; (ED25519) to the list of known hosts.\nLast login: Sat Aug 23 13:49:55 2025 from 81.49.207.77\n‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê \\_\\_| \\_\\_|\\_ ) ‚îÇ ‚ï∑‚ï≠‚ïØ‚ï∑ ‚îÇ \\_| ( / ‚îÇ ‚îî‚ïÆ ‚îÇ \\_\\_\\_|\\\\\\_\\_\\_|\\_\\_\\_| ‚îÇ ‚ï∞‚îÄ‚îº‚ïØ ‚îÇ Amazon EC2 ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò macOS Sequoia 15.6 ec2-user@ip-172-31-54-74 ~ % brew tap sebsto/macos\n==\u0026gt; Tapping sebsto/macos\nCloning into \u0026lsquo;/opt/homebrew/Library/Taps/sebsto/homebrew-macos\u0026rsquo;\u0026hellip;\nremote: Enumerating objects: 227, done.\nremote: Counting objects: 100% (71/71), done.\nremote: Compressing objects: 100% (57/57), done.\nremote: Total 227 (delta 22), reused 63 (delta 14), pack-reused 156 (from 1)\nReceiving objects: 100% (227/227), 37.93 KiB | 7.59 MiB/s, done.\nResolving deltas: 100% (72/72), done.\nTapped 1 formula (13 files, 61KB).\nec2-user@ip-172-31-54-74 ~ % brew install xcodeinstall\n==\u0026gt; Fetching downloads for: xcodeinstall\n==\u0026gt; Fetching sebsto/macos/xcodeinstall\n==\u0026gt; Downloading https://github.com/sebsto/xcodeinstall/releases/download/v0.12.0/xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\nAlready downloaded: /Users/ec2-user/Library/Caches/Homebrew/downloads/9f68a7a50ccfdc479c33074716fd654b8528be0ec2430c87bc2b2fa0c36abb2d\u0026ndash;xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\n==\u0026gt; Installing xcodeinstall from sebsto/macos\n==\u0026gt; Pouring xcodeinstall-0.12.0.arm64_sequoia.bottle.tar.gz\nüç∫ /opt/homebrew/Cellar/xcodeinstall/0.12.0: 8 files, 55.2MB\n==\u0026gt; Running `brew cleanup xcodeinstall`\u0026hellip;\nDisable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\nHide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n==\u0026gt; No outdated dependents to upgrade!\nec2-user@ip-172-31-54-74 ~ % xcodeinstall download -s eu-central-1 -f -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nDownloading Xcode 16.4\n100% [============================================================] 2895 MB / 180.59 MBs\n[ OK ]\n‚úÖ Xcode 16.4.xip downloaded\nec2-user@ip-172-31-54-74 ~ % xcodeinstall install -n \u0026ldquo;Xcode 16.4.xip\u0026rdquo;\nInstalling\u0026hellip;\n[1/6] Expanding Xcode xip (this might take a while)\n[2/6] Moving Xcode to /Applications\n[3/6] Installing additional packages\u0026hellip; XcodeSystemResources.pkg\n[4/6] Installing additional packages\u0026hellip; CoreTypes.pkg\n[5/6] Installing additional packages\u0026hellip; MobileDevice.pkg\n[6/6] Installing additional packages\u0026hellip; MobileDeviceDevelopment.pkg\n[ OK ]\n‚úÖ file:///Users/ec2-user/.xcodeinstall/download/Xcode%2016.4.xip installed\nec2-user@ip-172-31-54-74 ~ % sudo xcodebuild -license accept\nec2-user@ip-172-31-54-74 ~ %\nNh·ªØng ƒëi·ªÅu c·∫ßn bi·∫øt Ch·ªçn volume EBS t·ªëi thi·ªÉu 200 GB cho m·ª•c ƒë√≠ch ph√°t tri·ªÉn. Volume m·∫∑c ƒë·ªãnh 100 GB kh√¥ng ƒë·ªß ƒë·ªÉ c√†i Xcode. T√¥i th∆∞·ªùng ch·ªçn 500 GB. Khi tƒÉng k√≠ch th∆∞·ªõc EBS sau khi instance ƒë√£ kh·ªüi ch·∫°y, nh·ªõ to resize the APFS filesystem.\nNgo√†i ra, b·∫°n c√≥ th·ªÉ ch·ªçn c√†i c√¥ng c·ª• ph√°t tri·ªÉn v√† framework c·ªßa b·∫°n l√™n ·ªï SSD n·ªôi b·ªô 2 TB ƒë·ªô tr·ªÖ th·∫•p c√≥ s·∫µn trong Mac mini. L∆∞u √Ω r·∫±ng n·ªôi dung volume n√†y g·∫Øn v·ªõi v√≤ng ƒë·ªùi instance, kh√¥ng v·ªõi dedicated host. Nghƒ©a l√† m·ªçi th·ª© s·∫Ω b·ªã x√≥a kh·ªèi ·ªï SSD n·ªôi b·ªô khi b·∫°n d·ª´ng v√† kh·ªüi ƒë·ªông l·∫°i instance.\nC√°c instance mac-m4.metal v√† mac-m4pro.metal h·ªó tr·ª£ macOS Sequoia 15.6 v√† c√°c phi√™n b·∫£n m·ªõi h∆°n.\nB·∫°n c√≥ th·ªÉ di chuy·ªÉn c√°c instance EC2 Mac hi·ªán t·∫°i khi instance di chuy·ªÉn ƒëang ch·∫°y macOS 15 (Sequoia). T·∫°o m·ªôt AMI t√πy ch·ªânh t·ª´ instance hi·ªán t·∫°i v√† kh·ªüi ƒë·ªông m·ªôt instance M4 ho·∫∑c M4 Pro t·ª´ AMI ƒë√≥.\nCu·ªëi c√πng, t√¥i g·ª£i √Ω b·∫°n xem c√°c h∆∞·ªõng d·∫´n t√¥i vi·∫øt ƒë·ªÉ gi√∫p b·∫°n b·∫Øt ƒë·∫ßu v·ªõi EC2 Mac:\nKh·ªüi ƒë·ªông m·ªôt instance EC2 Mac\nK·∫øt n·ªëi t·ªõi instance EC2 Mac (t√¥i ch·ªâ b·∫°n ba c√°ch kh√°c nhau ƒë·ªÉ k·∫øt n·ªëi)\nX√¢y ·ª©ng d·ª•ng nhanh h∆°n v·ªõi pipeline CI/CD tr√™n EC2 Mac\nGi√° c·∫£ v√† kh·∫£ d·ª•ng C√°c instance EC2 M4 v√† M4 Pro Mac hi·ªán c√≥ t·∫°i US East (N. Virginia) v√† US West (Oregon), d·ª± ki·∫øn m·ªü r·ªông sang c√°c v√πng kh√°c trong t∆∞∆°ng lai.\nC√°c instance EC2 Mac c√≥ th·ªÉ mua d∆∞·ªõi d·∫°ng Dedicated Hosts theo m√¥ h√¨nh gi√° On-Demand v√† Savings Plans. Vi·ªác t√≠nh ph√≠ cho EC2 Mac l√† theo gi√¢y v·ªõi m·ª©c t·ªëi thi·ªÉu 24 gi·ªù c·∫•p ph√°t ƒë·ªÉ tu√¢n theo Th·ªèa thu·∫≠n B·∫£n quy·ªÅn ph·∫ßn m·ªÅm macOS c·ªßa Apple. Sau kho·∫£ng th·ªùi gian t·ªëi thi·ªÉu 24 gi·ªù, host c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i ph√≥ng b·∫•t c·ª© l√∫c n√†o m√† kh√¥ng c·∫ßn cam k·∫øt ti·∫øp.\nL√† ng∆∞·ªùi l√†m vi·ªác ch·∫∑t v·ªõi c√°c nh√† ph√°t tri·ªÉn Apple, t√¥i t√≤ m√≤ xem b·∫°n s·∫Ω d√πng c√°c instance m·ªõi n√†y nh∆∞ th·∫ø n√†o ƒë·ªÉ tƒÉng t·ªëc chu k·ª≥ ph√°t tri·ªÉn c·ªßa b·∫°n. S·ª± k·∫øt h·ª£p gi·ªØa hi·ªáu nƒÉng gia tƒÉng, dung l∆∞·ª£ng b·ªô nh·ªõ c·∫£i thi·ªán v√† t√≠ch h·ª£p v·ªõi d·ªãch v·ª• AWS m·ªü ra nhi·ªÅu kh·∫£ nƒÉng m·ªõi cho c√°c ƒë·ªôi x√¢y ·ª©ng d·ª•ng cho iOS, macOS, iPadOS, tvOS, watchOS, v√† visionOS. Ngo√†i ph√°t tri·ªÉn ·ª©ng d·ª•ng, Neural Engine c·ªßa Apple silicon khi·∫øn c√°c instance n√†y l√† ·ª©ng vi√™n hi·ªáu qu·∫£ chi ph√≠ ƒë·ªÉ ch·∫°y workload inference machine learning (ML). T√¥i s·∫Ω th·∫£o lu·∫≠n chi ti·∫øt ch·ªß ƒë·ªÅ n√†y t·∫°i AWS re:Invent 2025, n∆°i t√¥i s·∫Ω chia s·∫ª benchmark v√† best practices ƒë·ªÉ t·ªëi ∆∞u workload ML tr√™n EC2 Mac.\nƒê·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ c√°c instance EC2 M4 v√† M4 Pro Mac, b·∫°n c√≥ th·ªÉ truy c·∫≠p trang Amazon EC2 Mac Instances ho·∫∑c tham kh·∫£o EC2 Mac documentation. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng c√°c instance n√†y ngay h√¥m nay ƒë·ªÉ hi·ªán ƒë·∫°i h√≥a workflow ph√°t tri·ªÉn Apple tr√™n AWS.\n‚Äî seb\nS√©bastien Stormacq\nSeb ƒë√£ vi·∫øt code t·ª´ khi ch·∫°m Commodore 64 gi·ªØa nh·ªØng nƒÉm t√°m m∆∞∆°i. Anh truy·ªÅn c·∫£m h·ª©ng cho nh·ªØng ng∆∞·ªùi x√¢y d·ª±ng ƒë·ªÉ khai ph√° gi√° tr·ªã c·ªßa ƒë√°m m√¢y AWS, d√πng h·ªón h·ª£p b√≠ m·∫≠t gi·ªØa ƒëam m√™, nhi·ªát huy·∫øt, advocacy kh√°ch h√†ng, t√≤ m√≤ v√† s√°ng t·∫°o. Anh ta quan t√¢m ƒë·∫øn ki·∫øn tr√∫c ph·∫ßn m·ªÅm, c√¥ng c·ª• dev v√† ƒëi·ªán to√°n di ƒë·ªông. N·∫øu b·∫°n mu·ªën b√°n cho anh c√°i g√¨ ƒë√≥, ƒë·∫£m b·∫£o n√≥ c√≥ API. Theo d√µi @sebsto tr√™n Bluesky, X, Mastodon v√† c√°c n·ªÅn t·∫£ng kh√°c.\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "H∆∞·ªõng d·∫´n tinh ch·ªânh cho Amazon EC2 instances d√πng AMD b·ªüi Suyash Nadkarni v√† Dylan Souvage v√†o ng√†y 12 TH√ÅNG 9 2025 trong Amazon EC2, Best Practices, Expert (400), Technical\nKhi c√°c t·ªï ch·ª©c di chuy·ªÉn nhi·ªÅu kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác quan tr·ªçng sang ƒë√°m m√¢y, t·ªëi ∆∞u h√≥a v·ªÅ gi√° ‚Äî hi·ªáu su·∫•t (price-performance) tr·ªü th√†nh m·ªôt c√¢n nh·∫Øc ch·ªß ch·ªët. C√°c instance Amazon Elastic Compute Cloud(Amazon EC2) s·ª≠ d·ª•ng b·ªô x·ª≠ l√Ω AMD EPYC ƒëem l·∫°i m·∫≠t ƒë·ªô l√µi cao, bƒÉng th√¥ng b·ªô nh·ªõ l·ªõn v√† c√°c t√≠nh nƒÉng b·∫£o m·∫≠t ƒë∆∞·ª£c h·ªó tr·ª£ ph·∫ßn c·ª©ng, khi·∫øn ch√∫ng tr·ªü th√†nh m·ªôt l·ª±a ch·ªçn m·∫°nh m·∫Ω cho nhi·ªÅu lo·∫°i kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác t√≠nh to√°n, b·ªô nh·ªõ ho·∫∑c I/O. Trong b√†i vi·∫øt n√†y, ch√∫ng t√¥i gi·∫£i th√≠ch c√°ch ch·ªçn lo·∫°i instance Amazon EC2 d·ª±a tr√™n AMD ph√π h·ª£p v√† m√¥ t·∫£ c√°c k·ªπ thu·∫≠t ƒëi·ªÅu ch·ªânh c√≥ th·ªÉ gi√∫p ng∆∞·ªùi d√πng c·∫£i thi·ªán hi·ªáu qu·∫£ kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác. Cho d√π b·∫°n ƒëang ch·∫°y m√¥ ph·ªèng, ph√¢n t√≠ch quy m√¥ l·ªõn ho·∫∑c c√°c kh·ªëi l∆∞·ª£ng inference, b√†i vi·∫øt n√†y cung c·∫•p h∆∞·ªõng d·∫´n th·ª±c ti·ªÖn ƒë·ªÉ t·ªëi ∆∞u h√≥a instance Amazon EC2 d√πng AMD.\nAmazon EC2 cung c·∫•p instances AMD d·ª±a tr√™n nhi·ªÅu th·∫ø h·ªá AMD EPYC. B√†i vi·∫øt t·∫≠p trung v√†o chi·∫øn l∆∞·ª£c t·ªëi ∆∞u cho th·∫ø h·ªá 3 v√† 4, v·ªën tƒÉng c∆∞·ªùng kh·∫£ nƒÉng cho workload t√≠nh to√°n v√† b·ªô nh·ªõ chuy√™n s√¢u.\nTh·∫ø h·ªá 3 (M6a, R6a, C6a, Hpc6a): C√¢n b·∫±ng t√≠nh to√°n, b·ªô nh·ªõ v√† l∆∞u tr·ªØ ‚Äî ph√π h·ª£p v·ªõi ph√¢n t√≠ch d·ªØ li·ªáu, m√°y ch·ªß web v√† t√≠nh to√°n hi·ªáu nƒÉng cao.\nTh·∫ø h·ªá th·ª© 4 (M7a, R7a, C7a, Hpc7a): Cung c·∫•p hi·ªáu su·∫•t t·ªët h∆°n t·ªõi 50% so v·ªõi c√°c th·∫ø h·ªá AMD tr∆∞·ªõc ƒë√≥. C√°c instance n√†y gi·ªõi thi·ªáu h·ªó tr·ª£ AVX‚Äë512, b·ªô nh·ªõ DDR5 v√† Simultaneous Multithreading (SMT) b·ªã t·∫Øt; SMT l√† c√¥ng ngh·ªá cho ph√©p m·ªôt l√µi v·∫≠t l√Ω ch·∫°y nhi·ªÅu lu·ªìng c√πng l√∫c; v·ªõi SMT b·ªã t·∫Øt, m·ªói vCPU (virtual CPU) √°nh x·∫° tr·ª±c ti·∫øp ƒë·∫øn m·ªôt l√µi v·∫≠t l√Ω, ƒëi·ªÅu n√†y c√≥ th·ªÉ c·∫£i thi·ªán t√≠nh c√¥ l·∫≠p v√† nh·∫•t qu√°n trong kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác.\nCh·ªçn lo·∫°i instance Amazon EC2 d√πng AMD EPYC ph√π h·ª£p Vi·ªác ch·ªçn lo·∫°i instance Amazon EC2 d√πng AMD EPYC ph√π h·ª£p b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác hi·ªÉu c√°ch ·ª©ng d·ª•ng c·ªßa b·∫°n s·ª≠ d·ª•ng t√†i nguy√™n t√≠nh to√°n (compute), b·ªô nh·ªõ, l∆∞u tr·ªØ v√† m·∫°ng. M·ªói instance family ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho nh·ªØng ƒë·∫∑c t√≠nh kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác nh·∫•t ƒë·ªãnh.\nKh·ªëi l∆∞·ª£ng c√¥ng vi·ªác t√≠nh to√°n\nNh·ªØng kh·ªëi l∆∞·ª£ng n√†y li√™n quan t·ªõi c√°c ph√©p t√≠nh quy m√¥ l·ªõn, m√¥ ph·ªèng, ho·∫∑c m√£ h√≥a, v√† th∆∞·ªùng c·∫ßn th√¥ng l∆∞·ª£ng CPU cao v√† h·ªó tr·ª£ t·∫≠p l·ªánh n√¢ng cao.\nKhuy·∫øn ngh·ªã: C7a, Hpc7a, C6a, Hpc6a\nT√¨nh hu·ªëng d√πng: ƒëi·ªán to√°n khoa h·ªçc, m√¥ h√¨nh t√†i ch√≠nh, chuy·ªÉn m√£ media, m√£ h√≥a, inference ML\nBig Data \u0026amp; Analytics\n·ª®ng d·ª•ng x·ª≠ l√Ω v√† ph√¢n t√≠ch t·∫≠p d·ªØ li·ªáu l·ªõn h∆∞·ªüng l·ª£i t·ª´ bƒÉng th√¥ng b·ªô nh·ªõ cao v√† t·ª∑ l·ªá t√≠nh to√°n‚Äëb·ªô nh·ªõ c√¢n b·∫±ng.\nKhuy·∫øn ngh·ªã: R7a, M7a, R6a, M6a\nT√¨nh hu·ªëng d√πng: x·ª≠ l√Ω lu·ªìng, ph√¢n t√≠ch th·ªùi gian th·ª±c, c√¥ng c·ª• business intelligence, caching ph√¢n t√°n\nKh·ªëi l∆∞·ª£ng c√¥ng vi·ªác c∆° s·ªü d·ªØ li·ªáu\nC√¥ng vi·ªác database th∆∞·ªùng c·∫ßn hi·ªáu su·∫•t b·ªô nh·ªõ ·ªïn ƒë·ªãnh v√† throughput I/O cao cho c√°c ho·∫°t ƒë·ªông ƒë·ªçc/ghi.\nKhuy·∫øn ngh·ªã: R7a, M7a, R6a, M6a\nT√¨nh hu·ªëng d√πng: database quan h·ªá (MySQL, PostgreSQL), NoSQL (MongoDB, Cassandra), database trong b·ªô nh·ªõ (Redis)\nWeb v√† m√°y ch·ªß ·ª©ng d·ª•ng\nNh·ªØng ·ª©ng d·ª•ng n√†y x·ª≠ l√Ω c√°c t·∫£i y√™u c·∫ßu bi·∫øn ƒë·ªïi v√† h∆∞·ªüng l·ª£i t·ª´ s·ª± c√¢n b·∫±ng gi·ªØa t√≠nh to√°n, b·ªô nh·ªõ v√† hi·ªáu nƒÉng m·∫°ng.\nKhuy·∫øn ngh·ªã: C7a, M7a, C6a, M6a\nT√¨nh hu·ªëng d√πng: m√°y ch·ªß web, h·ªá qu·∫£n l√Ω n·ªôi dung, n·ªÅn t·∫£ng e‚Äëcommerce, c√°c ƒëi·ªÉm cu·ªëi API\nAI/ML tr√™n CPU\nC√°c t√°c v·ª• ML kh√¥ng c·∫ßn GPU ‚Äî nh∆∞ inference ho·∫∑c ti·ªÅn x·ª≠ l√Ω ‚Äî c√≥ th·ªÉ ch·∫°y hi·ªáu qu·∫£ tr√™n c√°c instance d·ª±a CPU.\nKhuy·∫øn ngh·ªã: M7a, R7a, C7a\nT√¨nh hu·ªëng d√πng: inference m√¥ h√¨nh, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, th·ªã gi√°c m√°y t√≠nh, h·ªá g·ª£i √Ω\nHigh Performance Computing\nNh·ªØng kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác n√†y c·∫ßn nhi·ªÅu l√µi, bƒÉng th√¥ng b·ªô nh·ªõ cao v√† m·∫°ng ƒë·ªô tr·ªÖ th·∫•p cho c√°c t√≠nh to√°n li√™n k·∫øt ch·∫∑t ch·∫Ω.\nKhuy·∫øn ngh·ªã: Hpc7a, Hpc6a, R7a, M7a\nT√¨nh hu·ªëng d√πng: ƒë·ªông l·ª±c ch·∫•t l·ªèng, genomics, ph√¢n t√≠ch ƒë·ªãa ch·∫•n, m√¥ ph·ªèng k·ªπ thu·∫≠t\nVi·ªác ph√π h·ª£p h√≥a lo·∫°i instance v·ªõi nhu c·∫ßu kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác gi√∫p cung c·∫•p hi·ªáu su·∫•t d·ª± ƒëo√°n ƒë∆∞·ª£c v√† hi·ªáu qu·∫£ chi ph√≠. C√°c d·ªãch v·ª• nh∆∞ Amazon EC2 Auto Scaling v√† AWS Compute Optimizer c√≥ th·ªÉ h·ªó tr·ª£ trong vi·ªác l·ª±a ch·ªçn instance v√† ra quy·∫øt ƒë·ªãnh scale li√™n t·ª•c.\nT·ªëi ∆∞u h√≥a c√°c instance Amazon EC2 d√πng AMD EPYC C√°c instance EC2 d√πng b·ªô x·ª≠ l√Ω AMD EPYC th·∫ø h·ªá th·ª© 4 v·∫≠n h√†nh v·ªõi ki·∫øn tr√∫c ‚Äúchiplet modular‚Äù, nh∆∞ minh h·ªça trong h√¨nh d∆∞·ªõi ƒë√¢y. M·ªói b·ªô x·ª≠ l√Ω bao g·ªìm nhi·ªÅu Core Complex Dies (CCD), v√† m·ªói CCD ch·ª©a m·ªôt ho·∫∑c nhi·ªÅu t·ªï h·ª£p l√µi (core complexes, g·ªçi l√† CCX). M·ªôt CCX gom t·ªëi ƒëa t√°m l√µi v·∫≠t l√Ω, m·ªói l√µi c√≥ 1 MB b·ªô nh·ªõ ƒë·ªám L2 ri√™ng v√† t√°m l√µi ƒë√≥ c√πng chia s·∫ª 32 MB b·ªô nh·ªõ ƒë·ªám L3. C√°c CCD n√†y ƒë∆∞·ª£c k·∫øt n·ªëi v·ªõi m·ªôt die I/O trung t√¢m, ch·ªãu tr√°ch nhi·ªám qu·∫£n l√Ω b·ªô nh·ªõ v√† li√™n k·∫øt gi·ªØa c√°c chip.\n(Bi·ªÉu ƒë·ªì 1: S∆° ƒë·ªì c·ªßa die CPU ‚ÄòZen 4‚Äô v·ªõi 8 l√µi m·ªói die)\nKi·∫øn tr√∫c module c·ªßa c√°c b·ªô x·ª≠ l√Ω AMD EPYC th·∫ø h·ªá th·ª© 4 cho ph√©p c√°c instance nh∆∞ m7a.24xlarge v√† m7a.48xlarge h·ªó tr·ª£ s·ªë l∆∞·ª£ng l√µi cao ‚Äî l√™n ƒë·∫øn 96 l√µi v·∫≠t l√Ω m·ªói socket. V√≠ d·ª•:\nm7a.24xlarge cung c·∫•p 96 l√µi v·∫≠t l√Ω t·ª´ m·ªôt socket ƒë∆°n\nm7a.48xlarge tr·∫£i r·ªông hai socket, cung c·∫•p 192 l√µi v·∫≠t l√Ω\nHi·ªÉu c√°ch c√°c k√≠ch c·ª° instance c·ªßa EC2 √°nh x·∫° t·ªõi b·ªë c·ª•c b·ªô x·ª≠ l√Ω v·∫≠t l√Ω c√≥ th·ªÉ gi√∫p b·∫°n t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v√† t√≠nh nh·∫•t qu√°n b·ªô nh·ªõ ƒë·ªám (cache). Nh·ªØng kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác li√™n quan t·ªõi truy c·∫≠p b·ªô nh·ªõ chia s·∫ª ho·∫∑c ƒë·ªìng b·ªô h√≥a lu·ªìng ‚Äî nh∆∞ HPC ho·∫∑c database trong b·ªô nh·ªõ ‚Äî c√≥ th·ªÉ h∆∞·ªüng l·ª£i khi ch·ªçn k√≠ch c·ª° instance gi·∫£m thi·ªÉu giao ti·∫øp gi·ªØa socket v√† t·∫≠n d·ª•ng hi·ªáu qu·∫£ b·ªô nh·ªõ ƒë·ªám L3.\n(Bi·ªÉu ƒë·ªì 2: B·ªë c·ª•c CPU ‚ÄòEPYC Chiplet‚Äô)\nC√°c instance EC2 d√πng AMD EPYC th·∫ø h·ªá th·ª© 4 ho·∫°t ƒë·ªông v·ªõi SMT b·ªã t·∫Øt. Trong c·∫•u h√¨nh n√†y, m·ªói vCPU √°nh x·∫° tr·ª±c ti·∫øp t·ªõi m·ªôt l√µi v·∫≠t l√Ω, lo·∫°i b·ªè chia s·∫ª t√†i nguy√™n nh∆∞ ƒë∆°n v·ªã th·ª±c thi v√† b·ªô nh·ªõ ƒë·ªám gi·ªØa c√°c lu·ªìng ‚Äúch·ªã/em‚Äù. Thi·∫øt k·∫ø n√†y c√≥ th·ªÉ gi·∫£m nhi·ªÖu n·ªôi l√µi v√† gi√∫p cung c·∫•p hi·ªáu su·∫•t ·ªïn ƒë·ªãnh h∆°n cho c√°c kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác nh·∫•t ƒë·ªãnh. Ng∆∞·ªùi d√πng c√≥ th·ªÉ c√¥ l·∫≠p lu·ªìng ·ªü c·∫•p l√µi v√† quan s√°t ƒë·ªô bi·∫øn thi√™n th·∫•p h∆°n v√† th√¥ng l∆∞·ª£ng ·ªïn ƒë·ªãnh h∆°n cho c√°c kh·ªëi l∆∞·ª£ng nh∆∞ HPC, inference ML v√† database giao d·ªãch.\nCPU optimizations C√°c c√¥ng c·ª• nh∆∞ htop gi√∫p x√°c ƒë·ªãnh m·∫´u s·ª≠ d·ª•ng CPU, trung b√¨nh t·∫£i h·ªá th·ªëng, v√† ti√™u th·ª• t√†i nguy√™n theo ti·∫øn tr√¨nh. Vi·ªác s·ª≠ d·ª•ng CPU n√™n ƒë∆∞·ª£c ƒë√°nh gi√° trong ng·ªØ c·∫£nh kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác v√† y√™u c·∫ßu hi·ªáu nƒÉng. N·∫øu m·ª©c s·ª≠ d·ª•ng li√™n t·ª•c ƒë·∫°t 100%, ƒëi·ªÅu ƒë√≥ c√≥ th·ªÉ ch·ªâ ra r·∫±ng kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác b·ªã CPU-bound v√† ch∆∞a ƒë∆∞·ª£c c√¢n b·∫±ng t·ªëi ∆∞u. Tr∆∞·ªõc khi thay ƒë·ªïi k√≠ch th∆∞·ªõc instance, b·∫≠t Auto Scaling, ho·∫∑c chuy·ªÉn ƒë·ªïi gi·ªØa c√°c gia ƒë√¨nh instance, c·∫ßn th·ª±c hi·ªán ƒë√°nh gi√° c√°c c∆° h·ªôi tuning c√≥ th·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√† kh√¥ng thay ƒë·ªïi h·∫° t·∫ßng. Trung b√¨nh t·∫£i (load averages) v∆∞·ª£t th∆∞·ªùng xuy√™n h∆°n s·ªë l∆∞·ª£ng vCPU c≈©ng c√≥ th·ªÉ l√† d·∫•u hi·ªáu b√£o h√≤a t√≠nh to√°n v√† c√≥ th·ªÉ y√™u c·∫ßu t·ªëi ∆∞u ti·∫øp.\nS·ª≠ d·ª•ng b·ªô nh·ªõ ƒë·ªám L3 L3 cache l√† l·ªõp nh·ªõ nhanh chia s·∫ª ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi m·ªôt nh√≥m l√µi CPU. Tr√™n EC2 d·ª±a AMD, c√°c l√µi ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh c√°c slice b·ªô nh·ªõ ƒë·ªám L3, m·ªói slice ƒë∆∞·ª£c chia s·∫ª b·ªüi m·ªôt t·∫≠p h·ª£p l√µi tr√™n c√πng m·ªôt socket. C√°c lu·ªìng ƒë∆∞·ª£c l·∫≠p l·ªãch trong c√πng slice c√≥ th·ªÉ truy c·∫≠p d·ªØ li·ªáu chia s·∫ª hi·ªáu qu·∫£ h∆°n, gi·∫£m ƒë·ªô tr·ªÖ b·ªô nh·ªõ. Tr√™n c√°c instance AMD th·∫ø h·ªá 4 nh∆∞ m7a.2xlarge ho·∫∑c r7a.2xlarge, t·∫•t c·∫£ vCPU th∆∞·ªùng √°nh x·∫° t·ªõi c√°c l√µi n·∫±m trong c√πng m·ªôt slice L3, ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n ƒë·ªãa ph∆∞∆°ng b·ªô nh·ªõ ƒë·ªám. ƒê·ªëi v·ªõi c√°c k√≠ch c·ª° l·ªõn h∆°n (v√≠ d·ª• m7a.8xlarge tr·ªü l√™n), thread pinning ‚Äî g√°n c√°c lu·ªìng t·ªõi l√µi v·∫≠t l√Ω c·ª• th·ªÉ ‚Äî c√≥ th·ªÉ gi√∫p duy tr√¨ t√≠nh ƒë·ªãa ph∆∞∆°ng n√†y. Thread pinning c√≥ th·ªÉ gi·∫£m bi·∫øn ƒë·ªông hi·ªáu su·∫•t trong c√°c kh·ªëi l∆∞·ª£ng v·ªõi m·∫´u truy c·∫≠p b·ªô nh·ªõ chia s·∫ª th∆∞·ªùng xuy√™n.\nB·∫°n c√≥ th·ªÉ pin lu·ªìng v·ªõi l·ªánh:\ntaskset -c 0-3 ./your_application\nV√≠ d·ª• n√†y pin ·ª©ng d·ª•ng c·ªßa b·∫°n v√†o c√°c l√µi CPU 0 ƒë·∫øn 3. ƒê·ªÉ x√°c ƒë·ªãnh l√µi n√†o chia s·∫ª c√πng v√πng b·ªô nh·ªõ ƒë·ªám L3, s·ª≠ d·ª•ng c√°c c√¥ng c·ª• nh∆∞ lscpu ho·∫∑c lstopo ƒë·ªÉ ki·ªÉm tra topology CPU h·ªá th·ªëng. Gom c√°c lu·ªìng li√™n quan v√†o c√°c l√µi c√πng chia s·∫ª L3 cache c√≥ th·ªÉ c·∫£i thi·ªán t√≠nh nh·∫•t qu√°n hi·ªáu su·∫•t cho c√°c kh·ªëi l∆∞·ª£ng c√≥ truy c·∫≠p b·ªô nh·ªõ chia s·∫ª.\nT·ªëi ∆∞u container Docker Trong m√¥i tr∆∞·ªùng container ch·∫°y tr√™n c√°c instance EC2 d·ª±a AMD, ƒëi·ªÅu ch·ªânh c√°c thi·∫øt l·∫≠p li√™n quan CPU c√≥ th·ªÉ c·∫£i thi·ªán t√≠nh nh·∫•t qu√°n v√† hi·ªáu qu·∫£ kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác ‚Äî ƒë·∫∑c bi·ªát cho c√°c ·ª©ng d·ª•ng t√≠nh to√°n n·∫∑ng ho·∫∑c nh·∫°y ƒë·ªô tr·ªÖ. M·∫∑c d√π c·∫•u h√¨nh m·∫∑c ƒë·ªãnh ho·∫°t ƒë·ªông cho nhi·ªÅu k·ªãch b·∫£n t·ªïng qu√°t, m·ªôt s·ªë workload c√≥ th·ªÉ l·ª£i t·ª´ vi·ªác ki·ªÉm so√°t r√µ r√†ng c√°ch ph√¢n b·ªï t√†i nguy√™n CPU. Theo m·∫∑c ƒë·ªãnh, runtime container nh∆∞ Docker cho ph√©p h·ªá ƒëi·ªÅu h√†nh l·∫≠p l·ªãch container tr√™n b·∫•t k·ª≥ l√µi CPU n√†o s·∫µn c√≥. Vi·ªác n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn bi·∫øn thi√™n hi·ªáu su·∫•t khi container di chuy·ªÉn gi·ªØa c√°c l√µi kh√¥ng chia s·∫ª cache. ƒê·ªÉ gi·∫£m bi·∫øn thi√™n v√† c·∫£i thi·ªán hi·ªáu qu·∫£ cache, container c√≥ th·ªÉ ƒë∆∞·ª£c pin v√†o c√°c l√µi c·ª• th·ªÉ b·∫±ng flag --cpuset-cpus.\ndocker run --cpuset-cpus=\u0026ldquo;1,3\u0026rdquo; my-container\nThi·∫øt l·∫≠p n√†y gi·ªõi h·∫°n container ch·ªâ d√πng c√°c l√µi ƒë√£ ch·ªâ ƒë·ªãnh. Trong v√≠ d·ª• n√†y, l√µi 1 v√† 3 ƒë∆∞·ª£c d√πng ƒë·ªÉ minh ho·∫°. L·ª±a ch·ªçn l√µi th·ª±c t·∫ø n√™n d·ª±a tr√™n topology CPU ƒë·ªÉ ƒë·∫£m b·∫£o l·∫≠p l·ªãch hi·ªáu qu·∫£ b·ªô nh·ªõ ƒë·ªám. Pin container v√†o c√°c l√µi chia s·∫ª b·ªô nh·ªõ ƒë·ªám L3 c√≥ th·ªÉ gi·∫£m overhead l·∫≠p l·ªãch v√† c·∫£i thi·ªán t√≠nh nh·∫•t qu√°n cho c√°c workload c√≥ m·∫´u truy c·∫≠p b·ªô nh·ªõ chia s·∫ª.\nThi·∫øt l·∫≠p governor t·∫ßn s·ªë CPU M·ªôt s·ªë h·ªá ƒëi·ªÅu h√†nh ƒëi·ªÅu ch·ªânh t·∫ßn s·ªë CPU ƒë·ªông ƒë·ªÉ ti·∫øt ki·ªám ƒëi·ªán. Th√¥ng th∆∞·ªùng ƒëi·ªÅu n√†y ƒë∆∞·ª£c ki·ªÉm so√°t b·ªüi thi·∫øt l·∫≠p g·ªçi l√† CPU frequency governor. M·∫∑c d√π h√†nh vi n√†y hi·ªáu qu·∫£ cho c√°c workload t·ªïng qu√°t, n√≥ c√≥ th·ªÉ g√¢y ƒë·ªô tr·ªÖ ho·∫∑c bi·∫øn thi√™n hi·ªáu su·∫•t trong m√¥i tr∆∞·ªùng nh·∫°y v·ªõi t√≠nh to√°n. V·ªõi c√°c workload c·∫ßn hi·ªáu su·∫•t CPU ·ªïn ƒë·ªãnh cao ‚Äî nh∆∞ x·ª≠ l√Ω d·ªØ li·ªáu th√¥ng l∆∞·ª£ng l·ªõn, m√¥ ph·ªèng, ho·∫∑c ·ª©ng d·ª•ng th·ªùi gian th·ª±c ‚Äî ch√∫ng t√¥i khuy·∫øn ngh·ªã ƒë·∫∑t governor c·ªßa CPU v·ªÅ performance mode. ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o CPU ch·∫°y ·ªü t·∫ßn s·ªë t·ªëi ƒëa khi ch·ªãu t·∫£i, tr√°nh th·ªùi gian tƒÉng t·ªëc t·ª´ tr·∫°ng th√°i nƒÉng l∆∞·ª£ng th·∫•p.\nB·∫°n c√≥ th·ªÉ √°p d·ª•ng thi·∫øt l·∫≠p n√†y tr√™n c√°c instance bare metal ho·∫∑c Amazon EC2 Dedicated Hosts b·∫±ng l·ªánh:\nsudo cpupower frequency-set -g performance\nTr∆∞·ªõc khi √°p d·ª•ng, h√£y c√¢n nh·∫Øc benchmark hi·ªáu su·∫•t workload v·ªõi c√°c governor kh√°c (nh∆∞ ondemand ho·∫∑c schedutil) ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng ch·∫ø ƒë·ªô performance mang l·∫°i l·ª£i √≠ch ƒëo ƒë∆∞·ª£c m√† kh√¥ng ƒë√°nh ƒë·ªïi ƒëi·ªán nƒÉng kh√¥ng c·∫ßn thi·∫øt.\nD√πng flag compiler ki·∫øn tr√∫c c·ª• th·ªÉ Khi bi√™n d·ªãch c√°c ·ª©ng d·ª•ng C ho·∫∑c C++ nh·∫°y hi·ªáu su·∫•t, c√°c flag ki·∫øn tr√∫c c·ª• th·ªÉ nh∆∞ -march=znverX c√≥ th·ªÉ m·ªü kh√≥a t·ªëi ∆∞u h√≥a d√†nh ri√™ng cho AMD EPYC, bao g·ªìm c·∫£i thi·ªán vectorization v√† hi·ªáu su·∫•t s·ªë th·ª±c. D√π ƒëi·ªÅu n√†y c√≥ l·ª£i cho workload t√≠nh to√°n n·∫∑ng, n√≥ c√≥ th·ªÉ gi·∫£m t√≠nh di ƒë·ªông gi·ªØa c√°c ki·∫øn tr√∫c. ƒê·ªÉ c√¢n b·∫±ng gi·ªØa hi·ªáu su·∫•t v√† t√≠nh linh ho·∫°t, c√¢n nh·∫Øc d√πng ph√°t hi·ªán t√≠nh nƒÉng th·ªùi ch·∫°y (runtime feature detection) v√† dispatching ‚Äî c√°ch nhi·ªÅu th∆∞ vi·ªán t·ªëi ∆∞u d√πng ƒë·ªÉ th√≠ch ·ª©ng h√†nh vi d·ª±a tr√™n CPU n·ªÅn t·∫£ng.\nTr∆∞·ªõc khi d√πng c√°c flag n√†y, x√°c minh r·∫±ng phi√™n b·∫£n compiler c·ªßa b·∫°n h·ªó tr·ª£ ch√∫ng v√† ƒë·∫£m b·∫£o ki·∫øn tr√∫c instance EC2 ƒë√≠ch ph√π h·ª£p v·ªõi flag ch·ªâ ƒë·ªãnh. V√≠ d·ª•, m·ªôt binary bi√™n d·ªãch v·ªõi -march=znver4 c√≥ th·ªÉ l·ªói ch·∫°y v·ªõi l·ªói ‚Äúillegal instruction‚Äù (SIGILL) n·∫øu ch·∫°y tr√™n c√°c instance th·∫ø h·ªá tr∆∞·ªõc nh∆∞ M5a. B·∫£ng d∆∞·ªõi ƒë√¢y m√¥ t·∫£ c√°c flag ph√π h·ª£p v√† phi√™n b·∫£n compiler t·ªëi thi·ªÉu h·ªó tr·ª£ cho m·ªói th·∫ø h·ªá AMD EPYC:\nTh·∫ø h·ªá AMD EPYC Flag -march Phi√™n b·∫£n GCC t·ªëi thi·ªÉu Phi√™n b·∫£n LLVM/Clang t·ªëi thi·ªÉu Th·∫ø h·ªá 4 (v√≠ d·ª• M7a) znver4 GCC 12 Clang 15 Th·∫ø h·ªá 3 (v√≠ d·ª• M6a) znver3 GCC 11 Clang 13 Th·∫ø h·ªá 2 (v√≠ d·ª• M5a) znver2 GCC 9 Clang 11 C√°c flag sau ƒë∆∞·ª£c h·ªó tr·ª£ cho GCC 11+ ho·∫∑c LLVM Clang 13+:\nCho EPYC th·∫ø h·ªá 4 (M7a, R7a, C7a, Hpc7a): -march=znver4\nCho EPYC th·∫ø h·ªá 3 (M6a, R6a, C6a): -march=znver3\nCho EPYC th·∫ø h·ªá 2 (M5a, R5a, C5a): -march=znver2\nKhi n√†o b·∫≠t AVX‚Äë512 v√† VNNI C√°c instance EC2 d√πng AMD EPYC th·∫ø h·ªá 4 h·ªó tr·ª£ c√°c t·∫≠p l·ªánh SIMD ti√™n ti·∫øn nh∆∞ AVX2, AVX‚Äë512 v√† VNNI. Nh·ªØng t·∫≠p l·ªánh n√†y c√≥ th·ªÉ c·∫£i thi·ªán th√¥ng l∆∞·ª£ng cho c√°c workload vector n·∫∑ng nh∆∞ inference ML, x·ª≠ l√Ω h√¨nh ·∫£nh ho·∫∑c m√¥ ph·ªèng khoa h·ªçc. Tuy nhi√™n, nh·ªØng flag n√†y l√† ƒë·∫∑c tr∆∞ng theo th·∫ø h·ªá ‚Äî vi·ªác c·ªë ch·∫°y c√°c binary ƒë∆∞·ª£c bi√™n d·ªãch v·ªõi AVX‚Äë512 tr√™n instance kh√¥ng h·ªó tr·ª£ (v√≠ d·ª• th·∫ø h·ªá 2 nh∆∞ M5a) c√≥ th·ªÉ g√¢y l·ªói th·ªùi gian ch·∫°y nh∆∞ ‚Äúillegal instruction‚Äù (SIGILL).\nKhi bi√™n d·ªãch m√£ C ho·∫∑c C++:\ngcc -mavx2 -mavx512f -O2 your_program.c -o your_program\nƒê·ªÉ hi·ªÉu r√µ h∆°n t·ªëi ∆∞u h√≥a n√†o ƒë∆∞·ª£c √°p d·ª•ng, d√πng:\n-ftree-vectorizer-verbose=2 -fopt-info-vec-missed\nC√°ch n√†y gi√∫p x√°c ƒë·ªãnh c√°c v√≤ng l·∫∑p h∆∞·ªüng l·ª£i t·ª´ vectorization v√† nh·ªØng v√≤ng l·∫∑p kh√¥ng. Ch·ªâ b·∫≠t c√°c t·ªëi ∆∞u h√≥a n√†y n·∫øu workload c·ªßa b·∫°n h∆∞·ªüng l·ª£i v√† b·∫°n ƒë√£ x√°c th·ª±c t√≠nh t∆∞∆°ng th√≠ch v·ªõi th·∫ø h·ªá instance ƒëang d√πng. Tr√°nh √°p d·ª•ng flag AVX m·ªôt c√°ch b·ª´a b√£i, v√¨ c√≥ th·ªÉ l√†m gi·∫£m t√≠nh di ƒë·ªông v√† tƒÉng ƒë·ªô ph·ª©c t·∫°p binary.\nTh∆∞ vi·ªán t·ªëi ∆∞u CPU c·ªßa AMD (AMD Optimizing CPU Libraries ‚Äì AOCL) Th∆∞ vi·ªán AMD Optimizing CPU Libraries (AOCL) cung c·∫•p c√°c th∆∞ vi·ªán to√°n h·ªçc ƒë∆∞·ª£c tinh ch·ªânh hi·ªáu nƒÉng d√†nh ri√™ng cho c√°c b·ªô x·ª≠ l√Ω AMD EPYC. Nh·ªØng th∆∞ vi·ªán n√†y bao g·ªìm c√°c tri·ªÉn khai t·ªëi ∆∞u c·ªßa c√°c h√†m hay d√πng trong khoa h·ªçc, k·ªπ thu·∫≠t v√† workload ML. B·∫°n c√≥ th·ªÉ li√™n k·∫øt ·ª©ng d·ª•ng c·ªßa b·∫°n v·ªõi AOCL ƒë·ªÉ s·ª≠ d·ª•ng c√°c t·ªëi ∆∞u ph·∫ßn c·ª©ng m√† kh√¥ng c·∫ßn vi·∫øt l·∫°i m√£. AOCL g·ªìm c√°c th∆∞ vi·ªán cho to√°n vector, scalar, t·∫°o s·ªë ng·∫´u nhi√™n, FFT, BLAS v√† LAPACK, trong s·ªë nh·ªØng c√°i kh√°c.\nC·∫•u h√¨nh AOCL G√°n bi·∫øn m√¥i tr∆∞·ªùng AOCL_ROOT tr·ªè t·ªõi th∆∞ m·ª•c c√†i ƒë·∫∑t:\nexport AOCL_ROOT=/path/to/aocl\nBi√™n d·ªãch ·ª©ng d·ª•ng v·ªõi ƒë∆∞·ªùng d·∫´n include v√† library t∆∞∆°ng ·ª©ng:\ngcc -I$AOCL_ROOT/include -L$AOCL_ROOT/lib -lamdlibm -lm your_program.c -o your_program\nT·ªëi ∆∞u to√°n vector v√† scalar: b·∫°n c√≥ th·ªÉ b·∫≠t c√°c tuning vector h√≥a hay scalar c·ª• th·ªÉ cho workload:\n# T·ªëi ∆∞u to√°n vector\ngcc -lamdlibm -fveclib=AMDLIBM -lm your_program.c -o your_program\n# To√°n scalar nhanh h∆°n\ngcc -lamdlibm -fsclrlib=AMDLIBM -lamdlibmfast -lm your_program.c -o your_program\nProfiling runtime AOCL\nAOCL h·ªó tr·ª£ profiling runtime, gi√∫p c√°c nh√† ph√°t tri·ªÉn x√°c ƒë·ªãnh c√°c ph√©p to√°n n√†o chi·∫øm th·ªùi gian th·ª±c thi l·ªõn. ƒê·ªÉ b·∫≠t profiling:\nexport AOCL_PROFILE=1\n./your_program\nSau khi ch·∫°y, m·ªôt file b√°o c√°o t√™n aocl_profile_report.txt ƒë∆∞·ª£c sinh ra. N√≥ cung c·∫•p ph√¢n t√≠ch theo h√†m g·ªìm s·ªë l·∫ßn g·ªçi, th·ªùi gian th·ª±c thi v√† vi·ªác s·ª≠ d·ª•ng lu·ªìng. C√°c nh√† ph√°t tri·ªÉn c√≥ th·ªÉ d√πng n√≥ ƒë·ªÉ t·∫≠p trung t·ªëi ∆∞u h√≥a v√†o c√°c ph·∫ßn c√≥ ·∫£nh h∆∞·ªüng cao nh·∫•t.\nK·∫øt lu·∫≠n B√†i vi·∫øt n√†y kh·∫£o s√°t c√°ch ch·ªçn c√°c lo·∫°i instance Amazon EC2 d·ª±a AMD ph√π h·ª£p v·ªõi ƒë·∫∑c ƒëi·ªÉm kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác, v√† c√°ch √°p d·ª•ng c√°c k·ªπ thu·∫≠t ƒëi·ªÅu ch·ªânh t·∫≠p trung v√†o vi·ªác s·ª≠ d·ª•ng CPU, ƒë·ªãnh v·ªã lu·ªìng, hi·ªáu qu·∫£ cache v√† t·ªëi ∆∞u th∆∞ vi·ªán to√°n h·ªçc. Nh·ªØng ph∆∞∆°ng ph√°p n√†y ƒë·∫∑c bi·ªát quan tr·ªçng v·ªõi kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác b·ªã gi·ªõi h·∫°n b·ªüi CPU ho·∫∑c nh·∫°y ƒë·ªô tr·ªÖ, n∆°i hi·ªáu su·∫•t ·ªïn ƒë·ªãnh l√† thi·∫øt y·∫øu.\nS·∫µn s√†ng b·∫Øt ƒë·∫ßu? ƒêƒÉng nh·∫≠p AWS Management Console v√† kh·ªüi ƒë·ªông c√°c instance Amazon EC2 d√πng AMD EPYC ƒë·ªÉ b·∫Øt tay v√†o t·ªëi ∆∞u h√≥a workload c·ªßa b·∫°n ngay h√¥m nay.\nTAGS: AMD\n"
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]